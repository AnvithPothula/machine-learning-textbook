{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iL6vPYyVV_fP9BmBurU6alSiE8BMNXMA","timestamp":1752595372081}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rZYIhYT0-_a0"},"source":["#Convolutional Neural Networks\n","This is short introduction to training convolutional neural networks in PyTorch. We first define the convolutional neural network and training and testing functions."]},{"cell_type":"code","metadata":{"id":"NoLJRLXk-7pQ","executionInfo":{"status":"ok","timestamp":1752595736813,"user_tz":300,"elapsed":9226,"user":{"displayName":"Anvith Pothula","userId":"13143211961217781547"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        w = (32,64) #Number of channels in 1st and 2nd layers\n","        self.conv1 = nn.Conv2d(1, w[0], 3, 1)\n","        self.conv2 = nn.Conv2d(w[0], w[1], 3, 1)\n","        self.fc1 = nn.Linear(w[1]*5*5, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","    def forward(self, x):\n","        # images start out 28x28\n","        x = self.conv1(x)  #26x26 images\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)  #13x13 images\n","        x = self.conv2(x)  #11x11 images\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)  #5x5 images\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.dropout(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","\n","#stochastic gradient descent\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXTt1vna_r5y"},"source":["Now let's run code to download MNIST and run the training."]},{"cell_type":"code","metadata":{"id":"dXf1ggvR_vdi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8fc79d3-d35d-41b7-a98e-be82589895c1","executionInfo":{"status":"ok","timestamp":1752595931724,"user_tz":300,"elapsed":172447,"user":{"displayName":"Anvith Pothula","userId":"13143211961217781547"}}},"source":["#Training settings\n","cuda = True   #Use GPU acceleration (Edit->Notebook Settings and enable GPU)\n","seed = 1       #Seed for random number generator\n","batch_size = 64\n","test_batch_size = 1000\n","learning_rate = 1.0   #Learning rate\n","gamma = 0.7     #Learning rate step\n","epochs = 14\n","torch.manual_seed(seed)\n","\n","#GPU\n","use_cuda = cuda and torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","train_kwargs = {'batch_size': batch_size}\n","test_kwargs = {'batch_size': test_batch_size}\n","\n","if use_cuda:\n","    cuda_kwargs = {'num_workers': 1,'pin_memory': True,'shuffle': True}\n","    train_kwargs.update(cuda_kwargs)\n","    test_kwargs.update(cuda_kwargs)\n","\n","transform = transforms.Compose([transforms.ToTensor()])\n","dataset1 = datasets.MNIST('./data', train=True, download=True,transform=transform)\n","dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n","train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n","test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n","\n","model = Net().to(device)\n","optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n","\n","scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)\n","    scheduler.step()\n","\n","#Save model\n","torch.save(model.state_dict(), \"mnist_cnn.pth\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:11<00:00, 900kB/s] \n","100%|██████████| 28.9k/28.9k [00:00<00:00, 63.7kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.25MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 8.24MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311258\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.381221\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.124412\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.143522\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.104330\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.095187\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.177269\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.060563\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.205674\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.171758\n","\n","Test set: Average loss: 0.0453, Accuracy: 9848/10000 (98%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.060170\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.077313\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.016890\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.005444\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.053288\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.100470\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.051439\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.135846\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.034816\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.004338\n","\n","Test set: Average loss: 0.0301, Accuracy: 9888/10000 (99%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.041514\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.019059\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.090488\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.033937\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.021457\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.011198\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007585\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.051566\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.003210\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008128\n","\n","Test set: Average loss: 0.0230, Accuracy: 9922/10000 (99%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.011743\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.006406\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.009704\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.115213\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.023060\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.008354\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.010772\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007304\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.078304\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.009494\n","\n","Test set: Average loss: 0.0213, Accuracy: 9925/10000 (99%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.033157\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000726\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.011513\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.003127\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.034711\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.015359\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.069245\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.003413\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001770\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.002391\n","\n","Test set: Average loss: 0.0207, Accuracy: 9929/10000 (99%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.012082\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.000201\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.007109\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.002226\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.021977\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.001846\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.002229\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.000978\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.004597\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.045269\n","\n","Test set: Average loss: 0.0197, Accuracy: 9929/10000 (99%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001216\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.003809\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004475\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000788\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.002104\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001423\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.012141\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.007050\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.002112\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.002812\n","\n","Test set: Average loss: 0.0203, Accuracy: 9928/10000 (99%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.041942\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.018451\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000933\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.003324\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.005224\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001162\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.000786\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.005991\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.002811\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.004757\n","\n","Test set: Average loss: 0.0190, Accuracy: 9931/10000 (99%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004122\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.000322\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.003461\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.019433\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.003924\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001162\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.005299\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.003081\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.009074\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.005460\n","\n","Test set: Average loss: 0.0192, Accuracy: 9932/10000 (99%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006942\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.006261\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001341\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.012580\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001743\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000525\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002221\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.001029\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002757\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.015428\n","\n","Test set: Average loss: 0.0190, Accuracy: 9931/10000 (99%)\n","\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.000226\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.000992\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.000452\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.004408\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.009624\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000546\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.001885\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.016043\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.003812\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.001545\n","\n","Test set: Average loss: 0.0189, Accuracy: 9930/10000 (99%)\n","\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000090\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.001178\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.000941\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.000428\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001422\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.036743\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.004494\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.005075\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.001129\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.004024\n","\n","Test set: Average loss: 0.0190, Accuracy: 9928/10000 (99%)\n","\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.002782\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.000308\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.000095\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.010430\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.024996\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.011073\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.001048\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.008578\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.050522\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.088728\n","\n","Test set: Average loss: 0.0191, Accuracy: 9930/10000 (99%)\n","\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001352\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.003093\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.000234\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.005074\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.000074\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000723\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.018081\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.000569\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.034785\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.004048\n","\n","Test set: Average loss: 0.0190, Accuracy: 9931/10000 (99%)\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"y9C4XaBzL_6K"},"source":["#Exercises\n","1. Try another dataset from [Torch Datasets](https://pytorch.org/vision/stable/datasets.html).\n","2. Try to plot the filters from the second layer, using graphlearning.image_grid."]},{"cell_type":"code","metadata":{"id":"MToYs4hNN_ez","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752604461844,"user_tz":300,"elapsed":173351,"user":{"displayName":"Anvith Pothula","userId":"13143211961217781547"}},"outputId":"583d1ba9-364c-4d2c-c4ad-f0a5435b729f"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","\n","\n","class CifarNet(nn.Module):\n","    def __init__(self):\n","        super(CifarNet, self).__init__()\n","        w = (32,64) #Number of channels in 1st and 2nd layers\n","        self.conv1 = nn.Conv2d(3, w[0], 3, 1)\n","        self.conv2 = nn.Conv2d(w[0], w[1], 3, 1)\n","        self.fc1 = nn.Linear(w[1]*6*6, 512)\n","        self.fc2 = nn.Linear(512, 128)\n","        self.fc3 = nn.Linear(128,10)\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","    def forward(self, x):\n","        # images start out 28x28\n","        x = self.conv1(x)  #26x26 images\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)  #13x13 images\n","        x = self.conv2(x)  #11x11 images\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)  #5x5 images\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.dropout(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","\n","#stochastic gradient descent\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","#Training settings\n","cuda = True   #Use GPU acceleration (Edit->Notebook Settings and enable GPU)\n","seed = 1       #Seed for random number generator\n","batch_size = 64\n","test_batch_size = 1000\n","learning_rate = 1.0   #Learning rate\n","gamma = 0.7     #Learning rate step\n","epochs = 15\n","torch.manual_seed(seed)\n","\n","#GPU\n","use_cuda = cuda and torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","train_kwargs = {'batch_size': batch_size}\n","test_kwargs = {'batch_size': test_batch_size}\n","\n","if use_cuda:\n","    cuda_kwargs = {'num_workers': 1,'pin_memory': True,'shuffle': True}\n","    train_kwargs.update(cuda_kwargs)\n","    test_kwargs.update(cuda_kwargs)\n","\n","transform = transforms.Compose([transforms.ToTensor()])\n","dataset1 = datasets.CIFAR10('./data', train=True, download=True,transform=transform)\n","dataset2 = datasets.CIFAR10('./data', train=False, transform=transform)\n","train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n","test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n","\n","model = CifarNet().to(device)\n","optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n","\n","scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","for epoch in range(1, epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)\n","    scheduler.step()\n","\n","#Save model\n","torch.save(model.state_dict(), \"cifar10_cnn.pth\")\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.311228\n","Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.184641\n","Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.994224\n","Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.838387\n","Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.778403\n","Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.833293\n","Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.559589\n","Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.656407\n","\n","Test set: Average loss: 1.6761, Accuracy: 4078/10000 (41%)\n","\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.871727\n","Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.521663\n","Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.372251\n","Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.029970\n","Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.234065\n","Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.201975\n","Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.352843\n","Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.976336\n","\n","Test set: Average loss: 1.1442, Accuracy: 5957/10000 (60%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.148339\n","Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.158562\n","Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.166690\n","Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.015023\n","Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.083229\n","Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.018461\n","Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.906556\n","Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.103551\n","\n","Test set: Average loss: 1.0637, Accuracy: 6404/10000 (64%)\n","\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.827658\n","Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.067676\n","Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.843629\n","Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.900697\n","Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.946546\n","Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.001087\n","Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.948344\n","Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.887950\n","\n","Test set: Average loss: 0.9392, Accuracy: 6764/10000 (68%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.836312\n","Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.968666\n","Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.724480\n","Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.868553\n","Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.838219\n","Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.668585\n","Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.943746\n","Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.756927\n","\n","Test set: Average loss: 0.8946, Accuracy: 6990/10000 (70%)\n","\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.704260\n","Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.847545\n","Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.674816\n","Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.713688\n","Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.516760\n","Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.781241\n","Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.864325\n","Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.842413\n","\n","Test set: Average loss: 0.8508, Accuracy: 7142/10000 (71%)\n","\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.977518\n","Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.475753\n","Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.798489\n","Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.809376\n","Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.727445\n","Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.968991\n","Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.828744\n","Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.695376\n","\n","Test set: Average loss: 0.9056, Accuracy: 7001/10000 (70%)\n","\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.872035\n","Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.577736\n","Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.685561\n","Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.760106\n","Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.765183\n","Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.563950\n","Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.906963\n","Train Epoch: 8 [44800/50000 (90%)]\tLoss: 1.072725\n","\n","Test set: Average loss: 0.8233, Accuracy: 7313/10000 (73%)\n","\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.674597\n","Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.625515\n","Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.706584\n","Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.619677\n","Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.570741\n","Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.580241\n","Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.457010\n","Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.442518\n","\n","Test set: Average loss: 0.8269, Accuracy: 7315/10000 (73%)\n","\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.517391\n","Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.697824\n","Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.581932\n","Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.546470\n","Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.761963\n","Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.653357\n","Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.765670\n","Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.497426\n","\n","Test set: Average loss: 0.8129, Accuracy: 7301/10000 (73%)\n","\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.603207\n","Train Epoch: 11 [6400/50000 (13%)]\tLoss: 0.508620\n","Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.713618\n","Train Epoch: 11 [19200/50000 (38%)]\tLoss: 0.515278\n","Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.557835\n","Train Epoch: 11 [32000/50000 (64%)]\tLoss: 0.584568\n","Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.695607\n","Train Epoch: 11 [44800/50000 (90%)]\tLoss: 0.494626\n","\n","Test set: Average loss: 0.8146, Accuracy: 7337/10000 (73%)\n","\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.581011\n","Train Epoch: 12 [6400/50000 (13%)]\tLoss: 0.569580\n","Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.670871\n","Train Epoch: 12 [19200/50000 (38%)]\tLoss: 0.480378\n","Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.711377\n","Train Epoch: 12 [32000/50000 (64%)]\tLoss: 0.448105\n","Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.512107\n","Train Epoch: 12 [44800/50000 (90%)]\tLoss: 0.563728\n","\n","Test set: Average loss: 0.8122, Accuracy: 7355/10000 (74%)\n","\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.431904\n","Train Epoch: 13 [6400/50000 (13%)]\tLoss: 0.469693\n","Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.527155\n","Train Epoch: 13 [19200/50000 (38%)]\tLoss: 0.626336\n","Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.559236\n","Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.653155\n","Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.705918\n","Train Epoch: 13 [44800/50000 (90%)]\tLoss: 0.401119\n","\n","Test set: Average loss: 0.8147, Accuracy: 7390/10000 (74%)\n","\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.675833\n","Train Epoch: 14 [6400/50000 (13%)]\tLoss: 0.426817\n","Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.586952\n","Train Epoch: 14 [19200/50000 (38%)]\tLoss: 0.844023\n","Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.821495\n","Train Epoch: 14 [32000/50000 (64%)]\tLoss: 0.617226\n","Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.833517\n","Train Epoch: 14 [44800/50000 (90%)]\tLoss: 0.660542\n","\n","Test set: Average loss: 0.8131, Accuracy: 7387/10000 (74%)\n","\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.791013\n","Train Epoch: 15 [6400/50000 (13%)]\tLoss: 0.736592\n","Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.536567\n","Train Epoch: 15 [19200/50000 (38%)]\tLoss: 0.553359\n","Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.391413\n","Train Epoch: 15 [32000/50000 (64%)]\tLoss: 0.647801\n","Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.628402\n","Train Epoch: 15 [44800/50000 (90%)]\tLoss: 0.720462\n","\n","Test set: Average loss: 0.8101, Accuracy: 7392/10000 (74%)\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"5sb7m7zwOIIq"},"source":["Below is the solution to the second exercise."]},{"cell_type":"code","metadata":{"id":"qftMk5aikODb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b03acaf-fef6-4698-a14d-e489b563db12","executionInfo":{"status":"ok","timestamp":1752598361961,"user_tz":300,"elapsed":29027,"user":{"displayName":"Anvith Pothula","userId":"13143211961217781547"}}},"source":["!pip install -q graphlearning"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for graphlearning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"1l4DJOXGMM9j","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"799addb9-480c-418c-e86e-04f4a3c33f52","executionInfo":{"status":"error","timestamp":1752598432081,"user_tz":300,"elapsed":1755,"user":{"displayName":"Anvith Pothula","userId":"13143211961217781547"}}},"source":["import numpy as np\n","import graphlearning as gl\n","\n","w2 = model.conv2.weight.cpu().detach().numpy()\n","print(w2.shape)\n","\n","I = np.reshape(w2[:,16,:,:], (64,9))\n","print(I.shape)\n","gl.utils.image_grid(I,n_rows=8,n_cols=8)\n","I = np.reshape(np.sum(np.absolute(w2),axis=1), (64,9))\n","print(I.shape)\n","gl.utils.image_grid(I,n_rows=8,n_cols=8)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-1934255242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraphlearning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]}]}