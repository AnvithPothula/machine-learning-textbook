<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="From artificial neurons to deep learning - understanding the architecture and training of neural networks"><meta name=author content="Anvith Pothula"><link href=https://example.com/chapters/09-neural-networks/ rel=canonical><link href=../08-data-preprocessing/quiz/ rel=prev><link href=quiz/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Neural Networks Fundamentals - Machine Learning - Algorithms and Applications</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#neural-networks-fundamentals class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Machine Learning - Algorithms and Applications" class="md-header__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning - Algorithms and Applications </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Neural Networks Fundamentals </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../course-description/ class=md-tabs__link> Course Description </a> </li> <li class=md-tabs__item> <a href=../../faq/ class=md-tabs__link> FAQ </a> </li> <li class=md-tabs__item> <a href=../../glossary/ class=md-tabs__link> Glossary </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> Chapters </a> </li> <li class=md-tabs__item> <a href=../../learning-graph/ class=md-tabs__link> Learning Graph </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Machine Learning - Algorithms and Applications" class="md-nav__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Machine Learning - Algorithms and Applications </label> <div class=md-nav__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../../course-description/ class=md-nav__link> <span class=md-ellipsis> Course Description </span> </a> </li> <li class=md-nav__item> <a href=../../faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> <li class=md-nav__item> <a href=../../glossary/ class=md-nav__link> <span class=md-ellipsis> Glossary </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Chapters </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> 1. ML Fundamentals </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> 1. ML Fundamentals </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01-intro-to-ml-fundamentals/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../01-intro-to-ml-fundamentals/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> 2. K-Nearest Neighbors </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> 2. K-Nearest Neighbors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../02-k-nearest-neighbors/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../02-k-nearest-neighbors/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex> <span class=md-ellipsis> 3. Decision Trees </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> 3. Decision Trees </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../03-decision-trees/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../03-decision-trees/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex> <span class=md-ellipsis> 4. Logistic Regression </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> 4. Logistic Regression </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../04-logistic-regression/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../04-logistic-regression/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex> <span class=md-ellipsis> 5. Regularization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> 5. Regularization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../05-regularization/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../05-regularization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_7> <label class=md-nav__link for=__nav_5_7 id=__nav_5_7_label tabindex> <span class=md-ellipsis> 6. Support Vector Machines </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> 6. Support Vector Machines </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../06-support-vector-machines/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../06-support-vector-machines/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex> <span class=md-ellipsis> 7. K-Means Clustering </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> 7. K-Means Clustering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../07-k-means-clustering/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../07-k-means-clustering/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_9> <label class=md-nav__link for=__nav_5_9 id=__nav_5_9_label tabindex> <span class=md-ellipsis> 8. Data Preprocessing </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_9_label aria-expanded=false> <label class=md-nav__title for=__nav_5_9> <span class="md-nav__icon md-icon"></span> 8. Data Preprocessing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../08-data-preprocessing/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../08-data-preprocessing/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_10 checked> <label class=md-nav__link for=__nav_5_10 id=__nav_5_10_label tabindex> <span class=md-ellipsis> 9. Neural Networks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_10_label aria-expanded=true> <label class=md-nav__title for=__nav_5_10> <span class="md-nav__icon md-icon"></span> 9. Neural Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Content </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Content </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#concepts-covered class=md-nav__link> <span class=md-ellipsis> Concepts Covered </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#introduction-inspired-by-the-brain class=md-nav__link> <span class=md-ellipsis> Introduction: Inspired by the Brain </span> </a> </li> <li class=md-nav__item> <a href=#the-artificial-neuron class=md-nav__link> <span class=md-ellipsis> The Artificial Neuron </span> </a> <nav class=md-nav aria-label="The Artificial Neuron"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mathematical-model class=md-nav__link> <span class=md-ellipsis> Mathematical Model </span> </a> </li> <li class=md-nav__item> <a href=#the-perceptron class=md-nav__link> <span class=md-ellipsis> The Perceptron </span> </a> </li> <li class=md-nav__item> <a href=#biological-inspiration class=md-nav__link> <span class=md-ellipsis> Biological Inspiration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#activation-functions class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> <nav class=md-nav aria-label="Activation Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sigmoid class=md-nav__link> <span class=md-ellipsis> Sigmoid </span> </a> </li> <li class=md-nav__item> <a href=#hyperbolic-tangent-tanh class=md-nav__link> <span class=md-ellipsis> Hyperbolic Tangent (Tanh) </span> </a> </li> <li class=md-nav__item> <a href=#rectified-linear-unit-relu class=md-nav__link> <span class=md-ellipsis> Rectified Linear Unit (ReLU) </span> </a> </li> <li class=md-nav__item> <a href=#leaky-relu class=md-nav__link> <span class=md-ellipsis> Leaky ReLU </span> </a> </li> <li class=md-nav__item> <a href=#choosing-activation-functions class=md-nav__link> <span class=md-ellipsis> Choosing Activation Functions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#network-architecture class=md-nav__link> <span class=md-ellipsis> Network Architecture </span> </a> <nav class=md-nav aria-label="Network Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#layer-types class=md-nav__link> <span class=md-ellipsis> Layer Types </span> </a> </li> <li class=md-nav__item> <a href=#multilayer-perceptron-mlp class=md-nav__link> <span class=md-ellipsis> Multilayer Perceptron (MLP) </span> </a> </li> <li class=md-nav__item> <a href=#deep-learning class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#forward-propagation class=md-nav__link> <span class=md-ellipsis> Forward Propagation </span> </a> <nav class=md-nav aria-label="Forward Propagation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#algorithm class=md-nav__link> <span class=md-ellipsis> Algorithm </span> </a> </li> <li class=md-nav__item> <a href=#example-computation class=md-nav__link> <span class=md-ellipsis> Example Computation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#loss-functions class=md-nav__link> <span class=md-ellipsis> Loss Functions </span> </a> <nav class=md-nav aria-label="Loss Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mean-squared-error-mse class=md-nav__link> <span class=md-ellipsis> Mean Squared Error (MSE) </span> </a> </li> <li class=md-nav__item> <a href=#cross-entropy-loss class=md-nav__link> <span class=md-ellipsis> Cross-Entropy Loss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#backpropagation class=md-nav__link> <span class=md-ellipsis> Backpropagation </span> </a> <nav class=md-nav aria-label=Backpropagation> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-chain-rule class=md-nav__link> <span class=md-ellipsis> The Chain Rule </span> </a> </li> <li class=md-nav__item> <a href=#backpropagation-algorithm class=md-nav__link> <span class=md-ellipsis> Backpropagation Algorithm </span> </a> </li> <li class=md-nav__item> <a href=#why-backpropagation-matters class=md-nav__link> <span class=md-ellipsis> Why Backpropagation Matters </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#gradient-descent class=md-nav__link> <span class=md-ellipsis> Gradient Descent </span> </a> <nav class=md-nav aria-label="Gradient Descent"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#batch-gradient-descent class=md-nav__link> <span class=md-ellipsis> Batch Gradient Descent </span> </a> </li> <li class=md-nav__item> <a href=#stochastic-gradient-descent-sgd class=md-nav__link> <span class=md-ellipsis> Stochastic Gradient Descent (SGD) </span> </a> </li> <li class=md-nav__item> <a href=#mini-batch-gradient-descent class=md-nav__link> <span class=md-ellipsis> Mini-Batch Gradient Descent </span> </a> </li> <li class=md-nav__item> <a href=#learning-rate class=md-nav__link> <span class=md-ellipsis> Learning Rate </span> </a> </li> <li class=md-nav__item> <a href=#epochs class=md-nav__link> <span class=md-ellipsis> Epochs </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#weight-initialization class=md-nav__link> <span class=md-ellipsis> Weight Initialization </span> </a> <nav class=md-nav aria-label="Weight Initialization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#why-initialization-matters class=md-nav__link> <span class=md-ellipsis> Why Initialization Matters </span> </a> </li> <li class=md-nav__item> <a href=#xavier-glorot-initialization class=md-nav__link> <span class=md-ellipsis> Xavier (Glorot) Initialization </span> </a> </li> <li class=md-nav__item> <a href=#he-initialization class=md-nav__link> <span class=md-ellipsis> He Initialization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#training-challenges class=md-nav__link> <span class=md-ellipsis> Training Challenges </span> </a> <nav class=md-nav aria-label="Training Challenges"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#vanishing-gradients class=md-nav__link> <span class=md-ellipsis> Vanishing Gradients </span> </a> </li> <li class=md-nav__item> <a href=#exploding-gradients class=md-nav__link> <span class=md-ellipsis> Exploding Gradients </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#regularization-techniques class=md-nav__link> <span class=md-ellipsis> Regularization Techniques </span> </a> <nav class=md-nav aria-label="Regularization Techniques"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dropout class=md-nav__link> <span class=md-ellipsis> Dropout </span> </a> </li> <li class=md-nav__item> <a href=#early-stopping class=md-nav__link> <span class=md-ellipsis> Early Stopping </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#universal-approximation-theorem class=md-nav__link> <span class=md-ellipsis> Universal Approximation Theorem </span> </a> </li> <li class=md-nav__item> <a href=#neural-networks-in-practice class=md-nav__link> <span class=md-ellipsis> Neural Networks in Practice </span> </a> <nav class=md-nav aria-label="Neural Networks in Practice"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#building-a-neural-network-with-scikit-learn class=md-nav__link> <span class=md-ellipsis> Building a Neural Network with Scikit-Learn </span> </a> </li> <li class=md-nav__item> <a href=#training-the-network class=md-nav__link> <span class=md-ellipsis> Training the Network </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-multiple-runs class=md-nav__link> <span class=md-ellipsis> Evaluating Multiple Runs </span> </a> </li> <li class=md-nav__item> <a href=#hyperparameter-tuning class=md-nav__link> <span class=md-ellipsis> Hyperparameter Tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-architectures class=md-nav__link> <span class=md-ellipsis> Advanced Architectures </span> </a> <nav class=md-nav aria-label="Advanced Architectures"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#pooling-layers class=md-nav__link> <span class=md-ellipsis> Pooling Layers </span> </a> </li> <li class=md-nav__item> <a href=#freezing-layers class=md-nav__link> <span class=md-ellipsis> Freezing Layers </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#interactive-visualization-neural-network-architecture class=md-nav__link> <span class=md-ellipsis> Interactive Visualization: Neural Network Architecture </span> </a> </li> <li class=md-nav__item> <a href=#interactive-visualization-activation-functions class=md-nav__link> <span class=md-ellipsis> Interactive Visualization: Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=#summary_1 class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#key-takeaways class=md-nav__link> <span class=md-ellipsis> Key Takeaways </span> </a> </li> <li class=md-nav__item> <a href=#further-reading class=md-nav__link> <span class=md-ellipsis> Further Reading </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_11> <label class=md-nav__link for=__nav_5_11 id=__nav_5_11_label tabindex> <span class=md-ellipsis> 10. Convolutional Networks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_11_label aria-expanded=false> <label class=md-nav__title for=__nav_5_11> <span class="md-nav__icon md-icon"></span> 10. Convolutional Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../10-convolutional-networks/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../10-convolutional-networks/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_12> <label class=md-nav__link for=__nav_5_12 id=__nav_5_12_label tabindex> <span class=md-ellipsis> 11. Transfer Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_12_label aria-expanded=false> <label class=md-nav__title for=__nav_5_12> <span class="md-nav__icon md-icon"></span> 11. Transfer Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../11-transfer-learning/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../11-transfer-learning/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_13> <label class=md-nav__link for=__nav_5_13 id=__nav_5_13_label tabindex> <span class=md-ellipsis> 12. Evaluation & Optimization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_13_label aria-expanded=false> <label class=md-nav__title for=__nav_5_13> <span class="md-nav__icon md-icon"></span> 12. Evaluation & Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../12-evaluation-optimization/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../12-evaluation-optimization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Learning Graph </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Learning Graph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../learning-graph/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../sims/graph-viewer/ class=md-nav__link> <span class=md-ellipsis> Graph Viewer </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/course-description-assessment/ class=md-nav__link> <span class=md-ellipsis> Course Description Assessment </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/concept-list/ class=md-nav__link> <span class=md-ellipsis> Concept List </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/concept-taxonomy/ class=md-nav__link> <span class=md-ellipsis> Concept Taxonomy </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/learning-graph.csv class=md-nav__link> <span class=md-ellipsis> Learning Graph (CSV) </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/learning-graph.json class=md-nav__link> <span class=md-ellipsis> Learning Graph (JSON) </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/quality-metrics/ class=md-nav__link> <span class=md-ellipsis> Quality Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/taxonomy-distribution/ class=md-nav__link> <span class=md-ellipsis> Taxonomy Distribution </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/glossary-quality-report/ class=md-nav__link> <span class=md-ellipsis> Glossary Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/faq-quality-report/ class=md-nav__link> <span class=md-ellipsis> FAQ Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/faq-coverage-gaps/ class=md-nav__link> <span class=md-ellipsis> FAQ Coverage Gaps </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#concepts-covered class=md-nav__link> <span class=md-ellipsis> Concepts Covered </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#introduction-inspired-by-the-brain class=md-nav__link> <span class=md-ellipsis> Introduction: Inspired by the Brain </span> </a> </li> <li class=md-nav__item> <a href=#the-artificial-neuron class=md-nav__link> <span class=md-ellipsis> The Artificial Neuron </span> </a> <nav class=md-nav aria-label="The Artificial Neuron"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mathematical-model class=md-nav__link> <span class=md-ellipsis> Mathematical Model </span> </a> </li> <li class=md-nav__item> <a href=#the-perceptron class=md-nav__link> <span class=md-ellipsis> The Perceptron </span> </a> </li> <li class=md-nav__item> <a href=#biological-inspiration class=md-nav__link> <span class=md-ellipsis> Biological Inspiration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#activation-functions class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> <nav class=md-nav aria-label="Activation Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sigmoid class=md-nav__link> <span class=md-ellipsis> Sigmoid </span> </a> </li> <li class=md-nav__item> <a href=#hyperbolic-tangent-tanh class=md-nav__link> <span class=md-ellipsis> Hyperbolic Tangent (Tanh) </span> </a> </li> <li class=md-nav__item> <a href=#rectified-linear-unit-relu class=md-nav__link> <span class=md-ellipsis> Rectified Linear Unit (ReLU) </span> </a> </li> <li class=md-nav__item> <a href=#leaky-relu class=md-nav__link> <span class=md-ellipsis> Leaky ReLU </span> </a> </li> <li class=md-nav__item> <a href=#choosing-activation-functions class=md-nav__link> <span class=md-ellipsis> Choosing Activation Functions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#network-architecture class=md-nav__link> <span class=md-ellipsis> Network Architecture </span> </a> <nav class=md-nav aria-label="Network Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#layer-types class=md-nav__link> <span class=md-ellipsis> Layer Types </span> </a> </li> <li class=md-nav__item> <a href=#multilayer-perceptron-mlp class=md-nav__link> <span class=md-ellipsis> Multilayer Perceptron (MLP) </span> </a> </li> <li class=md-nav__item> <a href=#deep-learning class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#forward-propagation class=md-nav__link> <span class=md-ellipsis> Forward Propagation </span> </a> <nav class=md-nav aria-label="Forward Propagation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#algorithm class=md-nav__link> <span class=md-ellipsis> Algorithm </span> </a> </li> <li class=md-nav__item> <a href=#example-computation class=md-nav__link> <span class=md-ellipsis> Example Computation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#loss-functions class=md-nav__link> <span class=md-ellipsis> Loss Functions </span> </a> <nav class=md-nav aria-label="Loss Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mean-squared-error-mse class=md-nav__link> <span class=md-ellipsis> Mean Squared Error (MSE) </span> </a> </li> <li class=md-nav__item> <a href=#cross-entropy-loss class=md-nav__link> <span class=md-ellipsis> Cross-Entropy Loss </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#backpropagation class=md-nav__link> <span class=md-ellipsis> Backpropagation </span> </a> <nav class=md-nav aria-label=Backpropagation> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-chain-rule class=md-nav__link> <span class=md-ellipsis> The Chain Rule </span> </a> </li> <li class=md-nav__item> <a href=#backpropagation-algorithm class=md-nav__link> <span class=md-ellipsis> Backpropagation Algorithm </span> </a> </li> <li class=md-nav__item> <a href=#why-backpropagation-matters class=md-nav__link> <span class=md-ellipsis> Why Backpropagation Matters </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#gradient-descent class=md-nav__link> <span class=md-ellipsis> Gradient Descent </span> </a> <nav class=md-nav aria-label="Gradient Descent"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#batch-gradient-descent class=md-nav__link> <span class=md-ellipsis> Batch Gradient Descent </span> </a> </li> <li class=md-nav__item> <a href=#stochastic-gradient-descent-sgd class=md-nav__link> <span class=md-ellipsis> Stochastic Gradient Descent (SGD) </span> </a> </li> <li class=md-nav__item> <a href=#mini-batch-gradient-descent class=md-nav__link> <span class=md-ellipsis> Mini-Batch Gradient Descent </span> </a> </li> <li class=md-nav__item> <a href=#learning-rate class=md-nav__link> <span class=md-ellipsis> Learning Rate </span> </a> </li> <li class=md-nav__item> <a href=#epochs class=md-nav__link> <span class=md-ellipsis> Epochs </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#weight-initialization class=md-nav__link> <span class=md-ellipsis> Weight Initialization </span> </a> <nav class=md-nav aria-label="Weight Initialization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#why-initialization-matters class=md-nav__link> <span class=md-ellipsis> Why Initialization Matters </span> </a> </li> <li class=md-nav__item> <a href=#xavier-glorot-initialization class=md-nav__link> <span class=md-ellipsis> Xavier (Glorot) Initialization </span> </a> </li> <li class=md-nav__item> <a href=#he-initialization class=md-nav__link> <span class=md-ellipsis> He Initialization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#training-challenges class=md-nav__link> <span class=md-ellipsis> Training Challenges </span> </a> <nav class=md-nav aria-label="Training Challenges"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#vanishing-gradients class=md-nav__link> <span class=md-ellipsis> Vanishing Gradients </span> </a> </li> <li class=md-nav__item> <a href=#exploding-gradients class=md-nav__link> <span class=md-ellipsis> Exploding Gradients </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#regularization-techniques class=md-nav__link> <span class=md-ellipsis> Regularization Techniques </span> </a> <nav class=md-nav aria-label="Regularization Techniques"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dropout class=md-nav__link> <span class=md-ellipsis> Dropout </span> </a> </li> <li class=md-nav__item> <a href=#early-stopping class=md-nav__link> <span class=md-ellipsis> Early Stopping </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#universal-approximation-theorem class=md-nav__link> <span class=md-ellipsis> Universal Approximation Theorem </span> </a> </li> <li class=md-nav__item> <a href=#neural-networks-in-practice class=md-nav__link> <span class=md-ellipsis> Neural Networks in Practice </span> </a> <nav class=md-nav aria-label="Neural Networks in Practice"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#building-a-neural-network-with-scikit-learn class=md-nav__link> <span class=md-ellipsis> Building a Neural Network with Scikit-Learn </span> </a> </li> <li class=md-nav__item> <a href=#training-the-network class=md-nav__link> <span class=md-ellipsis> Training the Network </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-multiple-runs class=md-nav__link> <span class=md-ellipsis> Evaluating Multiple Runs </span> </a> </li> <li class=md-nav__item> <a href=#hyperparameter-tuning class=md-nav__link> <span class=md-ellipsis> Hyperparameter Tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-architectures class=md-nav__link> <span class=md-ellipsis> Advanced Architectures </span> </a> <nav class=md-nav aria-label="Advanced Architectures"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#pooling-layers class=md-nav__link> <span class=md-ellipsis> Pooling Layers </span> </a> </li> <li class=md-nav__item> <a href=#freezing-layers class=md-nav__link> <span class=md-ellipsis> Freezing Layers </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#interactive-visualization-neural-network-architecture class=md-nav__link> <span class=md-ellipsis> Interactive Visualization: Neural Network Architecture </span> </a> </li> <li class=md-nav__item> <a href=#interactive-visualization-activation-functions class=md-nav__link> <span class=md-ellipsis> Interactive Visualization: Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=#summary_1 class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#key-takeaways class=md-nav__link> <span class=md-ellipsis> Key Takeaways </span> </a> </li> <li class=md-nav__item> <a href=#further-reading class=md-nav__link> <span class=md-ellipsis> Further Reading </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=neural-networks-fundamentals>Neural Networks Fundamentals<a class=headerlink href=#neural-networks-fundamentals title="Permanent link">&para;</a></h1> <h2 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h2> <p>This comprehensive chapter introduces neural networks, the foundation of modern deep learning. Students will learn about artificial neurons and the perceptron model, explore various activation functions (ReLU, tanh, sigmoid, Leaky ReLU) and their properties, and understand the architecture of multilayer networks with input, hidden, and output layers. The chapter provides detailed coverage of forward propagation for making predictions and backpropagation for computing gradients, introduces gradient descent and its variants (stochastic, mini-batch), and covers essential topics including loss functions (mean squared error, cross-entropy), weight initialization strategies (Xavier, He), and challenges like vanishing and exploding gradients. Students will also learn about advanced concepts including the universal approximation theorem, network architectures, and deep learning fundamentals.</p> <h2 id=concepts-covered>Concepts Covered<a class=headerlink href=#concepts-covered title="Permanent link">&para;</a></h2> <p>This chapter covers the following 38 concepts from the learning graph:</p> <ol> <li>Neural Network</li> <li>Artificial Neuron</li> <li>Perceptron</li> <li>Activation Function</li> <li>ReLU</li> <li>Tanh</li> <li>Leaky ReLU</li> <li>Weights</li> <li>Bias</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Gradient Descent</li> <li>Stochastic Gradient Descent</li> <li>Mini-Batch Gradient Descent</li> <li>Learning Rate</li> <li>Mean Squared Error</li> <li>Epoch</li> <li>Batch Size</li> <li>Vanishing Gradient</li> <li>Exploding Gradient</li> <li>Weight Initialization</li> <li>Xavier Initialization</li> <li>He Initialization</li> <li>Fully Connected Layer</li> <li>Hidden Layer</li> <li>Output Layer</li> <li>Input Layer</li> <li>Network Architecture</li> <li>Deep Learning</li> <li>Multilayer Perceptron</li> <li>Universal Approximation</li> <li>Pooling Layer</li> <li>Freezing Layers</li> <li>Learning Rate Scheduling</li> <li>Bias-Variance Tradeoff</li> <li>Batch Processing</li> <li>Dropout</li> <li>Early Stopping</li> </ol> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h2> <p>This chapter builds on concepts from:</p> <ul> <li><a href=../01-intro-to-ml-fundamentals/ >Chapter 1: Introduction to Machine Learning Fundamentals</a></li> <li><a href=../03-decision-trees/ >Chapter 3: Decision Trees and Tree-Based Learning</a></li> <li><a href=../05-regularization/ >Chapter 5: Regularization Techniques</a></li> </ul> <hr> <h2 id=introduction-inspired-by-the-brain>Introduction: Inspired by the Brain<a class=headerlink href=#introduction-inspired-by-the-brain title="Permanent link">&para;</a></h2> <p><strong>Neural networks</strong> are computational models inspired by the biological neural networks in animal brains. While greatly simplified compared to actual neurons, artificial neural networks have proven remarkably effective at learning complex patterns from data, powering modern advances in computer vision, natural language processing, speech recognition, and game playing.</p> <p>Unlike traditional algorithms with explicit rules, neural networks <em>learn</em> from examples. Show a neural network thousands of images labeled "cat" or "dog," and it learns to distinguish between themnot through programmed rules about whiskers or ears, but by discovering patterns in the pixel data itself.</p> <p>This chapter builds neural networks from the ground up, starting with a single artificial neuron and progressing to deep multilayer architectures capable of solving complex real-world problems.</p> <h2 id=the-artificial-neuron>The Artificial Neuron<a class=headerlink href=#the-artificial-neuron title="Permanent link">&para;</a></h2> <p>An <strong>artificial neuron</strong> (or simply "neuron") is the fundamental building block of neural networks. It receives inputs, combines them with learned weights, adds a bias, and applies an activation function to produce an output.</p> <h3 id=mathematical-model>Mathematical Model<a class=headerlink href=#mathematical-model title="Permanent link">&para;</a></h3> <p>For a neuron with <span class=arithmatex>\(n\)</span> inputs <span class=arithmatex>\(x_1, x_2, \ldots, x_n\)</span>:</p> <ol> <li><strong>Weighted sum</strong>: Compute <span class=arithmatex>\(z = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b\)</span></li> <li><strong>Activation</strong>: Apply activation function <span class=arithmatex>\(a = f(z)\)</span></li> </ol> <p>where: - <strong>Weights</strong> <span class=arithmatex>\(w_1, \ldots, w_n\)</span> scale the importance of each input - <strong>Bias</strong> <span class=arithmatex>\(b\)</span> shifts the activation threshold - <strong>Activation function</strong> <span class=arithmatex>\(f\)</span> introduces nonlinearity</p> <p>In vector notation:</p> <div class=arithmatex>\[z = \mathbf{w}^T \mathbf{x} + b$$ $$a = f(z)\]</div> <p>The neuron learns by adjusting weights <span class=arithmatex>\(\mathbf{w}\)</span> and bias <span class=arithmatex>\(b\)</span> during training.</p> <h3 id=the-perceptron>The Perceptron<a class=headerlink href=#the-perceptron title="Permanent link">&para;</a></h3> <p>The <strong>perceptron</strong>, introduced by Frank Rosenblatt in 1958, is the simplest neural network model. It uses a step activation function:</p> <div class=arithmatex>\[f(z) = \begin{cases} 1 &amp; \text{if } z \geq 0 \\ 0 &amp; \text{if } z &lt; 0 \end{cases}\]</div> <p>For linearly separable binary classification problems, the perceptron learning algorithm is guaranteed to converge. However, perceptrons cannot solve non-linearly separable problems (like XOR), which motivated the development of multilayer networks.</p> <h3 id=biological-inspiration>Biological Inspiration<a class=headerlink href=#biological-inspiration title="Permanent link">&para;</a></h3> <p>Real biological neurons: - Receive signals through dendrites - Integrate signals in the cell body - Fire an electrical spike down the axon if threshold is exceeded - Transmit signals to other neurons via synapses</p> <p>Artificial neurons capture this essence: weighted inputs (synapses), summation (cell body integration), and activation (neuron firing).</p> <h2 id=activation-functions>Activation Functions<a class=headerlink href=#activation-functions title="Permanent link">&para;</a></h2> <p><strong>Activation functions</strong> introduce nonlinearity into neural networks. Without nonlinearity, stacking multiple layers would be mathematically equivalent to a single layerthe network couldn't learn complex patterns.</p> <h3 id=sigmoid>Sigmoid<a class=headerlink href=#sigmoid title="Permanent link">&para;</a></h3> <p>The sigmoid function was historically popular for its smooth, S-shaped curve:</p> <div class=arithmatex>\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</div> <p><strong>Properties:</strong> - Output range: (0, 1) - Smooth and differentiable - Derivative: <span class=arithmatex>\(\sigma'(z) = \sigma(z)(1 - \sigma(z))\)</span> - Interpretable as probability</p> <p><strong>Drawbacks:</strong> - <strong>Vanishing gradients</strong>: For large <span class=arithmatex>\(|z|\)</span>, gradient approaches zero, slowing learning - <strong>Not zero-centered</strong>: Outputs always positive, causing zig-zagging in gradient descent - <strong>Expensive computation</strong>: Exponential function</p> <h3 id=hyperbolic-tangent-tanh>Hyperbolic Tangent (Tanh)<a class=headerlink href=#hyperbolic-tangent-tanh title="Permanent link">&para;</a></h3> <p><strong>Tanh</strong> is a scaled, shifted sigmoid:</p> <div class=arithmatex>\[\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = \frac{2}{1 + e^{-2z}} - 1\]</div> <p><strong>Properties:</strong> - Output range: (-1, 1) - Zero-centered (better than sigmoid) - Derivative: <span class=arithmatex>\(\tanh'(z) = 1 - \tanh^2(z)\)</span></p> <p><strong>Drawbacks:</strong> - Still suffers from vanishing gradients - Still computationally expensive</p> <h3 id=rectified-linear-unit-relu>Rectified Linear Unit (ReLU)<a class=headerlink href=#rectified-linear-unit-relu title="Permanent link">&para;</a></h3> <p><strong>ReLU</strong> has become the default activation function for hidden layers:</p> <div class=arithmatex>\[\text{ReLU}(z) = \max(0, z) = \begin{cases} z &amp; \text{if } z &gt; 0 \\ 0 &amp; \text{if } z \leq 0 \end{cases}\]</div> <p><strong>Properties:</strong> - Derivative: <span class=arithmatex>\(\text{ReLU}'(z) = \begin{cases} 1 &amp; \text{if } z &gt; 0 \\ 0 &amp; \text{if } z \leq 0 \end{cases}\)</span> - Computationally cheap (simple threshold) - Does not saturate for positive values - Sparse activations (many neurons output zero)</p> <p><strong>Advantages:</strong> - Alleviates vanishing gradient problem - Accelerates convergence (6x faster than sigmoid/tanh in some studies) - Promotes sparse representations</p> <p><strong>Drawbacks:</strong> - <strong>Dying ReLU problem</strong>: Neurons with large negative weights never activate, becoming permanently inactive - Not zero-centered - Not differentiable at <span class=arithmatex>\(z = 0\)</span> (though subgradient works in practice)</p> <h3 id=leaky-relu>Leaky ReLU<a class=headerlink href=#leaky-relu title="Permanent link">&para;</a></h3> <p><strong>Leaky ReLU</strong> addresses the dying ReLU problem by allowing small negative values:</p> <div class=arithmatex>\[\text{LeakyReLU}(z) = \max(\alpha z, z) = \begin{cases} z &amp; \text{if } z &gt; 0 \\ \alpha z &amp; \text{if } z \leq 0 \end{cases}\]</div> <p>where <span class=arithmatex>\(\alpha\)</span> is a small constant (typically 0.01).</p> <p><strong>Properties:</strong> - Derivative: <span class=arithmatex>\(\text{LeakyReLU}'(z) = \begin{cases} 1 &amp; \text{if } z &gt; 0 \\ \alpha &amp; \text{if } z \leq 0 \end{cases}\)</span> - Prevents dying neurons - Still computationally cheap</p> <p><strong>Variants:</strong> - <strong>Parametric ReLU (PReLU)</strong>: Learn <span class=arithmatex>\(\alpha\)</span> during training - <strong>Exponential Linear Unit (ELU)</strong>: Smooth curve for negative values</p> <h3 id=choosing-activation-functions>Choosing Activation Functions<a class=headerlink href=#choosing-activation-functions title="Permanent link">&para;</a></h3> <p><strong>General guidelines:</strong> - <strong>Hidden layers</strong>: ReLU or Leaky ReLU (default choice) - <strong>Output layer (regression)</strong>: Linear (no activation) - <strong>Output layer (binary classification)</strong>: Sigmoid - <strong>Output layer (multiclass classification)</strong>: Softmax</p> <h2 id=network-architecture>Network Architecture<a class=headerlink href=#network-architecture title="Permanent link">&para;</a></h2> <p>A <strong>neural network</strong> consists of layers of interconnected neurons. The <strong>network architecture</strong> defines how many layers exist, how many neurons are in each layer, and how they connect.</p> <h3 id=layer-types>Layer Types<a class=headerlink href=#layer-types title="Permanent link">&para;</a></h3> <p><strong>Input Layer:</strong> The <strong>input layer</strong> receives raw features. It has one neuron per feature dimension and performs no computationit simply passes values to the next layer.</p> <p><strong>Hidden Layers:</strong> <strong>Hidden layers</strong> perform intermediate transformations. A network can have zero, one, or many hidden layers. Each neuron in a hidden layer connects to all neurons in the previous layer (in a <strong>fully connected layer</strong>) and applies:</p> <div class=arithmatex>\[a_j^{(l)} = f\left(\sum_{i} w_{ji}^{(l)} a_i^{(l-1)} + b_j^{(l)}\right)\]</div> <p>where: - <span class=arithmatex>\(a_j^{(l)}\)</span> is activation of neuron <span class=arithmatex>\(j\)</span> in layer <span class=arithmatex>\(l\)</span> - <span class=arithmatex>\(w_{ji}^{(l)}\)</span> is weight from neuron <span class=arithmatex>\(i\)</span> in layer <span class=arithmatex>\(l-1\)</span> to neuron <span class=arithmatex>\(j\)</span> in layer <span class=arithmatex>\(l\)</span> - <span class=arithmatex>\(b_j^{(l)}\)</span> is bias for neuron <span class=arithmatex>\(j\)</span> in layer <span class=arithmatex>\(l\)</span> - <span class=arithmatex>\(f\)</span> is the activation function</p> <p><strong>Output Layer:</strong> The <strong>output layer</strong> produces final predictions. For regression, it typically has one neuron with linear activation. For <span class=arithmatex>\(K\)</span>-class classification, it has <span class=arithmatex>\(K\)</span> neurons with softmax activation.</p> <h3 id=multilayer-perceptron-mlp>Multilayer Perceptron (MLP)<a class=headerlink href=#multilayer-perceptron-mlp title="Permanent link">&para;</a></h3> <p>A <strong>multilayer perceptron</strong> (MLP) is a feedforward neural network with one or more hidden layers. Despite the name, MLPs typically use nonlinear activations (not the perceptron's step function).</p> <p><strong>Example architecture:</strong> - Input layer: 4 neurons (4 features) - Hidden layer 1: 20 neurons (ReLU activation) - Hidden layer 2: 30 neurons (ReLU activation) - Hidden layer 3: 25 neurons (ReLU activation) - Output layer: 3 neurons (softmax activation for 3-class classification)</p> <p>This is a 4-20-30-25-3 architecture with 3 hidden layers.</p> <h3 id=deep-learning>Deep Learning<a class=headerlink href=#deep-learning title="Permanent link">&para;</a></h3> <p><strong>Deep learning</strong> refers to neural networks with multiple hidden layers (typically &gt;2). Deep networks can learn hierarchical representations: - Lower layers learn simple features (edges, textures) - Middle layers combine features into parts (eyes, wheels) - Upper layers recognize high-level concepts (faces, cars)</p> <p>The depth allows learning complex, compositional patterns that shallow networks struggle with.</p> <h2 id=forward-propagation>Forward Propagation<a class=headerlink href=#forward-propagation title="Permanent link">&para;</a></h2> <p><strong>Forward propagation</strong> is the process of computing the network's output given an input. Activations flow forward from input through hidden layers to output.</p> <h3 id=algorithm>Algorithm<a class=headerlink href=#algorithm title="Permanent link">&para;</a></h3> <p>For an <span class=arithmatex>\(L\)</span>-layer network with input <span class=arithmatex>\(\mathbf{x}\)</span>:</p> <ol> <li> <p><strong>Input layer</strong> (<span class=arithmatex>\(l = 0\)</span>): $<span class=arithmatex>\(\mathbf{a}^{(0)} = \mathbf{x}\)</span>$</p> </li> <li> <p><strong>Hidden and output layers</strong> (<span class=arithmatex>\(l = 1, \ldots, L\)</span>): $<span class=arithmatex>\(\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}\)</span>$ $<span class=arithmatex>\(\mathbf{a}^{(l)} = f^{(l)}(\mathbf{z}^{(l)})\)</span>$</p> </li> <li> <p><strong>Output</strong>: $<span class=arithmatex>\(\hat{\mathbf{y}} = \mathbf{a}^{(L)}\)</span>$</p> </li> </ol> <p>where <span class=arithmatex>\(\mathbf{W}^{(l)}\)</span> is the weight matrix for layer <span class=arithmatex>\(l\)</span> and <span class=arithmatex>\(f^{(l)}\)</span> is the activation function.</p> <h3 id=example-computation>Example Computation<a class=headerlink href=#example-computation title="Permanent link">&para;</a></h3> <p>For a simple 2-3-1 network (2 inputs, 3 hidden neurons, 1 output):</p> <p><strong>Input</strong>: <span class=arithmatex>\(\mathbf{x} = [x_1, x_2]^T\)</span></p> <p><strong>Hidden layer</strong>: $<span class=arithmatex>\(z_1^{(1)} = w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}\)</span>$ $<span class=arithmatex>\(z_2^{(1)} = w_{21}^{(1)} x_1 + w_{22}^{(1)} x_2 + b_2^{(1)}\)</span>$ $<span class=arithmatex>\(z_3^{(1)} = w_{31}^{(1)} x_1 + w_{32}^{(1)} x_2 + b_3^{(1)}\)</span>$</p> <div class=arithmatex>\[a_1^{(1)} = \text{ReLU}(z_1^{(1)}), \quad a_2^{(1)} = \text{ReLU}(z_2^{(1)}), \quad a_3^{(1)} = \text{ReLU}(z_3^{(1)})\]</div> <p><strong>Output layer</strong>: $<span class=arithmatex>\(z^{(2)} = w_1^{(2)} a_1^{(1)} + w_2^{(2)} a_2^{(1)} + w_3^{(2)} a_3^{(1)} + b^{(2)}\)</span>$ $<span class=arithmatex>\(\hat{y} = z^{(2)}\)</span>$ (linear activation for regression)</p> <h2 id=loss-functions>Loss Functions<a class=headerlink href=#loss-functions title="Permanent link">&para;</a></h2> <p>Loss functions quantify how well the network's predictions match the true labels. Training minimizes the loss.</p> <h3 id=mean-squared-error-mse>Mean Squared Error (MSE)<a class=headerlink href=#mean-squared-error-mse title="Permanent link">&para;</a></h3> <p>For regression, <strong>mean squared error</strong> is commonly used:</p> <div class=arithmatex>\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</div> <p>MSE penalizes large errors heavily due to the squaring.</p> <h3 id=cross-entropy-loss>Cross-Entropy Loss<a class=headerlink href=#cross-entropy-loss title="Permanent link">&para;</a></h3> <p>For classification, <strong>cross-entropy loss</strong> (also called log-loss) measures the difference between predicted and true probability distributions.</p> <p><strong>Binary cross-entropy</strong> (for 2 classes): $<span class=arithmatex>\(\text{Loss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i)]\)</span>$</p> <p><strong>Categorical cross-entropy</strong> (for <span class=arithmatex>\(K\)</span> classes): $<span class=arithmatex>\(\text{Loss} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_{ik} \log \hat{y}_{ik}\)</span>$</p> <p>where <span class=arithmatex>\(y_{ik} = 1\)</span> if sample <span class=arithmatex>\(i\)</span> belongs to class <span class=arithmatex>\(k\)</span>, otherwise 0 (one-hot encoding).</p> <p>Cross-entropy loss combined with softmax output forms a numerically stable, theoretically motivated framework for classification.</p> <h2 id=backpropagation>Backpropagation<a class=headerlink href=#backpropagation title="Permanent link">&para;</a></h2> <p><strong>Backpropagation</strong> (short for "backward propagation of errors") computes gradients of the loss with respect to all weights and biases. These gradients guide parameter updates during training.</p> <h3 id=the-chain-rule>The Chain Rule<a class=headerlink href=#the-chain-rule title="Permanent link">&para;</a></h3> <p>Backpropagation applies the chain rule from calculus to efficiently compute gradients layer by layer, moving backward from output to input.</p> <p>For a simple network with loss <span class=arithmatex>\(L\)</span>, output <span class=arithmatex>\(\hat{y}\)</span>, and intermediate value <span class=arithmatex>\(z\)</span>:</p> <div class=arithmatex>\[\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}\]</div> <h3 id=backpropagation-algorithm>Backpropagation Algorithm<a class=headerlink href=#backpropagation-algorithm title="Permanent link">&para;</a></h3> <p>Starting from the output layer and moving backward:</p> <ol> <li> <p><strong>Output layer gradient</strong>: $<span class=arithmatex>\(\delta^{(L)} = \frac{\partial L}{\partial \mathbf{a}^{(L)}} \odot f'^{(L)}(\mathbf{z}^{(L)})\)</span>$</p> </li> <li> <p><strong>Hidden layer gradients</strong> (for <span class=arithmatex>\(l = L-1, \ldots, 1\)</span>): $<span class=arithmatex>\(\delta^{(l)} = [(\mathbf{W}^{(l+1)})^T \delta^{(l+1)}] \odot f'^{(l)}(\mathbf{z}^{(l)})\)</span>$</p> </li> <li> <p><strong>Weight and bias gradients</strong>: $<span class=arithmatex>\(\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T\)</span>$ $<span class=arithmatex>\(\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}\)</span>$</p> </li> </ol> <p>where <span class=arithmatex>\(\odot\)</span> denotes element-wise multiplication.</p> <h3 id=why-backpropagation-matters>Why Backpropagation Matters<a class=headerlink href=#why-backpropagation-matters title="Permanent link">&para;</a></h3> <p>Before backpropagation, training neural networks required numerical gradient estimation (finite differences), which was computationally prohibitive for large networks. Backpropagation enables efficient gradient computation, making deep learning practical.</p> <h2 id=gradient-descent>Gradient Descent<a class=headerlink href=#gradient-descent title="Permanent link">&para;</a></h2> <p><strong>Gradient descent</strong> is the optimization algorithm that updates weights to minimize the loss. The update rule is:</p> <div class=arithmatex>\[w_{new} = w_{old} - \eta \frac{\partial L}{\partial w}\]</div> <p>where <span class=arithmatex>\(\eta\)</span> is the <strong>learning rate</strong>, controlling step size.</p> <h3 id=batch-gradient-descent>Batch Gradient Descent<a class=headerlink href=#batch-gradient-descent title="Permanent link">&para;</a></h3> <p>Standard gradient descent computes gradients using the entire training set:</p> <ol> <li><strong>Forward propagation</strong>: Compute predictions for all samples</li> <li><strong>Compute loss</strong>: Average loss over all samples</li> <li><strong>Backpropagation</strong>: Compute gradients averaging over all samples</li> <li><strong>Update weights</strong>: Apply gradient descent update</li> </ol> <p>This is stable but slow for large datasets.</p> <h3 id=stochastic-gradient-descent-sgd>Stochastic Gradient Descent (SGD)<a class=headerlink href=#stochastic-gradient-descent-sgd title="Permanent link">&para;</a></h3> <p><strong>Stochastic gradient descent</strong> updates weights after each individual sample:</p> <ol> <li>Randomly shuffle training data</li> <li>For each sample:</li> <li>Forward propagation</li> <li>Compute loss for this sample</li> <li>Backpropagation</li> <li>Update weights</li> </ol> <p><strong>Advantages:</strong> - Much faster per update - Can escape local minima due to noise - Enables online learning (update as new data arrives)</p> <p><strong>Disadvantages:</strong> - Noisy gradients cause erratic convergence - Requires careful learning rate tuning</p> <h3 id=mini-batch-gradient-descent>Mini-Batch Gradient Descent<a class=headerlink href=#mini-batch-gradient-descent title="Permanent link">&para;</a></h3> <p><strong>Mini-batch gradient descent</strong> strikes a balance by updating on small batches of samples:</p> <ol> <li><strong>Batch size</strong> (e.g., 32, 64, 128): Number of samples per update</li> <li>For each mini-batch:</li> <li>Forward propagation on batch</li> <li>Compute average loss over batch</li> <li>Backpropagation</li> <li>Update weights</li> </ol> <p><strong>Advantages:</strong> - More stable than SGD, faster than full batch - Efficient matrix operations (GPUs excel at batch processing) - Reduces gradient variance while maintaining speed</p> <p><strong>Batch processing</strong> enables efficient use of modern hardware accelerators.</p> <h3 id=learning-rate>Learning Rate<a class=headerlink href=#learning-rate title="Permanent link">&para;</a></h3> <p>The <strong>learning rate</strong> <span class=arithmatex>\(\eta\)</span> critically affects training:</p> <ul> <li><strong>Too small</strong>: Slow convergence, may get stuck</li> <li><strong>Too large</strong>: Oscillation, divergence, missing minimum</li> <li><strong>Just right</strong>: Fast, stable convergence</li> </ul> <p><strong>Learning rate scheduling</strong> adaptively adjusts <span class=arithmatex>\(\eta\)</span> during training: - <strong>Step decay</strong>: Reduce <span class=arithmatex>\(\eta\)</span> by factor (e.g., 0.1) every <span class=arithmatex>\(N\)</span> epochs - <strong>Exponential decay</strong>: <span class=arithmatex>\(\eta(t) = \eta_0 e^{-kt}\)</span> - <strong>1/t decay</strong>: <span class=arithmatex>\(\eta(t) = \eta_0 / (1 + kt)\)</span> - <strong>Adaptive methods</strong> (Adam, RMSprop): Automatically adjust per-parameter learning rates</p> <h3 id=epochs>Epochs<a class=headerlink href=#epochs title="Permanent link">&para;</a></h3> <p>An <strong>epoch</strong> is one complete pass through the entire training dataset. Training typically runs for many epochs (10s to 1000s), with the network gradually improving as it sees data repeatedly.</p> <h2 id=weight-initialization>Weight Initialization<a class=headerlink href=#weight-initialization title="Permanent link">&para;</a></h2> <p><strong>Weight initialization</strong> significantly affects training dynamics. Poor initialization can prevent learning entirely.</p> <h3 id=why-initialization-matters>Why Initialization Matters<a class=headerlink href=#why-initialization-matters title="Permanent link">&para;</a></h3> <ul> <li><strong>All zeros</strong>: Neurons in a layer behave identically (symmetry problem)</li> <li><strong>Too large</strong>: Activations explode, gradients explode</li> <li><strong>Too small</strong>: Activations vanish, gradients vanish</li> </ul> <h3 id=xavier-glorot-initialization>Xavier (Glorot) Initialization<a class=headerlink href=#xavier-glorot-initialization title="Permanent link">&para;</a></h3> <p><strong>Xavier initialization</strong> keeps variance of activations and gradients stable across layers. For a layer with <span class=arithmatex>\(n_{in}\)</span> inputs and <span class=arithmatex>\(n_{out}\)</span> outputs:</p> <div class=arithmatex>\[w \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right)\]</div> <p>or uniform variant:</p> <div class=arithmatex>\[w \sim \text{Uniform}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)\]</div> <p><strong>Best for</strong>: Tanh and sigmoid activations</p> <h3 id=he-initialization>He Initialization<a class=headerlink href=#he-initialization title="Permanent link">&para;</a></h3> <p><strong>He initialization</strong> accounts for ReLU's characteristics (kills negative values):</p> <div class=arithmatex>\[w \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)\]</div> <p><strong>Best for</strong>: ReLU and Leaky ReLU activations</p> <p>Proper initialization is crucial for training deep networks.</p> <h2 id=training-challenges>Training Challenges<a class=headerlink href=#training-challenges title="Permanent link">&para;</a></h2> <h3 id=vanishing-gradients>Vanishing Gradients<a class=headerlink href=#vanishing-gradients title="Permanent link">&para;</a></h3> <p>The <strong>vanishing gradient problem</strong> occurs when gradients become extremely small as they propagate backward through many layers. This causes early layers to learn very slowly or not at all.</p> <p><strong>Causes:</strong> - Sigmoid/tanh activations saturate (gradients  0) - Deep networks multiply many small gradients</p> <p><strong>Solutions:</strong> - Use ReLU activations - Skip connections (ResNets) - Batch normalization - Proper weight initialization</p> <h3 id=exploding-gradients>Exploding Gradients<a class=headerlink href=#exploding-gradients title="Permanent link">&para;</a></h3> <p>The <strong>exploding gradient problem</strong> is the opposite: gradients become extremely large, causing numerical instability and divergence.</p> <p><strong>Causes:</strong> - Poor weight initialization - Deep networks multiply many large gradients</p> <p><strong>Solutions:</strong> - Gradient clipping (cap gradient magnitude) - Proper weight initialization - Batch normalization</p> <h2 id=regularization-techniques>Regularization Techniques<a class=headerlink href=#regularization-techniques title="Permanent link">&para;</a></h2> <h3 id=dropout>Dropout<a class=headerlink href=#dropout title="Permanent link">&para;</a></h3> <p><strong>Dropout</strong> randomly sets a fraction of neuron activations to zero during training. For example, with dropout rate 0.5, each neuron has a 50% chance of being "dropped."</p> <p><strong>Effect:</strong> - Prevents co-adaptation (neurons relying on specific other neurons) - Acts like training an ensemble of networks - Significantly reduces overfitting</p> <p><strong>Implementation:</strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># During training</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=k>if</span> <span class=n>training</span><span class=p>:</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>    <span class=n>mask</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>activations</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>    <span class=n>activations</span> <span class=o>*=</span> <span class=n>mask</span> <span class=o>/</span> <span class=n>keep_prob</span>  <span class=c1># Scale to maintain expected value</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=c1># During inference</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=c1># Use all activations (no dropout)</span>
</span></code></pre></div></p> <p>Dropout is typically applied to fully connected layers, not convolutional layers.</p> <h3 id=early-stopping>Early Stopping<a class=headerlink href=#early-stopping title="Permanent link">&para;</a></h3> <p><strong>Early stopping</strong> monitors validation loss during training and stops when validation performance stops improving. This prevents overfitting by avoiding overtraining.</p> <p><strong>Algorithm:</strong> 1. Train network and evaluate on validation set after each epoch 2. Track best validation loss seen so far 3. If validation loss doesn't improve for <span class=arithmatex>\(N\)</span> consecutive epochs (patience), stop training 4. Return weights from epoch with best validation loss</p> <p>Early stopping is a simple, effective regularization technique that requires no hyperparameter tuning beyond patience.</p> <h2 id=universal-approximation-theorem>Universal Approximation Theorem<a class=headerlink href=#universal-approximation-theorem title="Permanent link">&para;</a></h2> <p>The <strong>universal approximation theorem</strong> states that a neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary accuracy, given enough neurons.</p> <p><strong>Implications:</strong> - Neural networks are theoretically capable of learning any function - Shallow networks can represent complex functions but may require exponentially many neurons - Deep networks learn hierarchical representations more efficiently</p> <p><strong>Important caveats:</strong> - Theorem guarantees existence, not learnability (training may not find the solution) - Says nothing about generalization - Doesn't specify how many neurons are needed</p> <h2 id=neural-networks-in-practice>Neural Networks in Practice<a class=headerlink href=#neural-networks-in-practice title="Permanent link">&para;</a></h2> <h3 id=building-a-neural-network-with-scikit-learn>Building a Neural Network with Scikit-Learn<a class=headerlink href=#building-a-neural-network-with-scikit-learn title="Permanent link">&para;</a></h3> <p>Let's apply MLPClassifier to the Iris dataset:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.neural_network</span><span class=w> </span><span class=kn>import</span> <span class=n>MLPClassifier</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>confusion_matrix</span><span class=p>,</span> <span class=n>accuracy_score</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=kn>import</span><span class=w> </span><span class=nn>seaborn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>sns</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=c1># Load iris dataset</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=n>url</span> <span class=o>=</span> <span class=s2>&quot;https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/iris.csv&quot;</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a><span class=n>iris_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a><span class=c1># Examine feature correlations</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a><span class=n>feature_names</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;sepal_length&#39;</span><span class=p>,</span> <span class=s1>&#39;sepal_width&#39;</span><span class=p>,</span> <span class=s1>&#39;petal_length&#39;</span><span class=p>,</span> <span class=s1>&#39;petal_width&#39;</span><span class=p>]</span>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a><span class=n>correlation_matrix</span> <span class=o>=</span> <span class=n>iris_df</span><span class=p>[</span><span class=n>feature_names</span><span class=p>]</span><span class=o>.</span><span class=n>corr</span><span class=p>()</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span><span id=__span-1-18><a id=__codelineno-1-18 name=__codelineno-1-18 href=#__codelineno-1-18></a><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>correlation_matrix</span><span class=p>,</span> <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;coolwarm&#39;</span><span class=p>,</span> <span class=n>center</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-1-19><a id=__codelineno-1-19 name=__codelineno-1-19 href=#__codelineno-1-19></a><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Feature Correlations&#39;</span><span class=p>)</span>
</span><span id=__span-1-20><a id=__codelineno-1-20 name=__codelineno-1-20 href=#__codelineno-1-20></a><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></code></pre></div> <p>The correlation matrix reveals strong positive correlation between petal length and petal width (0.96), suggesting these features carry similar information. Sepal width and length have weak negative correlation.</p> <h3 id=training-the-network>Training the Network<a class=headerlink href=#training-the-network title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1># Prepare data</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>X</span> <span class=o>=</span> <span class=n>iris_df</span><span class=o>.</span><span class=n>loc</span><span class=p>[:,</span> <span class=n>feature_names</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>y</span> <span class=o>=</span> <span class=n>iris_df</span><span class=o>.</span><span class=n>loc</span><span class=p>[:,</span> <span class=s1>&#39;species&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=c1># Train/test split</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=c1># Create neural network with 3 hidden layers</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=c1># Architecture: 4 inputs  20 neurons  30 neurons  25 neurons  3 outputs</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=n>mlp</span> <span class=o>=</span> <span class=n>MLPClassifier</span><span class=p>(</span><span class=n>hidden_layer_sizes</span><span class=o>=</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>30</span><span class=p>,</span> <span class=mi>25</span><span class=p>),</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a>                    <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a>                    <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a>                    <span class=n>solver</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a>                    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a>
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a><span class=c1># Train</span>
</span><span id=__span-2-17><a id=__codelineno-2-17 name=__codelineno-2-17 href=#__codelineno-2-17></a><span class=n>mlp</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span><span id=__span-2-18><a id=__codelineno-2-18 name=__codelineno-2-18 href=#__codelineno-2-18></a>
</span><span id=__span-2-19><a id=__codelineno-2-19 name=__codelineno-2-19 href=#__codelineno-2-19></a><span class=c1># Predictions</span>
</span><span id=__span-2-20><a id=__codelineno-2-20 name=__codelineno-2-20 href=#__codelineno-2-20></a><span class=n>y_pred</span> <span class=o>=</span> <span class=n>mlp</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</span><span id=__span-2-21><a id=__codelineno-2-21 name=__codelineno-2-21 href=#__codelineno-2-21></a>
</span><span id=__span-2-22><a id=__codelineno-2-22 name=__codelineno-2-22 href=#__codelineno-2-22></a><span class=c1># Evaluate</span>
</span><span id=__span-2-23><a id=__codelineno-2-23 name=__codelineno-2-23 href=#__codelineno-2-23></a><span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span><span id=__span-2-24><a id=__codelineno-2-24 name=__codelineno-2-24 href=#__codelineno-2-24></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Test Accuracy: </span><span class=si>{</span><span class=n>accuracy</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-2-25><a id=__codelineno-2-25 name=__codelineno-2-25 href=#__codelineno-2-25></a>
</span><span id=__span-2-26><a id=__codelineno-2-26 name=__codelineno-2-26 href=#__codelineno-2-26></a><span class=c1># Confusion matrix</span>
</span><span id=__span-2-27><a id=__codelineno-2-27 name=__codelineno-2-27 href=#__codelineno-2-27></a><span class=n>cm</span> <span class=o>=</span> <span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span><span id=__span-2-28><a id=__codelineno-2-28 name=__codelineno-2-28 href=#__codelineno-2-28></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Confusion Matrix:&quot;</span><span class=p>)</span>
</span><span id=__span-2-29><a id=__codelineno-2-29 name=__codelineno-2-29 href=#__codelineno-2-29></a><span class=nb>print</span><span class=p>(</span><span class=n>cm</span><span class=p>)</span>
</span></code></pre></div> <h3 id=evaluating-multiple-runs>Evaluating Multiple Runs<a class=headerlink href=#evaluating-multiple-runs title="Permanent link">&para;</a></h3> <p>Neural network training is stochastic, so results vary across runs:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Run multiple times to assess stability</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>scores</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>20</span><span class=p>):</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>    <span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=n>i</span><span class=p>)</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>    <span class=n>mlp</span> <span class=o>=</span> <span class=n>MLPClassifier</span><span class=p>(</span><span class=n>hidden_layer_sizes</span><span class=o>=</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>30</span><span class=p>,</span> <span class=mi>25</span><span class=p>),</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=n>i</span><span class=p>)</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>    <span class=n>mlp</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>mlp</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    <span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>    <span class=n>scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>accuracy</span><span class=p>)</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Average accuracy: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Std deviation: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Min: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>, Max: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>This reveals the stability (or variability) of the model across different random initializations and data splits.</p> <h3 id=hyperparameter-tuning>Hyperparameter Tuning<a class=headerlink href=#hyperparameter-tuning title="Permanent link">&para;</a></h3> <p>Key hyperparameters to tune:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>GridSearchCV</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=c1># Define hyperparameter grid</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>param_grid</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>    <span class=s1>&#39;hidden_layer_sizes&#39;</span><span class=p>:</span> <span class=p>[(</span><span class=mi>50</span><span class=p>,),</span> <span class=p>(</span><span class=mi>100</span><span class=p>,),</span> <span class=p>(</span><span class=mi>50</span><span class=p>,</span> <span class=mi>50</span><span class=p>),</span> <span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>50</span><span class=p>)],</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>    <span class=s1>&#39;activation&#39;</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=s1>&#39;tanh&#39;</span><span class=p>],</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>    <span class=s1>&#39;alpha&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.0001</span><span class=p>,</span> <span class=mf>0.001</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>],</span>  <span class=c1># L2 regularization strength</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a>    <span class=s1>&#39;learning_rate_init&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.001</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>]</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a><span class=p>}</span>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a><span class=c1># Grid search with cross-validation</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a><span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>MLPClassifier</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>),</span> <span class=n>param_grid</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span><span class=p>)</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a><span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a>
</span><span id=__span-4-15><a id=__codelineno-4-15 name=__codelineno-4-15 href=#__codelineno-4-15></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Best parameters:&quot;</span><span class=p>,</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
</span><span id=__span-4-16><a id=__codelineno-4-16 name=__codelineno-4-16 href=#__codelineno-4-16></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Best CV score:&quot;</span><span class=p>,</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
</span><span id=__span-4-17><a id=__codelineno-4-17 name=__codelineno-4-17 href=#__codelineno-4-17></a>
</span><span id=__span-4-18><a id=__codelineno-4-18 name=__codelineno-4-18 href=#__codelineno-4-18></a><span class=c1># Evaluate on test set</span>
</span><span id=__span-4-19><a id=__codelineno-4-19 name=__codelineno-4-19 href=#__codelineno-4-19></a><span class=n>best_model</span> <span class=o>=</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_estimator_</span>
</span><span id=__span-4-20><a id=__codelineno-4-20 name=__codelineno-4-20 href=#__codelineno-4-20></a><span class=n>test_accuracy</span> <span class=o>=</span> <span class=n>best_model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span><span id=__span-4-21><a id=__codelineno-4-21 name=__codelineno-4-21 href=#__codelineno-4-21></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Test accuracy: </span><span class=si>{</span><span class=n>test_accuracy</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=advanced-architectures>Advanced Architectures<a class=headerlink href=#advanced-architectures title="Permanent link">&para;</a></h2> <h3 id=pooling-layers>Pooling Layers<a class=headerlink href=#pooling-layers title="Permanent link">&para;</a></h3> <p><strong>Pooling layers</strong> reduce spatial dimensions in convolutional networks by downsampling:</p> <ul> <li><strong>Max pooling</strong>: Take maximum value in each region</li> <li><strong>Average pooling</strong>: Take average value in each region</li> </ul> <p>Pooling provides translation invariance and reduces computational cost.</p> <h3 id=freezing-layers>Freezing Layers<a class=headerlink href=#freezing-layers title="Permanent link">&para;</a></h3> <p><strong>Freezing layers</strong> prevents weight updates during training. This is useful for:</p> <ul> <li><strong>Transfer learning</strong>: Freeze pretrained layers, train only final layers</li> <li><strong>Feature extraction</strong>: Use frozen network as feature extractor</li> <li><strong>Progressive training</strong>: Gradually unfreeze layers</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Conceptual example (PyTorch syntax)</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>layer1</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>  <span class=c1># Freeze layer1</span>
</span></code></pre></div> <h2 id=interactive-visualization-neural-network-architecture>Interactive Visualization: Neural Network Architecture<a class=headerlink href=#interactive-visualization-neural-network-architecture title="Permanent link">&para;</a></h2> <p>Build and explore different neural network architectures:</p> <iframe src=../../sims/network-architecture-visualizer/network-architecture.html width=100% height=750 frameborder=0></iframe> <h2 id=interactive-visualization-activation-functions>Interactive Visualization: Activation Functions<a class=headerlink href=#interactive-visualization-activation-functions title="Permanent link">&para;</a></h2> <p>Compare different activation functions and their properties:</p> <iframe src=../../sims/activation-functions/activation-functions.html width=100% height=850 frameborder=0></iframe> <h2 id=summary_1>Summary<a class=headerlink href=#summary_1 title="Permanent link">&para;</a></h2> <p>Neural networks are powerful function approximators built from layers of artificial neurons. Each neuron computes a weighted sum of inputs, adds a bias, and applies an activation function. Activation functions like ReLU, tanh, and sigmoid introduce essential nonlinearity.</p> <p>Forward propagation computes predictions by passing inputs through successive layers. Backpropagation efficiently computes gradients using the chain rule, enabling gradient descent optimization. Stochastic gradient descent and mini-batch variants balance speed and stability.</p> <p>Weight initialization (Xavier for tanh/sigmoid, He for ReLU) prevents vanishing and exploding gradients. Regularization techniques like dropout and early stopping combat overfitting. The universal approximation theorem guarantees that neural networks can represent any function, though depth enables more efficient learning.</p> <p>Modern deep learning frameworks automate much of the complexity, but understanding these fundamentalsneurons, activations, forward/back propagation, gradient descent, and training challengesprovides the foundation for effectively applying and debugging neural networks.</p> <h2 id=key-takeaways>Key Takeaways<a class=headerlink href=#key-takeaways title="Permanent link">&para;</a></h2> <ol> <li><strong>Artificial neurons</strong> compute weighted sums plus bias, then apply activation functions</li> <li><strong>Activation functions</strong> introduce nonlinearity; ReLU is the default for hidden layers</li> <li><strong>Network architecture</strong> defines layers (input, hidden, output) and connections</li> <li><strong>Forward propagation</strong> computes outputs by passing activations through layers</li> <li><strong>Backpropagation</strong> efficiently computes gradients using the chain rule</li> <li><strong>Gradient descent</strong> updates weights to minimize loss; SGD and mini-batch variants balance speed and stability</li> <li><strong>Learning rate</strong> controls step size; too large causes divergence, too small causes slow convergence</li> <li><strong>Weight initialization</strong> (Xavier, He) prevents vanishing/exploding gradients</li> <li><strong>Dropout</strong> and <strong>early stopping</strong> prevent overfitting</li> <li><strong>Vanishing gradients</strong> occur with sigmoid/tanh in deep networks; ReLU alleviates this</li> <li><strong>Batch size</strong> affects gradient variance and computational efficiency</li> <li><strong>Universal approximation theorem</strong> guarantees representation capacity</li> </ol> <h2 id=further-reading>Further Reading<a class=headerlink href=#further-reading title="Permanent link">&para;</a></h2> <ul> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em> (Chapters 6-8)</li> <li>Nielsen, M. (2015). <em>Neural Networks and Deep Learning</em> (Free online book)</li> <li>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). "Deep learning." <em>Nature</em>, 521, 436-444</li> <li>Rumelhart, D., Hinton, G., &amp; Williams, R. (1986). "Learning representations by back-propagating errors." <em>Nature</em>, 323, 533-536</li> <li>Glorot, X., &amp; Bengio, Y. (2010). "Understanding the difficulty of training deep feedforward neural networks." <em>AISTATS</em></li> </ul> <h2 id=exercises>Exercises<a class=headerlink href=#exercises title="Permanent link">&para;</a></h2> <ol> <li> <p><strong>Manual Forward Propagation</strong>: Given a 2-3-1 network with specific weights and biases, manually compute the output for input <span class=arithmatex>\([1, 2]^T\)</span> using ReLU activations.</p> </li> <li> <p><strong>Activation Function Analysis</strong>: Plot sigmoid, tanh, ReLU, and Leaky ReLU (=0.01) and their derivatives on the same graph. At what input values does each function saturate?</p> </li> <li> <p><strong>Backpropagation by Hand</strong>: For a simple 2-2-1 network, compute gradients with respect to all weights and biases for a single training example using MSE loss.</p> </li> <li> <p><strong>Learning Rate Experiment</strong>: Train a network on a small dataset with learning rates [0.0001, 0.001, 0.01, 0.1, 1.0]. Plot training loss vs epoch for each. Which converges fastest? Which diverges?</p> </li> <li> <p><strong>Architecture Comparison</strong>: Compare 1-layer (4-50-3), 2-layer (4-25-25-3), and 3-layer (4-20-20-20-3) networks on Iris. Which achieves best test accuracy? Why might deeper not always be better for small datasets?</p> </li> <li> <p><strong>Dropout Impact</strong>: Train identical networks with dropout rates [0, 0.2, 0.5, 0.8]. Plot training vs validation accuracy. How does dropout affect the train-validation gap?</p> </li> <li> <p><strong>Weight Initialization</strong>: Initialize a deep network (10 layers) with all zeros, Xavier, He, and random uniform [-1, 1]. Plot activation distributions after forward pass. Which initializations cause saturation or vanishing activations?</p> </li> </ol> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 | CC BY-NC-SA 4.0 DEED </div> </div> <div class=md-social> <a href=https://github.com/AnvithPothula target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/anvith-pothula target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>