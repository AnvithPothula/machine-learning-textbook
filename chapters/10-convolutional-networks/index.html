<!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Specialized deep learning architectures for processing images through convolution, pooling, and spatial hierarchies"><meta name=author content="Anvith Pothula"><link href=https://example.com/chapters/10-convolutional-networks/ rel=canonical><link href=../09-neural-networks/quiz/ rel=prev><link href=quiz/ rel=next><link rel=icon href=../../img/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Convolutional Neural Networks for Computer Vision - Machine Learning - Algorithms and Applications</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#convolutional-neural-networks-for-computer-vision class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Machine Learning - Algorithms and Applications" class="md-header__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <img src=../../img/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning - Algorithms and Applications </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Convolutional Neural Networks for Computer Vision </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Machine Learning - Algorithms and Applications" class="md-nav__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <img src=../../img/logo.png alt=logo> </a> Machine Learning - Algorithms and Applications </label> <div class=md-nav__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../../course-description/ class=md-nav__link> <span class=md-ellipsis> Course Description </span> </a> </li> <li class=md-nav__item> <a href=../../faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> <li class=md-nav__item> <a href=../../glossary/ class=md-nav__link> <span class=md-ellipsis> Glossary </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <div class="md-nav__link md-nav__container"> <a href=../01-intro-to-ml-fundamentals/ class="md-nav__link "> <span class=md-ellipsis> 1. ML Fundamentals </span> </a> <label class="md-nav__link " for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> 1. ML Fundamentals </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01-intro-to-ml-fundamentals/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_3> <div class="md-nav__link md-nav__container"> <a href=../02-k-nearest-neighbors/ class="md-nav__link "> <span class=md-ellipsis> 2. K-Nearest Neighbors </span> </a> <label class="md-nav__link " for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> 2. K-Nearest Neighbors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../02-k-nearest-neighbors/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_4> <div class="md-nav__link md-nav__container"> <a href=../03-decision-trees/ class="md-nav__link "> <span class=md-ellipsis> 3. Decision Trees </span> </a> <label class="md-nav__link " for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> 3. Decision Trees </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../03-decision-trees/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_5> <div class="md-nav__link md-nav__container"> <a href=../04-logistic-regression/ class="md-nav__link "> <span class=md-ellipsis> 4. Logistic Regression </span> </a> <label class="md-nav__link " for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> 4. Logistic Regression </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../04-logistic-regression/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_6> <div class="md-nav__link md-nav__container"> <a href=../05-regularization/ class="md-nav__link "> <span class=md-ellipsis> 5. Regularization </span> </a> <label class="md-nav__link " for=__nav_5_6 id=__nav_5_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> 5. Regularization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../05-regularization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_7> <div class="md-nav__link md-nav__container"> <a href=../06-support-vector-machines/ class="md-nav__link "> <span class=md-ellipsis> 6. Support Vector Machines </span> </a> <label class="md-nav__link " for=__nav_5_7 id=__nav_5_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> 6. Support Vector Machines </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../06-support-vector-machines/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_8> <div class="md-nav__link md-nav__container"> <a href=../07-k-means-clustering/ class="md-nav__link "> <span class=md-ellipsis> 7. K-Means Clustering </span> </a> <label class="md-nav__link " for=__nav_5_8 id=__nav_5_8_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> 7. K-Means Clustering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../07-k-means-clustering/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_9> <div class="md-nav__link md-nav__container"> <a href=../08-data-preprocessing/ class="md-nav__link "> <span class=md-ellipsis> 8. Data Preprocessing </span> </a> <label class="md-nav__link " for=__nav_5_9 id=__nav_5_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_9_label aria-expanded=false> <label class=md-nav__title for=__nav_5_9> <span class="md-nav__icon md-icon"></span> 8. Data Preprocessing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../08-data-preprocessing/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_10> <div class="md-nav__link md-nav__container"> <a href=../09-neural-networks/ class="md-nav__link "> <span class=md-ellipsis> 9. Neural Networks </span> </a> <label class="md-nav__link " for=__nav_5_10 id=__nav_5_10_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_10_label aria-expanded=false> <label class=md-nav__title for=__nav_5_10> <span class="md-nav__icon md-icon"></span> 9. Neural Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../09-neural-networks/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_11 checked> <div class="md-nav__link md-nav__container"> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 10. Convolutional Networks </span> </a> <label class="md-nav__link md-nav__link--active" for=__nav_5_11 id=__nav_5_11_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_11_label aria-expanded=true> <label class=md-nav__title for=__nav_5_11> <span class="md-nav__icon md-icon"></span> 10. Convolutional Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_12> <div class="md-nav__link md-nav__container"> <a href=../11-transfer-learning/ class="md-nav__link "> <span class=md-ellipsis> 11. Transfer Learning </span> </a> <label class="md-nav__link " for=__nav_5_12 id=__nav_5_12_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_12_label aria-expanded=false> <label class=md-nav__title for=__nav_5_12> <span class="md-nav__icon md-icon"></span> 11. Transfer Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../11-transfer-learning/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_13> <div class="md-nav__link md-nav__container"> <a href=../12-evaluation-optimization/ class="md-nav__link "> <span class=md-ellipsis> 12. Evaluation &amp; Optimization </span> </a> <label class="md-nav__link " for=__nav_5_13 id=__nav_5_13_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_13_label aria-expanded=false> <label class=md-nav__title for=__nav_5_13> <span class="md-nav__icon md-icon"></span> 12. Evaluation &amp; Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../12-evaluation-optimization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <div class="md-nav__link md-nav__container"> <a href=../../sims/ class="md-nav__link "> <span class=md-ellipsis> MicroSims </span> </a> <label class="md-nav__link " for=__nav_6 id=__nav_6_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> MicroSims </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../sims/activation-functions/ class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=../../sims/categorical-encoding-explorer/ class=md-nav__link> <span class=md-ellipsis> Categorical Encoding Explorer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/cnn-architecture/ class=md-nav__link> <span class=md-ellipsis> CNN Architecture </span> </a> </li> <li class=md-nav__item> <a href=../../sims/confusion-matrix-explorer/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix Explorer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/convolution-operation/ class=md-nav__link> <span class=md-ellipsis> Convolution Operation </span> </a> </li> <li class=md-nav__item> <a href=../../sims/distance-metrics/ class=md-nav__link> <span class=md-ellipsis> Distance Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../sims/entropy-gini-comparison/ class=md-nav__link> <span class=md-ellipsis> Entropy-Gini Comparison </span> </a> </li> <li class=md-nav__item> <a href=../../sims/feature-scaling-visualizer/ class=md-nav__link> <span class=md-ellipsis> Feature Scaling Visualizer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/k-selection-simulator/ class=md-nav__link> <span class=md-ellipsis> K-Selection Simulator </span> </a> </li> <li class=md-nav__item> <a href=../../sims/kfold-cross-validation/ class=md-nav__link> <span class=md-ellipsis> K-Fold Cross Validation </span> </a> </li> <li class=md-nav__item> <a href=../../sims/lasso-regression-geometry/ class=md-nav__link> <span class=md-ellipsis> Lasso Regression Geometry </span> </a> </li> <li class=md-nav__item> <a href=../../sims/network-architecture-visualizer/ class=md-nav__link> <span class=md-ellipsis> Network Architecture Visualizer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/ridge-regression-geometry/ class=md-nav__link> <span class=md-ellipsis> Ridge Regression Geometry </span> </a> </li> <li class=md-nav__item> <a href=../../sims/roc-curve-comparison/ class=md-nav__link> <span class=md-ellipsis> ROC Curve Comparison </span> </a> </li> <li class=md-nav__item> <a href=../../sims/sigmoid-explorer/ class=md-nav__link> <span class=md-ellipsis> Sigmoid Explorer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/svm-margin-maximization/ class=md-nav__link> <span class=md-ellipsis> SVM Margin Maximization </span> </a> </li> <li class=md-nav__item> <a href=../../sims/training-validation-curves/ class=md-nav__link> <span class=md-ellipsis> Training Validation Curves </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <div class="md-nav__link md-nav__container"> <a href=../../learning-graph/ class="md-nav__link "> <span class=md-ellipsis> Learning Graph </span> </a> <label class="md-nav__link " for=__nav_7 id=__nav_7_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Learning Graph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../sims/graph-viewer/ class=md-nav__link> <span class=md-ellipsis> Graph Viewer </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/course-description-assessment/ class=md-nav__link> <span class=md-ellipsis> Course Description Assessment </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/concept-list/ class=md-nav__link> <span class=md-ellipsis> Concept List </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/concept-taxonomy/ class=md-nav__link> <span class=md-ellipsis> Concept Taxonomy </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/learning-graph.csv class=md-nav__link> <span class=md-ellipsis> Learning Graph (CSV) </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/learning-graph.json class=md-nav__link> <span class=md-ellipsis> Learning Graph (JSON) </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/quality-metrics/ class=md-nav__link> <span class=md-ellipsis> Quality Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/taxonomy-distribution/ class=md-nav__link> <span class=md-ellipsis> Taxonomy Distribution </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/glossary-quality-report/ class=md-nav__link> <span class=md-ellipsis> Glossary Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/faq-quality-report/ class=md-nav__link> <span class=md-ellipsis> FAQ Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/faq-coverage-gaps/ class=md-nav__link> <span class=md-ellipsis> FAQ Coverage Gaps </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/book-metrics/ class=md-nav__link> <span class=md-ellipsis> Book Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/chapter-metrics/ class=md-nav__link> <span class=md-ellipsis> Chapter Metrics </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> <li class=md-nav__item> <a href=../../license/ class=md-nav__link> <span class=md-ellipsis> License </span> </a> </li> <li class=md-nav__item> <a href=../../contact/ class=md-nav__link> <span class=md-ellipsis> Contact </span> </a> </li> <li class=md-nav__item> <a href=../../feature-checklist/ class=md-nav__link> <span class=md-ellipsis> Feature Checklist </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#concepts-covered class=md-nav__link> <span class=md-ellipsis> Concepts Covered </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#why-convolutional-neural-networks class=md-nav__link> <span class=md-ellipsis> Why Convolutional Neural Networks? </span> </a> </li> <li class=md-nav__item> <a href=#the-convolution-operation class=md-nav__link> <span class=md-ellipsis> The Convolution Operation </span> </a> <nav class=md-nav aria-label="The Convolution Operation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mathematical-definition class=md-nav__link> <span class=md-ellipsis> Mathematical Definition </span> </a> </li> <li class=md-nav__item> <a href=#how-convolution-works class=md-nav__link> <span class=md-ellipsis> How Convolution Works </span> </a> </li> <li class=md-nav__item> <a href=#filters-detect-features class=md-nav__link> <span class=md-ellipsis> Filters Detect Features </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#stride class=md-nav__link> <span class=md-ellipsis> Stride </span> </a> </li> <li class=md-nav__item> <a href=#padding class=md-nav__link> <span class=md-ellipsis> Padding </span> </a> <nav class=md-nav aria-label=Padding> <ul class=md-nav__list> <li class=md-nav__item> <a href=#valid-padding class=md-nav__link> <span class=md-ellipsis> Valid Padding </span> </a> </li> <li class=md-nav__item> <a href=#same-padding class=md-nav__link> <span class=md-ellipsis> Same Padding </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#receptive-field class=md-nav__link> <span class=md-ellipsis> Receptive Field </span> </a> </li> <li class=md-nav__item> <a href=#pooling-layers class=md-nav__link> <span class=md-ellipsis> Pooling Layers </span> </a> <nav class=md-nav aria-label="Pooling Layers"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#max-pooling class=md-nav__link> <span class=md-ellipsis> Max Pooling </span> </a> </li> <li class=md-nav__item> <a href=#average-pooling class=md-nav__link> <span class=md-ellipsis> Average Pooling </span> </a> </li> <li class=md-nav__item> <a href=#translation-invariance class=md-nav__link> <span class=md-ellipsis> Translation Invariance </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cnn-architecture-components class=md-nav__link> <span class=md-ellipsis> CNN Architecture Components </span> </a> <nav class=md-nav aria-label="CNN Architecture Components"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#local-connectivity class=md-nav__link> <span class=md-ellipsis> Local Connectivity </span> </a> </li> <li class=md-nav__item> <a href=#weight-sharing class=md-nav__link> <span class=md-ellipsis> Weight Sharing </span> </a> </li> <li class=md-nav__item> <a href=#spatial-hierarchies class=md-nav__link> <span class=md-ellipsis> Spatial Hierarchies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#building-a-cnn-in-pytorch class=md-nav__link> <span class=md-ellipsis> Building a CNN in PyTorch </span> </a> <nav class=md-nav aria-label="Building a CNN in PyTorch"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#training-the-cnn class=md-nav__link> <span class=md-ellipsis> Training the CNN </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#famous-cnn-architectures class=md-nav__link> <span class=md-ellipsis> Famous CNN Architectures </span> </a> <nav class=md-nav aria-label="Famous CNN Architectures"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#lenet-5-1998 class=md-nav__link> <span class=md-ellipsis> LeNet-5 (1998) </span> </a> </li> <li class=md-nav__item> <a href=#alexnet-2012 class=md-nav__link> <span class=md-ellipsis> AlexNet (2012) </span> </a> </li> <li class=md-nav__item> <a href=#vgg-2014 class=md-nav__link> <span class=md-ellipsis> VGG (2014) </span> </a> </li> <li class=md-nav__item> <a href=#resnet-2015 class=md-nav__link> <span class=md-ellipsis> ResNet (2015) </span> </a> </li> <li class=md-nav__item> <a href=#inception-googlenet-2014 class=md-nav__link> <span class=md-ellipsis> Inception (GoogLeNet, 2014) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#imagenet-and-the-evolution-of-computer-vision class=md-nav__link> <span class=md-ellipsis> ImageNet and the Evolution of Computer Vision </span> </a> </li> <li class=md-nav__item> <a href=#data-augmentation class=md-nav__link> <span class=md-ellipsis> Data Augmentation </span> </a> <nav class=md-nav aria-label="Data Augmentation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#common-image-augmentations class=md-nav__link> <span class=md-ellipsis> Common Image Augmentations </span> </a> </li> <li class=md-nav__item> <a href=#implementation-example class=md-nav__link> <span class=md-ellipsis> Implementation Example </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#interactive-visualization-convolution-operation class=md-nav__link> <span class=md-ellipsis> Interactive Visualization: Convolution Operation </span> </a> </li> <li class=md-nav__item> <a href=#interactive-visualization-cnn-architecture class=md-nav__link> <span class=md-ellipsis> Interactive Visualization: CNN Architecture </span> </a> </li> <li class=md-nav__item> <a href=#summary_1 class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#key-takeaways class=md-nav__link> <span class=md-ellipsis> Key Takeaways </span> </a> </li> <li class=md-nav__item> <a href=#further-reading class=md-nav__link> <span class=md-ellipsis> Further Reading </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <nav class=md-path aria-label=Navigation> <ol class=md-path__list> <li class=md-path__item> <a href=../.. class=md-path__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-path__item> <a href=../ class=md-path__link> <span class=md-ellipsis> Chapters </span> </a> </li> <li class=md-path__item> <a href=./ class=md-path__link> <span class=md-ellipsis> 10. Convolutional Networks </span> </a> </li> </ol> </nav> <article class="md-content__inner md-typeset"> <h1 id=convolutional-neural-networks-for-computer-vision>Convolutional Neural Networks for Computer Vision<a class=headerlink href=#convolutional-neural-networks-for-computer-vision title="Permanent link">¶</a></h1> <h2 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">¶</a></h2> <p>This chapter explores Convolutional Neural Networks (CNNs), specialized architectures designed for processing grid-like data such as images. Students will learn how convolution operations preserve spatial structure by applying filters across input images, understand the role of hyperparameters like kernel size, stride, and padding (valid vs. same), and explore pooling layers (max pooling, average pooling) that provide translation invariance and reduce computational complexity. The chapter covers fundamental CNN properties including local connectivity, weight sharing, and the formation of spatial hierarchies through feature maps and receptive fields. Students will study famous CNN architectures including LeNet, AlexNet, VGG, ResNet, and Inception, understanding how architectural innovations have driven progress in computer vision tasks.</p> <h2 id=concepts-covered>Concepts Covered<a class=headerlink href=#concepts-covered title="Permanent link">¶</a></h2> <p>This chapter covers the following 22 concepts from the learning graph:</p> <ol> <li>Convolutional Neural Network</li> <li>Convolution Operation</li> <li>Filter</li> <li>Stride</li> <li>Padding</li> <li>Valid Padding</li> <li>Same Padding</li> <li>Receptive Field</li> <li>Max Pooling</li> <li>Average Pooling</li> <li>Spatial Hierarchies</li> <li>Translation Invariance</li> <li>Local Connectivity</li> <li>Weight Sharing</li> <li>CNN Architecture</li> <li>LeNet</li> <li>AlexNet</li> <li>VGG</li> <li>ResNet</li> <li>Inception</li> <li>ImageNet</li> <li>Data Augmentation</li> </ol> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">¶</a></h2> <p>This chapter builds on concepts from:</p> <ul> <li><a href=../01-intro-to-ml-fundamentals/ >Chapter 1: Introduction to Machine Learning Fundamentals</a></li> <li><a href=../08-data-preprocessing/ >Chapter 8: Data Preprocessing and Feature Engineering</a></li> <li><a href=../09-neural-networks/ >Chapter 9: Neural Networks Fundamentals</a></li> </ul> <hr> <h2 id=why-convolutional-neural-networks>Why Convolutional Neural Networks?<a class=headerlink href=#why-convolutional-neural-networks title="Permanent link">¶</a></h2> <p>Standard fully connected neural networks struggle with image data. Consider a modest 224×224 color image—it contains 224 × 224 × 3 = 150,528 pixels. A fully connected hidden layer with 1,000 neurons would require over 150 million weights just for the first layer! This creates three critical problems:</p> <ol> <li><strong>Computational burden</strong>: Millions of parameters are expensive to store and compute</li> <li><strong>Overfitting</strong>: Too many parameters relative to training data</li> <li><strong>Loss of spatial structure</strong>: Flattening an image into a vector discards spatial relationships between nearby pixels</li> </ol> <p><strong>Convolutional Neural Networks</strong> (CNNs) solve these problems through three key principles:</p> <ul> <li><strong>Local connectivity</strong>: Each neuron connects only to a small local region</li> <li><strong>Weight sharing</strong>: The same weights (filters) apply across the entire image</li> <li><strong>Spatial hierarchies</strong>: Layers progressively build from simple to complex features</li> </ul> <p>These principles dramatically reduce parameters while preserving and exploiting spatial structure, making CNNs the dominant architecture for computer vision.</p> <h2 id=the-convolution-operation>The Convolution Operation<a class=headerlink href=#the-convolution-operation title="Permanent link">¶</a></h2> <p>The <strong>convolution operation</strong> is the fundamental building block of CNNs. Instead of learning separate weights for every pixel, convolution applies a small <strong>filter</strong> (also called a kernel) that slides across the input image.</p> <h3 id=mathematical-definition>Mathematical Definition<a class=headerlink href=#mathematical-definition title="Permanent link">¶</a></h3> <p>For a 2D image <span class=arithmatex>\(I\)</span> and filter <span class=arithmatex>\(K\)</span> of size <span class=arithmatex>\(k \times k\)</span>, the convolution operation is:</p> <div class=arithmatex>\[(I * K)[i,j] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} I[i+m, j+n] \cdot K[m,n]\]</div> <p>This computes a weighted sum of a local <span class=arithmatex>\(k \times k\)</span> neighborhood centered at position <span class=arithmatex>\((i,j)\)</span>.</p> <h3 id=how-convolution-works>How Convolution Works<a class=headerlink href=#how-convolution-works title="Permanent link">¶</a></h3> <p>Consider a 5×5 input image and a 3×3 filter:</p> <p><strong>Input image</strong> (<span class=arithmatex>\(5 \times 5\)</span>): </p><div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>1  2  0  1  3
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>0  1  2  1  0
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>1  0  1  2  1
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>2  1  0  1  2
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>1  0  2  1  0
</span></code></pre></div><p></p> <p><strong>Filter</strong> (<span class=arithmatex>\(3 \times 3\)</span>) - Edge detector: </p><div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>-1  -1  -1
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a> 0   0   0
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a> 1   1   1
</span></code></pre></div><p></p> <p>To compute the output at position (1,1): 1. Place filter over input region [0:3, 0:3] 2. Multiply element-wise: <span class=arithmatex>\((-1)(1) + (-1)(2) + (-1)(0) + (0)(0) + (0)(1) + (0)(2) + (1)(1) + (1)(0) + (1)(1)\)</span> 3. Sum the products: <span class=arithmatex>\(-1 - 2 + 0 + 0 + 0 + 0 + 1 + 0 + 1 = -1\)</span></p> <p>Sliding the filter across produces a <span class=arithmatex>\((5-3+1) \times (5-3+1) = 3 \times 3\)</span> output called a <strong>feature map</strong> or activation map.</p> <h3 id=filters-detect-features>Filters Detect Features<a class=headerlink href=#filters-detect-features title="Permanent link">¶</a></h3> <p>Different filters detect different features:</p> <ul> <li> <p><strong>Vertical edge detector</strong>: </p><div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>-1  0  1
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>-1  0  1
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>-1  0  1
</span></code></pre></div><p></p> </li> <li> <p><strong>Horizontal edge detector</strong>: </p><div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>-1  -1  -1
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a> 0   0   0
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a> 1   1   1
</span></code></pre></div><p></p> </li> <li> <p><strong>Blur (smoothing)</strong>: </p><div class="language-text highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>1/9  1/9  1/9
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>1/9  1/9  1/9
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>1/9  1/9  1/9
</span></code></pre></div><p></p> </li> </ul> <p>In CNNs, filter values are <strong>learned</strong> during training rather than hand-designed, allowing the network to discover optimal features for the task.</p> <h2 id=stride>Stride<a class=headerlink href=#stride title="Permanent link">¶</a></h2> <p><strong>Stride</strong> controls how far the filter moves at each step. A stride of 1 moves the filter one pixel at a time; stride of 2 skips every other position.</p> <p><strong>Effect on output size:</strong></p> <p>For input size <span class=arithmatex>\(n \times n\)</span>, filter size <span class=arithmatex>\(k \times k\)</span>, and stride <span class=arithmatex>\(s\)</span>:</p> <div class=arithmatex>\[\text{Output size} = \left\lfloor \frac{n - k}{s} + 1 \right\rfloor \times \left\lfloor \frac{n - k}{s} + 1 \right\rfloor\]</div> <p><strong>Example:</strong> - Input: <span class=arithmatex>\(7 \times 7\)</span>, Filter: <span class=arithmatex>\(3 \times 3\)</span>, Stride: 1 → Output: <span class=arithmatex>\(5 \times 5\)</span> - Input: <span class=arithmatex>\(7 \times 7\)</span>, Filter: <span class=arithmatex>\(3 \times 3\)</span>, Stride: 2 → Output: <span class=arithmatex>\(3 \times 3\)</span></p> <p>Larger strides reduce computational cost and output dimensions but may lose fine-grained spatial information.</p> <h2 id=padding>Padding<a class=headerlink href=#padding title="Permanent link">¶</a></h2> <p>Without padding, convolution shrinks the image—a <span class=arithmatex>\(5 \times 5\)</span> image convolved with a <span class=arithmatex>\(3 \times 3\)</span> filter produces a <span class=arithmatex>\(3 \times 3\)</span> output. After many layers, the image shrinks to unusable sizes.</p> <p><strong>Padding</strong> adds border pixels around the input to control output dimensions.</p> <h3 id=valid-padding>Valid Padding<a class=headerlink href=#valid-padding title="Permanent link">¶</a></h3> <p><strong>Valid padding</strong> (no padding) applies the filter only where it fits completely within the input:</p> <div class=arithmatex>\[\text{Output size} = \left\lfloor \frac{n - k}{s} + 1 \right\rfloor\]</div> <p>This shrinks the spatial dimensions with each layer.</p> <h3 id=same-padding>Same Padding<a class=headerlink href=#same-padding title="Permanent link">¶</a></h3> <p><strong>Same padding</strong> adds enough zeros around the border so that (with stride 1) the output has the same spatial dimensions as the input.</p> <p>For filter size <span class=arithmatex>\(k\)</span> and stride 1, we need padding <span class=arithmatex>\(p\)</span>:</p> <div class=arithmatex>\[p = \frac{k-1}{2}\]</div> <p>For example, a <span class=arithmatex>\(3 \times 3\)</span> filter needs padding of 1 on all sides; a <span class=arithmatex>\(5 \times 5\)</span> filter needs padding of 2.</p> <p><strong>Example with padding 1:</strong></p> <p>Input (<span class=arithmatex>\(5 \times 5\)</span>) becomes padded input (<span class=arithmatex>\(7 \times 7\)</span>): </p><div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>0  0  0  0  0  0  0
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>0  1  2  0  1  3  0
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>0  0  1  2  1  0  0
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>0  1  0  1  2  1  0
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>0  2  1  0  1  2  0
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>0  1  0  2  1  0  0
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>0  0  0  0  0  0  0
</span></code></pre></div><p></p> <p>Convolving with <span class=arithmatex>\(3 \times 3\)</span> filter produces <span class=arithmatex>\(5 \times 5\)</span> output—same as input size.</p> <p><strong>Why padding matters:</strong> - Preserves spatial dimensions through many layers - Allows information from edge pixels to be used - Prevents border information from being lost</p> <h2 id=receptive-field>Receptive Field<a class=headerlink href=#receptive-field title="Permanent link">¶</a></h2> <p>The <strong>receptive field</strong> of a neuron is the region of the input image that affects that neuron's activation. In deeper layers, neurons have larger receptive fields, capturing more global context.</p> <p><strong>Example:</strong> - Layer 1 neuron with <span class=arithmatex>\(3 \times 3\)</span> filter has <span class=arithmatex>\(3 \times 3\)</span> receptive field - Layer 2 neuron receiving from <span class=arithmatex>\(3 \times 3\)</span> region of layer 1, each with <span class=arithmatex>\(3 \times 3\)</span> receptive field, has <span class=arithmatex>\(5 \times 5\)</span> receptive field in input - Layer 3: <span class=arithmatex>\(7 \times 7\)</span> receptive field</p> <p>Deep CNNs build large receptive fields through stacking, enabling neurons in final layers to integrate information from the entire image while maintaining computational efficiency.</p> <h2 id=pooling-layers>Pooling Layers<a class=headerlink href=#pooling-layers title="Permanent link">¶</a></h2> <p><strong>Pooling layers</strong> downsample feature maps, reducing spatial dimensions while retaining important information. Pooling provides two critical benefits:</p> <ol> <li><strong>Translation invariance</strong>: Small shifts in input don't change output</li> <li><strong>Computational efficiency</strong>: Reduces dimensions for downstream layers</li> </ol> <h3 id=max-pooling>Max Pooling<a class=headerlink href=#max-pooling title="Permanent link">¶</a></h3> <p><strong>Max pooling</strong> takes the maximum value in each local region. For <span class=arithmatex>\(2 \times 2\)</span> max pooling with stride 2:</p> <p><strong>Input feature map</strong> (<span class=arithmatex>\(4 \times 4\)</span>): </p><div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>1  3  2  4
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>5  6  7  8
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>3  2  1  0
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>1  2  3  4
</span></code></pre></div><p></p> <p><strong>Output</strong> (<span class=arithmatex>\(2 \times 2\)</span>): </p><div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>6  8
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>3  4
</span></code></pre></div><p></p> <p>Each <span class=arithmatex>\(2 \times 2\)</span> region is replaced by its maximum value.</p> <p><strong>Properties:</strong> - Most common pooling operation - Preserves strongest activations (detected features) - Makes representations invariant to small translations - Typically uses <span class=arithmatex>\(2 \times 2\)</span> windows with stride 2 (halves spatial dimensions)</p> <h3 id=average-pooling>Average Pooling<a class=headerlink href=#average-pooling title="Permanent link">¶</a></h3> <p><strong>Average pooling</strong> takes the mean of each local region:</p> <p><strong>Input</strong> (same <span class=arithmatex>\(4 \times 4\)</span>): </p><div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>1  3  2  4
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>5  6  7  8
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>3  2  1  0
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>1  2  3  4
</span></code></pre></div><p></p> <p><strong>Output</strong> (<span class=arithmatex>\(2 \times 2\)</span>): </p><div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>3.75  5.25
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>2.00  2.00
</span></code></pre></div><p></p> <p>Average pooling is less commonly used than max pooling in convolutional layers but sometimes appears in final layers before classification.</p> <h3 id=translation-invariance>Translation Invariance<a class=headerlink href=#translation-invariance title="Permanent link">¶</a></h3> <p><strong>Translation invariance</strong> means that small spatial shifts in the input don't significantly change the output. Pooling achieves this by discarding precise spatial positions.</p> <p>For example, if an edge detector activates at position (10, 15) in one image and (10, 16) in a slightly shifted version, <span class=arithmatex>\(2 \times 2\)</span> max pooling groups both into the same output, making the representation robust to small translations.</p> <p>This is crucial for computer vision: a cat is still a cat whether it's in the top-left or top-right of the image.</p> <h2 id=cnn-architecture-components>CNN Architecture Components<a class=headerlink href=#cnn-architecture-components title="Permanent link">¶</a></h2> <h3 id=local-connectivity>Local Connectivity<a class=headerlink href=#local-connectivity title="Permanent link">¶</a></h3> <p>In <strong>local connectivity</strong>, each neuron connects only to a small spatial region of the previous layer. A neuron in a convolutional layer with a <span class=arithmatex>\(3 \times 3\)</span> filter receives input from a <span class=arithmatex>\(3 \times 3\)</span> patch, not the entire input.</p> <p><strong>Comparison:</strong> - Fully connected layer: <span class=arithmatex>\(n^2\)</span> input pixels, <span class=arithmatex>\(m\)</span> neurons → <span class=arithmatex>\(m \times n^2\)</span> weights - Convolutional layer: <span class=arithmatex>\(k \times k\)</span> filter, <span class=arithmatex>\(m\)</span> filters → <span class=arithmatex>\(m \times k^2\)</span> weights</p> <p>For <span class=arithmatex>\(n=224\)</span>, <span class=arithmatex>\(k=3\)</span>, <span class=arithmatex>\(m=64\)</span>: - Fully connected: <span class=arithmatex>\(64 \times 224^2 \approx 3.2\)</span> million weights - Convolutional: <span class=arithmatex>\(64 \times 3^2 = 576\)</span> weights</p> <p>Local connectivity dramatically reduces parameters while preserving spatial structure.</p> <h3 id=weight-sharing>Weight Sharing<a class=headerlink href=#weight-sharing title="Permanent link">¶</a></h3> <p>In <strong>weight sharing</strong>, the same filter (same weights) slides across the entire input. All neurons in a feature map share identical weights.</p> <p><strong>Benefits:</strong> - <strong>Parameter efficiency</strong>: One <span class=arithmatex>\(3 \times 3\)</span> filter has 9 weights, but processes entire image - <strong>Translation equivariance</strong>: Same pattern detected anywhere in the image - <strong>Inductive bias</strong>: Assumes useful features can appear anywhere (appropriate for images)</p> <p>A convolutional layer with 64 filters of size <span class=arithmatex>\(3 \times 3\)</span> has only <span class=arithmatex>\(64 \times 3 \times 3 = 576\)</span> weights, regardless of input size.</p> <h3 id=spatial-hierarchies>Spatial Hierarchies<a class=headerlink href=#spatial-hierarchies title="Permanent link">¶</a></h3> <p>CNNs learn <strong>spatial hierarchies</strong> of features: early layers detect simple patterns (edges, textures), middle layers combine these into parts (eyes, wheels), and deep layers recognize objects (faces, cars).</p> <p><strong>Typical hierarchy:</strong> - <strong>Layer 1</strong>: Edges, colors, simple patterns - <strong>Layer 2-3</strong>: Corners, textures, simple shapes - <strong>Layer 4-5</strong>: Object parts (eyes, noses, wheels, windows) - <strong>Layer 6+</strong>: Complete objects, scenes</p> <p>This hierarchical organization mirrors the visual cortex in biological vision systems.</p> <h2 id=building-a-cnn-in-pytorch>Building a CNN in PyTorch<a class=headerlink href=#building-a-cnn-in-pytorch title="Permanent link">¶</a></h2> <p>Let's implement a CNN for MNIST digit classification:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>transforms</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim.lr_scheduler</span><span class=w> </span><span class=kn>import</span> <span class=n>StepLR</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a><span class=k>class</span><span class=w> </span><span class=nc>Net</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-11><a id=__codelineno-10-11 name=__codelineno-10-11 href=#__codelineno-10-11></a>        <span class=nb>super</span><span class=p>(</span><span class=n>Net</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-10-12><a id=__codelineno-10-12 name=__codelineno-10-12 href=#__codelineno-10-12></a>        <span class=c1># First convolutional layer: 1 input channel (grayscale), 32 output channels, 3x3 filter</span>
</span><span id=__span-10-13><a id=__codelineno-10-13 name=__codelineno-10-13 href=#__codelineno-10-13></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-10-14><a id=__codelineno-10-14 name=__codelineno-10-14 href=#__codelineno-10-14></a>        <span class=c1># Second convolutional layer: 32 input channels, 64 output channels, 3x3 filter</span>
</span><span id=__span-10-15><a id=__codelineno-10-15 name=__codelineno-10-15 href=#__codelineno-10-15></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-10-16><a id=__codelineno-10-16 name=__codelineno-10-16 href=#__codelineno-10-16></a>        <span class=c1># Fully connected layers</span>
</span><span id=__span-10-17><a id=__codelineno-10-17 name=__codelineno-10-17 href=#__codelineno-10-17></a>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=o>*</span><span class=mi>5</span><span class=o>*</span><span class=mi>5</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
</span><span id=__span-10-18><a id=__codelineno-10-18 name=__codelineno-10-18 href=#__codelineno-10-18></a>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span><span id=__span-10-19><a id=__codelineno-10-19 name=__codelineno-10-19 href=#__codelineno-10-19></a>        <span class=c1># Dropout for regularization</span>
</span><span id=__span-10-20><a id=__codelineno-10-20 name=__codelineno-10-20 href=#__codelineno-10-20></a>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span><span id=__span-10-21><a id=__codelineno-10-21 name=__codelineno-10-21 href=#__codelineno-10-21></a>
</span><span id=__span-10-22><a id=__codelineno-10-22 name=__codelineno-10-22 href=#__codelineno-10-22></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-10-23><a id=__codelineno-10-23 name=__codelineno-10-23 href=#__codelineno-10-23></a>        <span class=c1># Input: 28x28 images</span>
</span><span id=__span-10-24><a id=__codelineno-10-24 name=__codelineno-10-24 href=#__codelineno-10-24></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>       <span class=c1># After conv1: 26x26 (28-3+1)</span>
</span><span id=__span-10-25><a id=__codelineno-10-25 name=__codelineno-10-25 href=#__codelineno-10-25></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>           <span class=c1># ReLU activation</span>
</span><span id=__span-10-26><a id=__codelineno-10-26 name=__codelineno-10-26 href=#__codelineno-10-26></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>max_pool2d</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># After pooling: 13x13 (26/2)</span>
</span><span id=__span-10-27><a id=__codelineno-10-27 name=__codelineno-10-27 href=#__codelineno-10-27></a>
</span><span id=__span-10-28><a id=__codelineno-10-28 name=__codelineno-10-28 href=#__codelineno-10-28></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>       <span class=c1># After conv2: 11x11 (13-3+1)</span>
</span><span id=__span-10-29><a id=__codelineno-10-29 name=__codelineno-10-29 href=#__codelineno-10-29></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>           <span class=c1># ReLU activation</span>
</span><span id=__span-10-30><a id=__codelineno-10-30 name=__codelineno-10-30 href=#__codelineno-10-30></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>max_pool2d</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># After pooling: 5x5 (11/2)</span>
</span><span id=__span-10-31><a id=__codelineno-10-31 name=__codelineno-10-31 href=#__codelineno-10-31></a>
</span><span id=__span-10-32><a id=__codelineno-10-32 name=__codelineno-10-32 href=#__codelineno-10-32></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=c1># Flatten to vector: 64*5*5 = 1600</span>
</span><span id=__span-10-33><a id=__codelineno-10-33 name=__codelineno-10-33 href=#__codelineno-10-33></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>         <span class=c1># Fully connected: 1600 → 512</span>
</span><span id=__span-10-34><a id=__codelineno-10-34 name=__codelineno-10-34 href=#__codelineno-10-34></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>     <span class=c1># Dropout regularization</span>
</span><span id=__span-10-35><a id=__codelineno-10-35 name=__codelineno-10-35 href=#__codelineno-10-35></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>           <span class=c1># ReLU activation</span>
</span><span id=__span-10-36><a id=__codelineno-10-36 name=__codelineno-10-36 href=#__codelineno-10-36></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>         <span class=c1># Output layer: 512 → 10 classes</span>
</span><span id=__span-10-37><a id=__codelineno-10-37 name=__codelineno-10-37 href=#__codelineno-10-37></a>        <span class=n>output</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Log probabilities</span>
</span><span id=__span-10-38><a id=__codelineno-10-38 name=__codelineno-10-38 href=#__codelineno-10-38></a>        <span class=k>return</span> <span class=n>output</span>
</span></code></pre></div> <p><strong>Architecture breakdown:</strong> - Input: 28×28 grayscale images - Conv1: 32 filters (3×3) → 26×26×32 - Max pool: 2×2 → 13×13×32 - Conv2: 64 filters (3×3) → 11×11×64 - Max pool: 2×2 → 5×5×64 - Flatten → 1600-dimensional vector - FC1: 1600 → 512 - Dropout (0.5) - FC2: 512 → 10 (digit classes)</p> <h3 id=training-the-cnn>Training the CNN<a class=headerlink href=#training-the-cnn title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=k>def</span><span class=w> </span><span class=nf>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>epoch</span><span class=p>):</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a>    <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>        <span class=n>data</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>           <span class=c1># Zero gradients</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>            <span class=c1># Forward pass</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>nll_loss</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>  <span class=c1># Compute loss</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>                 <span class=c1># Backpropagation</span>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>                <span class=c1># Update weights</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a>        <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-11-12><a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>'Epoch: </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s1> [</span><span class=si>{</span><span class=n>batch_idx</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>)</span><span class=si>}</span><span class=s1>/</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span><span class=si>}</span><span class=s1>] '</span>
</span><span id=__span-11-13><a id=__codelineno-11-13 name=__codelineno-11-13 href=#__codelineno-11-13></a>                  <span class=sa>f</span><span class=s1>'Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s1>.6f</span><span class=si>}</span><span class=s1>'</span><span class=p>)</span>
</span><span id=__span-11-14><a id=__codelineno-11-14 name=__codelineno-11-14 href=#__codelineno-11-14></a>
</span><span id=__span-11-15><a id=__codelineno-11-15 name=__codelineno-11-15 href=#__codelineno-11-15></a>
</span><span id=__span-11-16><a id=__codelineno-11-16 name=__codelineno-11-16 href=#__codelineno-11-16></a><span class=k>def</span><span class=w> </span><span class=nf>test</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>,</span> <span class=n>test_loader</span><span class=p>):</span>
</span><span id=__span-11-17><a id=__codelineno-11-17 name=__codelineno-11-17 href=#__codelineno-11-17></a>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span><span id=__span-11-18><a id=__codelineno-11-18 name=__codelineno-11-18 href=#__codelineno-11-18></a>    <span class=n>test_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-11-19><a id=__codelineno-11-19 name=__codelineno-11-19 href=#__codelineno-11-19></a>    <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-11-20><a id=__codelineno-11-20 name=__codelineno-11-20 href=#__codelineno-11-20></a>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span><span id=__span-11-21><a id=__codelineno-11-21 name=__codelineno-11-21 href=#__codelineno-11-21></a>        <span class=k>for</span> <span class=n>data</span><span class=p>,</span> <span class=n>target</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
</span><span id=__span-11-22><a id=__codelineno-11-22 name=__codelineno-11-22 href=#__codelineno-11-22></a>            <span class=n>data</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-11-23><a id=__codelineno-11-23 name=__codelineno-11-23 href=#__codelineno-11-23></a>            <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span><span id=__span-11-24><a id=__codelineno-11-24 name=__codelineno-11-24 href=#__codelineno-11-24></a>            <span class=n>test_loss</span> <span class=o>+=</span> <span class=n>F</span><span class=o>.</span><span class=n>nll_loss</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>'sum'</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span><span id=__span-11-25><a id=__codelineno-11-25 name=__codelineno-11-25 href=#__codelineno-11-25></a>            <span class=n>pred</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-11-26><a id=__codelineno-11-26 name=__codelineno-11-26 href=#__codelineno-11-26></a>            <span class=n>correct</span> <span class=o>+=</span> <span class=n>pred</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>target</span><span class=o>.</span><span class=n>view_as</span><span class=p>(</span><span class=n>pred</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span><span id=__span-11-27><a id=__codelineno-11-27 name=__codelineno-11-27 href=#__codelineno-11-27></a>
</span><span id=__span-11-28><a id=__codelineno-11-28 name=__codelineno-11-28 href=#__codelineno-11-28></a>    <span class=n>test_loss</span> <span class=o>/=</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_loader</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span>
</span><span id=__span-11-29><a id=__codelineno-11-29 name=__codelineno-11-29 href=#__codelineno-11-29></a>    <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>100.</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_loader</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span>
</span><span id=__span-11-30><a id=__codelineno-11-30 name=__codelineno-11-30 href=#__codelineno-11-30></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>'</span><span class=se>\n</span><span class=s1>Test: Average loss: </span><span class=si>{</span><span class=n>test_loss</span><span class=si>:</span><span class=s1>.4f</span><span class=si>}</span><span class=s1>, '</span>
</span><span id=__span-11-31><a id=__codelineno-11-31 name=__codelineno-11-31 href=#__codelineno-11-31></a>          <span class=sa>f</span><span class=s1>'Accuracy: </span><span class=si>{</span><span class=n>correct</span><span class=si>}</span><span class=s1>/</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>test_loader</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span><span class=si>}</span><span class=s1> (</span><span class=si>{</span><span class=n>accuracy</span><span class=si>:</span><span class=s1>.0f</span><span class=si>}</span><span class=s1>%)</span><span class=se>\n</span><span class=s1>'</span><span class=p>)</span>
</span><span id=__span-11-32><a id=__codelineno-11-32 name=__codelineno-11-32 href=#__codelineno-11-32></a>
</span><span id=__span-11-33><a id=__codelineno-11-33 name=__codelineno-11-33 href=#__codelineno-11-33></a>
</span><span id=__span-11-34><a id=__codelineno-11-34 name=__codelineno-11-34 href=#__codelineno-11-34></a><span class=c1># Training setup</span>
</span><span id=__span-11-35><a id=__codelineno-11-35 name=__codelineno-11-35 href=#__codelineno-11-35></a><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>"cuda"</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>"cpu"</span><span class=p>)</span>
</span><span id=__span-11-36><a id=__codelineno-11-36 name=__codelineno-11-36 href=#__codelineno-11-36></a><span class=n>model</span> <span class=o>=</span> <span class=n>Net</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-11-37><a id=__codelineno-11-37 name=__codelineno-11-37 href=#__codelineno-11-37></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adadelta</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span><span id=__span-11-38><a id=__codelineno-11-38 name=__codelineno-11-38 href=#__codelineno-11-38></a><span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
</span><span id=__span-11-39><a id=__codelineno-11-39 name=__codelineno-11-39 href=#__codelineno-11-39></a>
</span><span id=__span-11-40><a id=__codelineno-11-40 name=__codelineno-11-40 href=#__codelineno-11-40></a><span class=c1># Load MNIST dataset</span>
</span><span id=__span-11-41><a id=__codelineno-11-41 name=__codelineno-11-41 href=#__codelineno-11-41></a><span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()])</span>
</span><span id=__span-11-42><a id=__codelineno-11-42 name=__codelineno-11-42 href=#__codelineno-11-42></a><span class=n>train_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s1>'./data'</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span><span id=__span-11-43><a id=__codelineno-11-43 name=__codelineno-11-43 href=#__codelineno-11-43></a><span class=n>test_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s1>'./data'</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span><span id=__span-11-44><a id=__codelineno-11-44 name=__codelineno-11-44 href=#__codelineno-11-44></a>
</span><span id=__span-11-45><a id=__codelineno-11-45 name=__codelineno-11-45 href=#__codelineno-11-45></a><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-11-46><a id=__codelineno-11-46 name=__codelineno-11-46 href=#__codelineno-11-46></a><span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>test_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span><span id=__span-11-47><a id=__codelineno-11-47 name=__codelineno-11-47 href=#__codelineno-11-47></a>
</span><span id=__span-11-48><a id=__codelineno-11-48 name=__codelineno-11-48 href=#__codelineno-11-48></a><span class=c1># Train for 14 epochs</span>
</span><span id=__span-11-49><a id=__codelineno-11-49 name=__codelineno-11-49 href=#__codelineno-11-49></a><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>15</span><span class=p>):</span>
</span><span id=__span-11-50><a id=__codelineno-11-50 name=__codelineno-11-50 href=#__codelineno-11-50></a>    <span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>epoch</span><span class=p>)</span>
</span><span id=__span-11-51><a id=__codelineno-11-51 name=__codelineno-11-51 href=#__codelineno-11-51></a>    <span class=n>test</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>,</span> <span class=n>test_loader</span><span class=p>)</span>
</span><span id=__span-11-52><a id=__codelineno-11-52 name=__codelineno-11-52 href=#__codelineno-11-52></a>    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-11-53><a id=__codelineno-11-53 name=__codelineno-11-53 href=#__codelineno-11-53></a>
</span><span id=__span-11-54><a id=__codelineno-11-54 name=__codelineno-11-54 href=#__codelineno-11-54></a><span class=c1># Save trained model</span>
</span><span id=__span-11-55><a id=__codelineno-11-55 name=__codelineno-11-55 href=#__codelineno-11-55></a><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s2>"mnist_cnn.pth"</span><span class=p>)</span>
</span></code></pre></div> <p>This achieves ~99% accuracy on MNIST after 14 epochs, demonstrating CNN effectiveness for image classification.</p> <h2 id=famous-cnn-architectures>Famous CNN Architectures<a class=headerlink href=#famous-cnn-architectures title="Permanent link">¶</a></h2> <h3 id=lenet-5-1998>LeNet-5 (1998)<a class=headerlink href=#lenet-5-1998 title="Permanent link">¶</a></h3> <p><strong>LeNet</strong>, developed by Yann LeCun, was the first successful CNN architecture, originally designed for handwritten digit recognition (used by banks to read checks).</p> <p><strong>Architecture:</strong> - Input: 32×32 grayscale images - Conv1: 6 filters (5×5) + tanh → 28×28×6 - AvgPool: 2×2 → 14×14×6 - Conv2: 16 filters (5×5) + tanh → 10×10×16 - AvgPool: 2×2 → 5×5×16 - FC: 5×5×16 → 120 → 84 → 10</p> <p><strong>Innovations:</strong> - First practical demonstration of CNNs - Established conv-pool-conv-pool-FC pattern - Proved that gradient-based learning works for deep networks</p> <h3 id=alexnet-2012>AlexNet (2012)<a class=headerlink href=#alexnet-2012 title="Permanent link">¶</a></h3> <p><strong>AlexNet</strong>, by Alex Krizhevsky, achieved breakthrough performance on ImageNet, sparking the deep learning revolution.</p> <p><strong>Architecture:</strong> - Input: 227×227 RGB images - 5 convolutional layers (with ReLU) - 3 max pooling layers - 3 fully connected layers - 60 million parameters</p> <p><strong>Key innovations:</strong> - <strong>ReLU activation</strong>: Replaced tanh/sigmoid, enabling faster training - <strong>Dropout</strong>: Reduced overfitting in fully connected layers - <strong>Data augmentation</strong>: Random crops, flips, color jittering - <strong>GPU training</strong>: Parallelized across 2 GPUs - <strong>Local Response Normalization</strong> (LRN): Competitive normalization (later replaced by batch norm)</p> <p><strong>Impact:</strong> - Won ImageNet 2012 with 15.3% error (vs. 26.2% for second place) - Demonstrated deep networks could outperform hand-crafted features - Catalyzed the modern deep learning era</p> <h3 id=vgg-2014>VGG (2014)<a class=headerlink href=#vgg-2014 title="Permanent link">¶</a></h3> <p><strong>VGG</strong> (Visual Geometry Group, Oxford) showed that network depth is critical, using very small (3×3) filters throughout.</p> <p><strong>Architecture (VGG-16):</strong> - 13 convolutional layers (all 3×3 filters) - 5 max pooling layers (2×2) - 3 fully connected layers - 138 million parameters</p> <p><strong>Design principles:</strong> - <strong>Small filters</strong>: Only 3×3 convolutions throughout - <strong>Deep networks</strong>: 16-19 layers (hence VGG-16, VGG-19) - <strong>Simple architecture</strong>: Repetitive structure easy to understand and modify</p> <p><strong>Advantages:</strong> - Smaller filters reduce parameters while maintaining receptive field - Deeper networks learn more complex representations - Homogeneous architecture simplifies design</p> <p><strong>Limitation:</strong> - Very large number of parameters (memory intensive)</p> <h3 id=resnet-2015>ResNet (2015)<a class=headerlink href=#resnet-2015 title="Permanent link">¶</a></h3> <p><strong>ResNet</strong> (Residual Networks) introduced skip connections, enabling training of extremely deep networks (50-152 layers).</p> <p><strong>Core innovation: Residual blocks</strong></p> <p>Instead of learning <span class=arithmatex>\(H(x)\)</span> directly, learn residual <span class=arithmatex>\(F(x) = H(x) - x\)</span>:</p> <div class=arithmatex>\[H(x) = F(x) + x\]</div> <p>The skip connection adds the input <span class=arithmatex>\(x\)</span> to the output, creating a shortcut path.</p> <p><strong>Architecture (ResNet-50):</strong> - 50 layers organized into residual blocks - Each block: Conv-BatchNorm-ReLU-Conv-BatchNorm + Skip Connection - Deeper variants: ResNet-101, ResNet-152</p> <p><strong>Why skip connections work:</strong> - <strong>Gradient flow</strong>: Gradients flow directly through skip connections, alleviating vanishing gradients - <strong>Easier optimization</strong>: Learning residuals <span class=arithmatex>\(F(x)\)</span> easier than learning full transformation <span class=arithmatex>\(H(x)\)</span> - <strong>Identity mapping</strong>: If optimal transformation is close to identity, network can learn <span class=arithmatex>\(F(x) \approx 0\)</span></p> <p><strong>Impact:</strong> - Enabled training of 100+ layer networks - Won ImageNet 2015 with 3.6% error (surpassing human-level 5%) - Skip connections now ubiquitous in deep architectures</p> <h3 id=inception-googlenet-2014>Inception (GoogLeNet, 2014)<a class=headerlink href=#inception-googlenet-2014 title="Permanent link">¶</a></h3> <p><strong>Inception</strong> introduced multi-scale feature extraction through parallel conv paths with different filter sizes.</p> <p><strong>Inception module:</strong> - Parallel paths: 1×1 conv, 3×3 conv, 5×5 conv, 3×3 max pool - Concatenate outputs along channel dimension - 1×1 convolutions reduce dimensionality before expensive operations</p> <p><strong>Benefits:</strong> - <strong>Multi-scale features</strong>: Captures patterns at different scales simultaneously - <strong>Computational efficiency</strong>: 1×1 "bottleneck" convolutions reduce computation - <strong>Sparse connections</strong>: More efficient than fully dense layers</p> <p><strong>Architecture (GoogLeNet/Inception-v1):</strong> - 22 layers with 9 inception modules - Only 5 million parameters (vs. 60M for AlexNet) - Auxiliary classifiers during training to combat vanishing gradients</p> <p>Later versions (Inception-v2, v3, v4, Inception-ResNet) incorporated batch normalization, factorized convolutions, and residual connections.</p> <h2 id=imagenet-and-the-evolution-of-computer-vision>ImageNet and the Evolution of Computer Vision<a class=headerlink href=#imagenet-and-the-evolution-of-computer-vision title="Permanent link">¶</a></h2> <p><strong>ImageNet</strong> is a massive image dataset containing 14 million images across 20,000+ categories. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove much of CNN architecture development.</p> <p><strong>Annual progress:</strong> - 2010: Traditional methods ~28% error - 2012: AlexNet 15.3% (first deep CNN) - 2013: ZFNet 11.7% - 2014: VGG 7.3%, GoogLeNet 6.7% - 2015: ResNet 3.6% (surpassed human ~5%) - 2017+: &lt;3% error with advanced architectures</p> <p>ImageNet established deep CNNs as the dominant approach for computer vision, with pretrained ImageNet models becoming standard starting points for transfer learning on other vision tasks.</p> <h2 id=data-augmentation>Data Augmentation<a class=headerlink href=#data-augmentation title="Permanent link">¶</a></h2> <p><strong>Data augmentation</strong> artificially expands the training set by applying transformations that preserve labels. This is crucial for training deep CNNs, which require massive amounts of data.</p> <h3 id=common-image-augmentations>Common Image Augmentations<a class=headerlink href=#common-image-augmentations title="Permanent link">¶</a></h3> <p><strong>Geometric transformations:</strong> - <strong>Random crops</strong>: Extract random patches from images - <strong>Horizontal flips</strong>: Mirror images left-right (not vertical for natural images) - <strong>Rotations</strong>: Small random rotations (±15°) - <strong>Scaling</strong>: Zoom in/out - <strong>Shearing</strong>: Slant transformations</p> <p><strong>Color transformations:</strong> - <strong>Brightness/contrast</strong>: Adjust intensity - <strong>Color jittering</strong>: Randomly perturb RGB channels - <strong>Hue/saturation</strong>: Shift colors</p> <p><strong>Advanced augmentations:</strong> - <strong>Cutout</strong>: Randomly mask square regions - <strong>Mixup</strong>: Blend two images and their labels - <strong>AutoAugment</strong>: Learn optimal augmentation policies</p> <h3 id=implementation-example>Implementation Example<a class=headerlink href=#implementation-example title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>transforms</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=c1># Training augmentation pipeline</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a><span class=n>train_transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>RandomCrop</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>4</span><span class=p>),</span>        <span class=c1># Random crop with padding</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>RandomHorizontalFlip</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>),</span>      <span class=c1># 50% chance of horizontal flip</span>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>ColorJitter</span><span class=p>(</span><span class=n>brightness</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>        <span class=c1># Color perturbations</span>
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>                           <span class=n>contrast</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a>                           <span class=n>saturation</span><span class=o>=</span><span class=mf>0.2</span><span class=p>),</span>
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>                        <span class=c1># Convert to tensor</span>
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>),</span>        <span class=c1># Normalize to [-1, 1]</span>
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>                         <span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>))</span>
</span><span id=__span-12-13><a id=__codelineno-12-13 name=__codelineno-12-13 href=#__codelineno-12-13></a><span class=p>])</span>
</span><span id=__span-12-14><a id=__codelineno-12-14 name=__codelineno-12-14 href=#__codelineno-12-14></a>
</span><span id=__span-12-15><a id=__codelineno-12-15 name=__codelineno-12-15 href=#__codelineno-12-15></a><span class=c1># Test augmentation (no randomness)</span>
</span><span id=__span-12-16><a id=__codelineno-12-16 name=__codelineno-12-16 href=#__codelineno-12-16></a><span class=n>test_transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-12-17><a id=__codelineno-12-17 name=__codelineno-12-17 href=#__codelineno-12-17></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span><span id=__span-12-18><a id=__codelineno-12-18 name=__codelineno-12-18 href=#__codelineno-12-18></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>),</span> <span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>))</span>
</span><span id=__span-12-19><a id=__codelineno-12-19 name=__codelineno-12-19 href=#__codelineno-12-19></a><span class=p>])</span>
</span></code></pre></div> <p>Data augmentation acts as strong regularization, often improving generalization more than architectural changes or hyperparameter tuning.</p> <h2 id=interactive-visualization-convolution-operation>Interactive Visualization: Convolution Operation<a class=headerlink href=#interactive-visualization-convolution-operation title="Permanent link">¶</a></h2> <p>Understanding how convolution filters process images:</p> <pre class=mermaid><code>flowchart LR
    Input["Input Image&lt;br/&gt;(e.g., 32×32)"]
    Filter["3×3 Filter&lt;br/&gt;slides across"]
    Conv["Element-wise&lt;br/&gt;multiply &amp; sum"]
    Output["Feature Map&lt;br/&gt;(30×30)"]

    Input --&gt; Filter
    Filter --&gt; Conv
    Conv --&gt; Output

    style Input fill:#E3F2FD
    style Filter fill:#FFF3E0
    style Conv fill:#F3E5F5
    style Output fill:#E8F5E9</code></pre> <p><strong>Key Concepts:</strong> - <strong>Filter (Kernel)</strong>: Small matrix (e.g., 3×3) that slides across input - <strong>Stride</strong>: Step size (stride=1 moves one pixel at a time) - <strong>Padding</strong>: Zeros added around borders to control output size - <strong>Output Size</strong>: <span class=arithmatex>\((n - k + 2p)/s + 1\)</span> where n=input, k=kernel, p=padding, s=stride</p> <p><strong>Common Filters:</strong> - Edge Detection: <code>[[-1,-1,-1], [-1,8,-1], [-1,-1,-1]]</code> - Blur: <code>[[1,1,1], [1,1,1], [1,1,1]] / 9</code> - Sharpen: <code>[[0,-1,0], [-1,5,-1], [0,-1,0]]</code></p> <h2 id=interactive-visualization-cnn-architecture>Interactive Visualization: CNN Architecture<a class=headerlink href=#interactive-visualization-cnn-architecture title="Permanent link">¶</a></h2> <p>CNN layers progressively transform images into abstract feature representations:</p> <pre class=mermaid><code>flowchart TD
    Input["Input Image&lt;br/&gt;28×28×1"]
    Conv1["Conv Layer 1&lt;br/&gt;3×3 filters, 32 channels&lt;br/&gt;→ 26×26×32"]
    Pool1["Max Pool 2×2&lt;br/&gt;→ 13×13×32"]
    Conv2["Conv Layer 2&lt;br/&gt;3×3 filters, 64 channels&lt;br/&gt;→ 11×11×64"]
    Pool2["Max Pool 2×2&lt;br/&gt;→ 5×5×64"]
    Flatten["Flatten&lt;br/&gt;→ 1600 neurons"]
    FC["Fully Connected&lt;br/&gt;→ 10 classes"]
    Output["Softmax&lt;br/&gt;Probabilities"]

    Input --&gt; Conv1
    Conv1 --&gt; Pool1
    Pool1 --&gt; Conv2
    Conv2 --&gt; Pool2
    Pool2 --&gt; Flatten
    Flatten --&gt; FC
    FC --&gt; Output

    style Input fill:#E3F2FD
    style Conv1 fill:#FFF3E0
    style Pool1 fill:#F3E5F5
    style Conv2 fill:#FFF3E0
    style Pool2 fill:#F3E5F5
    style Flatten fill:#FCE4EC
    style FC fill:#E8F5E9
    style Output fill:#E8F5E9</code></pre> <p><strong>Spatial Hierarchy:</strong> - <strong>Early layers</strong> (Conv1): Detect edges, textures, simple patterns - <strong>Middle layers</strong> (Conv2): Recognize parts, combinations of features - <strong>Deep layers</strong> (FC): Identify complete objects, high-level concepts</p> <p><strong>Key Observations:</strong> - Spatial dimensions decrease (28×28 → 5×5) through pooling - Channel depth increases (1 → 32 → 64) capturing more features - Receptive field grows - deep neurons "see" larger input regions - Parameters concentrated in fully connected layers</p> <h2 id=summary_1>Summary<a class=headerlink href=#summary_1 title="Permanent link">¶</a></h2> <p>Convolutional Neural Networks revolutionized computer vision through three key principles: local connectivity (neurons connect to small regions), weight sharing (same filter across entire image), and spatial hierarchies (simple to complex features).</p> <p>The convolution operation applies filters that slide across images with configurable stride and padding. Filters are learned during training, automatically discovering optimal feature detectors. Max pooling downsamples feature maps, providing translation invariance and computational efficiency.</p> <p>CNNs build spatial hierarchies: early layers detect edges and textures, middle layers recognize parts, and deep layers identify complete objects. Receptive fields grow with depth, allowing deep neurons to integrate global context while maintaining computational efficiency.</p> <p>Landmark architectures drove progress: LeNet demonstrated feasibility, AlexNet sparked the deep learning revolution with ReLU and dropout, VGG showed that depth matters, ResNet enabled 100+ layer networks through skip connections, and Inception introduced multi-scale feature extraction.</p> <p>ImageNet competition accelerated development, with accuracy improving from 28% (2010) to below human-level 5% (2015). Data augmentation artificially expands training data through label-preserving transformations, acting as powerful regularization for deep CNNs.</p> <h2 id=key-takeaways>Key Takeaways<a class=headerlink href=#key-takeaways title="Permanent link">¶</a></h2> <ol> <li><strong>Convolution operation</strong> applies filters that slide across images, detecting features through learned weights</li> <li><strong>Local connectivity</strong> connects neurons only to small spatial regions, reducing parameters dramatically</li> <li><strong>Weight sharing</strong> uses same filter across entire image, enabling translation equivariance</li> <li><strong>Stride</strong> controls filter movement; larger stride reduces output dimensions</li> <li><strong>Same padding</strong> preserves spatial dimensions; <strong>valid padding</strong> shrinks them</li> <li><strong>Receptive field</strong> grows with depth, allowing deep neurons to see entire image</li> <li><strong>Max pooling</strong> provides translation invariance and reduces spatial dimensions by factor</li> <li><strong>Spatial hierarchies</strong> emerge: edges → parts → objects across layers</li> <li><strong>ResNet skip connections</strong> enable training very deep networks by alleviating vanishing gradients</li> <li><strong>Data augmentation</strong> expands training data through transformations, reducing overfitting</li> <li>CNNs achieve human-level performance on ImageNet through architectural innovation</li> <li>Transfer learning with pretrained CNNs enables strong performance on limited data</li> </ol> <h2 id=further-reading>Further Reading<a class=headerlink href=#further-reading title="Permanent link">¶</a></h2> <ul> <li>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). "Gradient-based learning applied to document recognition." <em>Proceedings of the IEEE</em>, 86(11), 2278-2324.</li> <li>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. (2012). "ImageNet classification with deep convolutional neural networks." <em>NeurIPS</em>, 1097-1105.</li> <li>Simonyan, K., &amp; Zisserman, A. (2014). "Very deep convolutional networks for large-scale image recognition." <em>ICLR</em>.</li> <li>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). "Deep residual learning for image recognition." <em>CVPR</em>, 770-778.</li> <li>Szegedy, C., et al. (2015). "Going deeper with convolutions." <em>CVPR</em>, 1-9.</li> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em> (Chapter 9: Convolutional Networks)</li> </ul> <h2 id=exercises>Exercises<a class=headerlink href=#exercises title="Permanent link">¶</a></h2> <ol> <li> <p><strong>Manual Convolution</strong>: Compute a 3×3 convolution with stride 1, valid padding on a 5×5 image using a vertical edge detection filter. Show all intermediate steps.</p> </li> <li> <p><strong>Receptive Field Calculation</strong>: For a network with three 3×3 conv layers (stride 1, same padding) followed by 2×2 max pooling (stride 2), calculate the receptive field size of a neuron in layer 3.</p> </li> <li> <p><strong>Output Size Formula</strong>: Derive the output size formula for convolution given input size <span class=arithmatex>\(n\)</span>, filter size <span class=arithmatex>\(k\)</span>, stride <span class=arithmatex>\(s\)</span>, and padding <span class=arithmatex>\(p\)</span>.</p> </li> <li> <p><strong>Architecture Design</strong>: Design a CNN for 32×32 RGB images with 10 classes. Specify all layer types, dimensions, and number of parameters. Aim for &lt;1 million parameters.</p> </li> <li> <p><strong>Filter Visualization</strong>: Train a CNN on MNIST and visualize the learned filters in the first convolutional layer. What patterns do they detect?</p> </li> <li> <p><strong>Pooling Comparison</strong>: Implement max pooling and average pooling. Compare their effects on feature map invariance and information preservation using test images.</p> </li> <li> <p><strong>Data Augmentation Impact</strong>: Train two identical CNNs on CIFAR-10: one with data augmentation, one without. Compare final test accuracy and training curves. What is the effect of each augmentation type?</p> </li> </ol> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../09-neural-networks/quiz/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Quiz"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Quiz </div> </div> </a> <a href=quiz/ class="md-footer__link md-footer__link--next" aria-label="Next: Quiz"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Quiz </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2025 | CC BY-NC-SA 4.0 DEED </div> </div> <div class=md-social> <a href=https://github.com/AnvithPothula target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> <a href=https://linkedin.com/in/anvith-pothula target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.path", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>