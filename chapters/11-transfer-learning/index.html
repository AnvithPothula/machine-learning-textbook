<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Leverage pre-trained models to solve new tasks with limited data through feature extraction, fine-tuning, and domain adaptation"><meta name=author content="Anvith Pothula"><link href=https://example.com/chapters/11-transfer-learning/ rel=canonical><link href=../10-convolutional-networks/quiz/ rel=prev><link href=quiz/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Transfer Learning and Pre-Trained Models - Machine Learning - Algorithms and Applications</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#transfer-learning-and-pre-trained-models class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Machine Learning - Algorithms and Applications" class="md-header__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning - Algorithms and Applications </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Transfer Learning and Pre-Trained Models </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../course-description/ class=md-tabs__link> Course Description </a> </li> <li class=md-tabs__item> <a href=../../faq/ class=md-tabs__link> FAQ </a> </li> <li class=md-tabs__item> <a href=../../glossary/ class=md-tabs__link> Glossary </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> Chapters </a> </li> <li class=md-tabs__item> <a href=../../sims/ class=md-tabs__link> MicroSims </a> </li> <li class=md-tabs__item> <a href=../../learning-graph/ class=md-tabs__link> Learning Graph </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Machine Learning - Algorithms and Applications" class="md-nav__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Machine Learning - Algorithms and Applications </label> <div class=md-nav__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../../course-description/ class=md-nav__link> <span class=md-ellipsis> Course Description </span> </a> </li> <li class=md-nav__item> <a href=../../faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> <li class=md-nav__item> <a href=../../glossary/ class=md-nav__link> <span class=md-ellipsis> Glossary </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Chapters </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> 1. ML Fundamentals </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> 1. ML Fundamentals </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01-intro-to-ml-fundamentals/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../01-intro-to-ml-fundamentals/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> 2. K-Nearest Neighbors </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> 2. K-Nearest Neighbors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../02-k-nearest-neighbors/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../02-k-nearest-neighbors/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex> <span class=md-ellipsis> 3. Decision Trees </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> 3. Decision Trees </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../03-decision-trees/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../03-decision-trees/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex> <span class=md-ellipsis> 4. Logistic Regression </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> 4. Logistic Regression </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../04-logistic-regression/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../04-logistic-regression/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex> <span class=md-ellipsis> 5. Regularization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> 5. Regularization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../05-regularization/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../05-regularization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_7> <label class=md-nav__link for=__nav_5_7 id=__nav_5_7_label tabindex> <span class=md-ellipsis> 6. Support Vector Machines </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> 6. Support Vector Machines </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../06-support-vector-machines/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../06-support-vector-machines/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex> <span class=md-ellipsis> 7. K-Means Clustering </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> 7. K-Means Clustering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../07-k-means-clustering/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../07-k-means-clustering/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_9> <label class=md-nav__link for=__nav_5_9 id=__nav_5_9_label tabindex> <span class=md-ellipsis> 8. Data Preprocessing </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_9_label aria-expanded=false> <label class=md-nav__title for=__nav_5_9> <span class="md-nav__icon md-icon"></span> 8. Data Preprocessing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../08-data-preprocessing/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../08-data-preprocessing/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_10> <label class=md-nav__link for=__nav_5_10 id=__nav_5_10_label tabindex> <span class=md-ellipsis> 9. Neural Networks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_10_label aria-expanded=false> <label class=md-nav__title for=__nav_5_10> <span class="md-nav__icon md-icon"></span> 9. Neural Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../09-neural-networks/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../09-neural-networks/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_11> <label class=md-nav__link for=__nav_5_11 id=__nav_5_11_label tabindex> <span class=md-ellipsis> 10. Convolutional Networks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_11_label aria-expanded=false> <label class=md-nav__title for=__nav_5_11> <span class="md-nav__icon md-icon"></span> 10. Convolutional Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../10-convolutional-networks/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../10-convolutional-networks/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_12 checked> <label class=md-nav__link for=__nav_5_12 id=__nav_5_12_label tabindex> <span class=md-ellipsis> 11. Transfer Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_12_label aria-expanded=true> <label class=md-nav__title for=__nav_5_12> <span class="md-nav__icon md-icon"></span> 11. Transfer Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Content </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Content </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#concepts-covered class=md-nav__link> <span class=md-ellipsis> Concepts Covered </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#introduction-to-transfer-learning class=md-nav__link> <span class=md-ellipsis> Introduction to Transfer Learning </span> </a> </li> <li class=md-nav__item> <a href=#the-model-zoo-pre-trained-models class=md-nav__link> <span class=md-ellipsis> The Model Zoo: Pre-Trained Models </span> </a> </li> <li class=md-nav__item> <a href=#loading-pre-trained-models-in-pytorch class=md-nav__link> <span class=md-ellipsis> Loading Pre-Trained Models in PyTorch </span> </a> </li> <li class=md-nav__item> <a href=#data-preprocessing-for-pre-trained-models class=md-nav__link> <span class=md-ellipsis> Data Preprocessing for Pre-Trained Models </span> </a> </li> <li class=md-nav__item> <a href=#feature-extraction-freezing-the-base-network class=md-nav__link> <span class=md-ellipsis> Feature Extraction: Freezing the Base Network </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning-adapting-all-layers class=md-nav__link> <span class=md-ellipsis> Fine-Tuning: Adapting All Layers </span> </a> </li> <li class=md-nav__item> <a href=#training-loop-for-transfer-learning class=md-nav__link> <span class=md-ellipsis> Training Loop for Transfer Learning </span> </a> </li> <li class=md-nav__item> <a href=#validation-error-and-generalization class=md-nav__link> <span class=md-ellipsis> Validation Error and Generalization </span> </a> </li> <li class=md-nav__item> <a href=#case-study-ants-vs-bees-classification class=md-nav__link> <span class=md-ellipsis> Case Study: Ants vs. Bees Classification </span> </a> </li> <li class=md-nav__item> <a href=#optimizers-and-momentum class=md-nav__link> <span class=md-ellipsis> Optimizers and Momentum </span> </a> </li> <li class=md-nav__item> <a href=#domain-adaptation class=md-nav__link> <span class=md-ellipsis> Domain Adaptation </span> </a> </li> <li class=md-nav__item> <a href=#online-learning-and-continual-adaptation class=md-nav__link> <span class=md-ellipsis> Online Learning and Continual Adaptation </span> </a> </li> <li class=md-nav__item> <a href=#practical-tips-for-transfer-learning-success class=md-nav__link> <span class=md-ellipsis> Practical Tips for Transfer Learning Success </span> </a> </li> <li class=md-nav__item> <a href=#transfer-learning-beyond-image-classification class=md-nav__link> <span class=md-ellipsis> Transfer Learning Beyond Image Classification </span> </a> <nav class=md-nav aria-label="Transfer Learning Beyond Image Classification"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#transfer-learning-strategy-comparison class=md-nav__link> <span class=md-ellipsis> Transfer Learning Strategy Comparison </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#summary-and-key-takeaways class=md-nav__link> <span class=md-ellipsis> Summary and Key Takeaways </span> </a> </li> <li class=md-nav__item> <a href=#further-reading class=md-nav__link> <span class=md-ellipsis> Further Reading </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_13> <label class=md-nav__link for=__nav_5_13 id=__nav_5_13_label tabindex> <span class=md-ellipsis> 12. Evaluation & Optimization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_13_label aria-expanded=false> <label class=md-nav__title for=__nav_5_13> <span class="md-nav__icon md-icon"></span> 12. Evaluation & Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../12-evaluation-optimization/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../12-evaluation-optimization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> MicroSims </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> MicroSims </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../sims/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../sims/activation-functions/ class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=../../sims/categorical-encoding-explorer/ class=md-nav__link> <span class=md-ellipsis> Categorical Encoding Explorer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/cnn-architecture/ class=md-nav__link> <span class=md-ellipsis> CNN Architecture </span> </a> </li> <li class=md-nav__item> <a href=../../sims/confusion-matrix-explorer/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix Explorer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/convolution-operation/ class=md-nav__link> <span class=md-ellipsis> Convolution Operation </span> </a> </li> <li class=md-nav__item> <a href=../../sims/distance-metrics/ class=md-nav__link> <span class=md-ellipsis> Distance Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../sims/entropy-gini-comparison/ class=md-nav__link> <span class=md-ellipsis> Entropy-Gini Comparison </span> </a> </li> <li class=md-nav__item> <a href=../../sims/feature-scaling-visualizer/ class=md-nav__link> <span class=md-ellipsis> Feature Scaling Visualizer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/k-selection-simulator/ class=md-nav__link> <span class=md-ellipsis> K-Selection Simulator </span> </a> </li> <li class=md-nav__item> <a href=../../sims/kfold-cross-validation/ class=md-nav__link> <span class=md-ellipsis> K-Fold Cross Validation </span> </a> </li> <li class=md-nav__item> <a href=../../sims/lasso-regression-geometry/ class=md-nav__link> <span class=md-ellipsis> Lasso Regression Geometry </span> </a> </li> <li class=md-nav__item> <a href=../../sims/network-architecture-visualizer/ class=md-nav__link> <span class=md-ellipsis> Network Architecture Visualizer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/ridge-regression-geometry/ class=md-nav__link> <span class=md-ellipsis> Ridge Regression Geometry </span> </a> </li> <li class=md-nav__item> <a href=../../sims/roc-curve-comparison/ class=md-nav__link> <span class=md-ellipsis> ROC Curve Comparison </span> </a> </li> <li class=md-nav__item> <a href=../../sims/sigmoid-explorer/ class=md-nav__link> <span class=md-ellipsis> Sigmoid Explorer </span> </a> </li> <li class=md-nav__item> <a href=../../sims/svm-margin-maximization/ class=md-nav__link> <span class=md-ellipsis> SVM Margin Maximization </span> </a> </li> <li class=md-nav__item> <a href=../../sims/training-validation-curves/ class=md-nav__link> <span class=md-ellipsis> Training Validation Curves </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Learning Graph </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Learning Graph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../learning-graph/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../sims/graph-viewer/ class=md-nav__link> <span class=md-ellipsis> Graph Viewer </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/course-description-assessment/ class=md-nav__link> <span class=md-ellipsis> Course Description Assessment </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/concept-list/ class=md-nav__link> <span class=md-ellipsis> Concept List </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/concept-taxonomy/ class=md-nav__link> <span class=md-ellipsis> Concept Taxonomy </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/learning-graph.csv class=md-nav__link> <span class=md-ellipsis> Learning Graph (CSV) </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/learning-graph.json class=md-nav__link> <span class=md-ellipsis> Learning Graph (JSON) </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/quality-metrics/ class=md-nav__link> <span class=md-ellipsis> Quality Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/taxonomy-distribution/ class=md-nav__link> <span class=md-ellipsis> Taxonomy Distribution </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/glossary-quality-report/ class=md-nav__link> <span class=md-ellipsis> Glossary Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/faq-quality-report/ class=md-nav__link> <span class=md-ellipsis> FAQ Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/faq-coverage-gaps/ class=md-nav__link> <span class=md-ellipsis> FAQ Coverage Gaps </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/book-metrics/ class=md-nav__link> <span class=md-ellipsis> Book Metrics </span> </a> </li> <li class=md-nav__item> <a href=../../learning-graph/chapter-metrics/ class=md-nav__link> <span class=md-ellipsis> Chapter Metrics </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> <li class=md-nav__item> <a href=#concepts-covered class=md-nav__link> <span class=md-ellipsis> Concepts Covered </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#introduction-to-transfer-learning class=md-nav__link> <span class=md-ellipsis> Introduction to Transfer Learning </span> </a> </li> <li class=md-nav__item> <a href=#the-model-zoo-pre-trained-models class=md-nav__link> <span class=md-ellipsis> The Model Zoo: Pre-Trained Models </span> </a> </li> <li class=md-nav__item> <a href=#loading-pre-trained-models-in-pytorch class=md-nav__link> <span class=md-ellipsis> Loading Pre-Trained Models in PyTorch </span> </a> </li> <li class=md-nav__item> <a href=#data-preprocessing-for-pre-trained-models class=md-nav__link> <span class=md-ellipsis> Data Preprocessing for Pre-Trained Models </span> </a> </li> <li class=md-nav__item> <a href=#feature-extraction-freezing-the-base-network class=md-nav__link> <span class=md-ellipsis> Feature Extraction: Freezing the Base Network </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning-adapting-all-layers class=md-nav__link> <span class=md-ellipsis> Fine-Tuning: Adapting All Layers </span> </a> </li> <li class=md-nav__item> <a href=#training-loop-for-transfer-learning class=md-nav__link> <span class=md-ellipsis> Training Loop for Transfer Learning </span> </a> </li> <li class=md-nav__item> <a href=#validation-error-and-generalization class=md-nav__link> <span class=md-ellipsis> Validation Error and Generalization </span> </a> </li> <li class=md-nav__item> <a href=#case-study-ants-vs-bees-classification class=md-nav__link> <span class=md-ellipsis> Case Study: Ants vs. Bees Classification </span> </a> </li> <li class=md-nav__item> <a href=#optimizers-and-momentum class=md-nav__link> <span class=md-ellipsis> Optimizers and Momentum </span> </a> </li> <li class=md-nav__item> <a href=#domain-adaptation class=md-nav__link> <span class=md-ellipsis> Domain Adaptation </span> </a> </li> <li class=md-nav__item> <a href=#online-learning-and-continual-adaptation class=md-nav__link> <span class=md-ellipsis> Online Learning and Continual Adaptation </span> </a> </li> <li class=md-nav__item> <a href=#practical-tips-for-transfer-learning-success class=md-nav__link> <span class=md-ellipsis> Practical Tips for Transfer Learning Success </span> </a> </li> <li class=md-nav__item> <a href=#transfer-learning-beyond-image-classification class=md-nav__link> <span class=md-ellipsis> Transfer Learning Beyond Image Classification </span> </a> <nav class=md-nav aria-label="Transfer Learning Beyond Image Classification"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#transfer-learning-strategy-comparison class=md-nav__link> <span class=md-ellipsis> Transfer Learning Strategy Comparison </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#summary-and-key-takeaways class=md-nav__link> <span class=md-ellipsis> Summary and Key Takeaways </span> </a> </li> <li class=md-nav__item> <a href=#further-reading class=md-nav__link> <span class=md-ellipsis> Further Reading </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=transfer-learning-and-pre-trained-models>Transfer Learning and Pre-Trained Models<a class=headerlink href=#transfer-learning-and-pre-trained-models title="Permanent link">&para;</a></h1> <h2 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h2> <p>This chapter introduces transfer learning, a powerful technique that enables practitioners to leverage knowledge from pre-trained models to solve new tasks with limited data. Students will learn how to use pre-trained models from model zoos, understand the difference between feature extraction (using a frozen pre-trained model) and fine-tuning (adapting model weights to new data), and explore domain adaptation strategies for transferring knowledge across different but related domains. The chapter demonstrates practical transfer learning workflows using models pre-trained on ImageNet, showing how to effectively reuse learned representations while managing learning rates and layer freezing to achieve strong performance with minimal training data.</p> <h2 id=concepts-covered>Concepts Covered<a class=headerlink href=#concepts-covered title="Permanent link">&para;</a></h2> <p>This chapter covers the following 10 concepts from the learning graph:</p> <ol> <li>Transfer Learning</li> <li>Pre-Trained Model</li> <li>Fine-Tuning</li> <li>Feature Extraction</li> <li>Domain Adaptation</li> <li>Model Zoo</li> <li>Validation Error</li> <li>Online Learning</li> <li>Optimizer</li> <li>Momentum</li> </ol> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h2> <p>This chapter builds on concepts from:</p> <ul> <li><a href=../09-neural-networks/ >Chapter 9: Neural Networks Fundamentals</a></li> <li><a href=../10-convolutional-networks/ >Chapter 10: Convolutional Neural Networks for Computer Vision</a></li> </ul> <hr> <h2 id=introduction-to-transfer-learning>Introduction to Transfer Learning<a class=headerlink href=#introduction-to-transfer-learning title="Permanent link">&para;</a></h2> <p>Imagine you're learning to play the piano. Would you start completely from scratch, not using any knowledge from previous musical experiences? Of course not! You'd leverage what you already know about reading music, rhythm, and hand coordination. Transfer learning in machine learning works the same way—it allows us to take knowledge learned from one task and apply it to a new, related task.</p> <p>In deep learning, training large neural networks from scratch requires enormous amounts of labeled data and computational resources. A ResNet-50 trained on ImageNet uses 25 million parameters and was trained on 1.2 million labeled images across 1,000 categories. Most practitioners don't have access to datasets of this scale or the computational budget to train such models from scratch.</p> <p>Transfer learning solves this problem by allowing us to start with a <strong>pre-trained model</strong>—a neural network that has already learned useful feature representations from a large dataset—and adapt it to our specific task. This approach is particularly powerful when working with small datasets, where training from scratch would lead to severe overfitting.</p> <p>The fundamental insight behind transfer learning is that the features learned by deep neural networks on large-scale tasks (like ImageNet classification) are often transferable to other computer vision tasks. Early layers in CNNs learn general features like edges, textures, and simple shapes, while later layers learn increasingly task-specific features. By reusing the general features and only retraining the task-specific layers, we can achieve excellent performance with far less data and computation.</p> <h2 id=the-model-zoo-pre-trained-models>The Model Zoo: Pre-Trained Models<a class=headerlink href=#the-model-zoo-pre-trained-models title="Permanent link">&para;</a></h2> <p>A <strong>model zoo</strong> is a collection of pre-trained neural network models that have been trained on large-scale benchmark datasets and made publicly available for reuse. The most famous model zoo consists of networks trained on ImageNet, a dataset containing 1.2 million training images across 1,000 object categories.</p> <p>Popular pre-trained models available in PyTorch's model zoo include:</p> <ul> <li><strong>ResNet</strong> (18, 34, 50, 101, 152 layers): Deep residual networks with skip connections</li> <li><strong>VGG</strong> (16, 19 layers): Very deep convolutional networks with small filters</li> <li><strong>AlexNet</strong>: The breakthrough architecture that won ImageNet 2012</li> <li><strong>DenseNet</strong>: Networks with dense connections between layers</li> <li><strong>MobileNet</strong>: Lightweight networks optimized for mobile devices</li> <li><strong>EfficientNet</strong>: Networks optimized for both accuracy and efficiency</li> </ul> <p>These models achieve impressive accuracy on ImageNet classification, with top-5 error rates (the percentage of test images where the correct label is not among the model's top 5 predictions) often below 5%. The learned feature representations from these models transfer remarkably well to other computer vision tasks.</p> <div class="admonition tip"> <p class=admonition-title>Choosing a Pre-Trained Model</p> <p>When selecting a pre-trained model, consider the tradeoff between accuracy and computational cost. ResNet-18 is faster and requires less memory than ResNet-152, but may achieve slightly lower accuracy. For most transfer learning tasks on moderate-sized datasets, ResNet-18 or ResNet-50 provide an excellent balance.</p> </div> <h2 id=loading-pre-trained-models-in-pytorch>Loading Pre-Trained Models in PyTorch<a class=headerlink href=#loading-pre-trained-models-in-pytorch title="Permanent link">&para;</a></h2> <p>PyTorch makes it straightforward to load pre-trained models from the <code>torchvision.models</code> module. Here's how to load a ResNet-18 model with weights pre-trained on ImageNet:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torchvision</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>models</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=c1># Load a pre-trained ResNet-18 model</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=n>model_ft</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=s1>&#39;IMAGENET1K_V1&#39;</span><span class=p>)</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=c1># Examine the architecture</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=nb>print</span><span class=p>(</span><span class=n>model_ft</span><span class=p>)</span>
</span></code></pre></div> <p>The <code>weights='IMAGENET1K_V1'</code> parameter loads the official ImageNet pre-trained weights. The model architecture includes:</p> <ul> <li>An initial convolutional layer with batch normalization and ReLU activation</li> <li>Four residual layer groups with increasing channel depths (64, 128, 256, 512)</li> <li>An adaptive average pooling layer</li> <li>A final fully connected layer with 1,000 outputs (one for each ImageNet class)</li> </ul> <p>For our custom task, we'll need to modify this architecture—specifically, we'll replace the final fully connected layer to match our number of output classes.</p> <h2 id=data-preprocessing-for-pre-trained-models>Data Preprocessing for Pre-Trained Models<a class=headerlink href=#data-preprocessing-for-pre-trained-models title="Permanent link">&para;</a></h2> <p>When using pre-trained models, it's crucial to preprocess input images in the same way the model was trained. All ImageNet pre-trained models expect input images to be normalized with specific mean and standard deviation values:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>transforms</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=c1># Data augmentation and normalization for training</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=n>data_transforms</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>    <span class=s1>&#39;train&#39;</span><span class=p>:</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>RandomResizedCrop</span><span class=p>(</span><span class=mi>224</span><span class=p>),</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>RandomHorizontalFlip</span><span class=p>(),</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>([</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>])</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a>    <span class=p>]),</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a>    <span class=s1>&#39;val&#39;</span><span class=p>:</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>CenterCrop</span><span class=p>(</span><span class=mi>224</span><span class=p>),</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a>        <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>([</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>])</span>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a>    <span class=p>]),</span>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a><span class=p>}</span>
</span></code></pre></div> <p>Let's break down these transformations:</p> <p><strong>Training transformations:</strong></p> <ul> <li><code>RandomResizedCrop(224)</code>: Randomly crops the image to 224×224 pixels with random scale and aspect ratio—this is <strong>data augmentation</strong> that helps the model generalize</li> <li><code>RandomHorizontalFlip()</code>: Randomly flips images horizontally with 50% probability—another augmentation technique</li> <li><code>ToTensor()</code>: Converts PIL images or NumPy arrays to PyTorch tensors with values in [0, 1]</li> <li><code>Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</code>: Normalizes each channel using ImageNet statistics</li> </ul> <p><strong>Validation transformations:</strong></p> <ul> <li><code>Resize(256)</code>: Resizes the smaller edge to 256 pixels while maintaining aspect ratio</li> <li><code>CenterCrop(224)</code>: Takes a 224×224 center crop</li> <li>No augmentation is applied during validation—we want consistent evaluation</li> </ul> <p>The normalization values <code>[0.485, 0.456, 0.406]</code> (mean) and <code>[0.229, 0.224, 0.225]</code> (standard deviation) are the channel-wise statistics computed across the entire ImageNet training set. Using these same values ensures that the pre-trained model receives inputs in the distribution it expects.</p> <h4 id=normalization-formula>Normalization Formula<a class=headerlink href=#normalization-formula title="Permanent link">&para;</a></h4> <p>For each pixel value <span class=arithmatex>\(x\)</span> in channel <span class=arithmatex>\(c\)</span>, the normalized value <span class=arithmatex>\(x'\)</span> is:</p> <p><span class=arithmatex>\(x' = \frac{x - \mu_c}{\sigma_c}\)</span></p> <p>where:</p> <ul> <li><span class=arithmatex>\(x\)</span> is the original pixel value in range [0, 1]</li> <li><span class=arithmatex>\(\mu_c\)</span> is the mean for channel <span class=arithmatex>\(c\)</span> (e.g., 0.485 for red channel)</li> <li><span class=arithmatex>\(\sigma_c\)</span> is the standard deviation for channel <span class=arithmatex>\(c\)</span> (e.g., 0.229 for red channel)</li> <li><span class=arithmatex>\(x'\)</span> is the normalized pixel value</li> </ul> <h2 id=feature-extraction-freezing-the-base-network>Feature Extraction: Freezing the Base Network<a class=headerlink href=#feature-extraction-freezing-the-base-network title="Permanent link">&para;</a></h2> <p><strong>Feature extraction</strong> is the simplest form of transfer learning. In this approach, we treat the pre-trained model as a fixed feature extractor, freezing all convolutional layers and only training a new classification head for our specific task.</p> <p>The rationale is straightforward: the convolutional layers of a network trained on ImageNet have already learned to extract useful visual features (edges, textures, object parts, etc.). These features are generally applicable to many computer vision tasks. By freezing these layers, we prevent their weights from updating during training, dramatically reducing the number of parameters we need to learn.</p> <p>Here's how to implement feature extraction with a frozen ResNet-18:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=kn>import</span> <span class=n>lr_scheduler</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=c1># Load pre-trained ResNet-18</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=n>model_conv</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=s1>&#39;IMAGENET1K_V1&#39;</span><span class=p>)</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=c1># Freeze all parameters in the base network</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model_conv</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a><span class=c1># Replace the final fully connected layer</span>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=c1># New layers have requires_grad=True by default</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=n>num_ftrs</span> <span class=o>=</span> <span class=n>model_conv</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>in_features</span>  <span class=c1># 512 for ResNet-18</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a><span class=n>model_conv</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_ftrs</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># 2 classes: ants and bees</span>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a>
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a><span class=c1># Move model to GPU if available</span>
</span><span id=__span-2-17><a id=__codelineno-2-17 name=__codelineno-2-17 href=#__codelineno-2-17></a><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&quot;cuda:0&quot;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&quot;cpu&quot;</span><span class=p>)</span>
</span><span id=__span-2-18><a id=__codelineno-2-18 name=__codelineno-2-18 href=#__codelineno-2-18></a><span class=n>model_conv</span> <span class=o>=</span> <span class=n>model_conv</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-2-19><a id=__codelineno-2-19 name=__codelineno-2-19 href=#__codelineno-2-19></a>
</span><span id=__span-2-20><a id=__codelineno-2-20 name=__codelineno-2-20 href=#__codelineno-2-20></a><span class=c1># Only optimize parameters of the final layer</span>
</span><span id=__span-2-21><a id=__codelineno-2-21 name=__codelineno-2-21 href=#__codelineno-2-21></a><span class=n>optimizer_conv</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model_conv</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span><span id=__span-2-22><a id=__codelineno-2-22 name=__codelineno-2-22 href=#__codelineno-2-22></a>
</span><span id=__span-2-23><a id=__codelineno-2-23 name=__codelineno-2-23 href=#__codelineno-2-23></a><span class=c1># Learning rate scheduler: decay LR by 0.1 every 7 epochs</span>
</span><span id=__span-2-24><a id=__codelineno-2-24 name=__codelineno-2-24 href=#__codelineno-2-24></a><span class=n>exp_lr_scheduler</span> <span class=o>=</span> <span class=n>lr_scheduler</span><span class=o>.</span><span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer_conv</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>7</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span><span id=__span-2-25><a id=__codelineno-2-25 name=__codelineno-2-25 href=#__codelineno-2-25></a>
</span><span id=__span-2-26><a id=__codelineno-2-26 name=__codelineno-2-26 href=#__codelineno-2-26></a><span class=c1># Loss function</span>
</span><span id=__span-2-27><a id=__codelineno-2-27 name=__codelineno-2-27 href=#__codelineno-2-27></a><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></code></pre></div> <p>The key steps are:</p> <ol> <li><strong>Freeze base network</strong>: Setting <code>param.requires_grad = False</code> for all parameters prevents gradient computation and weight updates for those layers</li> <li><strong>Replace classification head</strong>: The original <code>fc</code> layer outputs 1,000 classes (ImageNet); we replace it with a new layer outputting 2 classes (ants vs. bees)</li> <li><strong>Optimize only new layers</strong>: We pass only <code>model_conv.fc.parameters()</code> to the optimizer, so only the new classification head is trained</li> </ol> <p>This approach is extremely efficient when you have a small dataset. Training only the final layer requires far less computation and memory than training the entire network, and it's less prone to overfitting since we're only learning a linear classifier on top of robust features.</p> <h2 id=fine-tuning-adapting-all-layers>Fine-Tuning: Adapting All Layers<a class=headerlink href=#fine-tuning-adapting-all-layers title="Permanent link">&para;</a></h2> <p>While feature extraction works well for many tasks, <strong>fine-tuning</strong> can achieve even better performance by allowing the entire network to adapt to the new task. In fine-tuning, we initialize the network with pre-trained weights, then train all layers (or a subset of later layers) with a small learning rate.</p> <p>The intuition is that while early layers learn general features applicable to many tasks, later layers learn increasingly task-specific features. By fine-tuning, we allow the network to adjust these features for our specific domain while still benefiting from the initialization provided by pre-training.</p> <p>Here's how to implement fine-tuning:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Load pre-trained ResNet-18</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>model_ft</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=s1>&#39;IMAGENET1K_V1&#39;</span><span class=p>)</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=c1># Replace the final fully connected layer</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=n>num_ftrs</span> <span class=o>=</span> <span class=n>model_ft</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>in_features</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=n>model_ft</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_ftrs</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=n>model_ft</span> <span class=o>=</span> <span class=n>model_ft</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=c1># Optimize ALL parameters (not just the final layer)</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a><span class=n>optimizer_ft</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model_ft</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=c1># Decay learning rate by 0.1 every 7 epochs</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=n>exp_lr_scheduler</span> <span class=o>=</span> <span class=n>lr_scheduler</span><span class=o>.</span><span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer_ft</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>7</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=c1># Loss function</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></code></pre></div> <p>The key difference from feature extraction is that we optimize <code>model_ft.parameters()</code> (all parameters) rather than just <code>model_ft.fc.parameters()</code> (final layer only).</p> <div class="admonition warning"> <p class=admonition-title>Learning Rate for Fine-Tuning</p> <p>When fine-tuning, use a smaller learning rate than you would for training from scratch. The pre-trained weights are already in a good region of the parameter space, so aggressive updates can destroy the learned features. Learning rates like 0.001 or 0.0001 are typical for fine-tuning.</p> </div> <h2 id=training-loop-for-transfer-learning>Training Loop for Transfer Learning<a class=headerlink href=#training-loop-for-transfer-learning title="Permanent link">&para;</a></h2> <p>Both feature extraction and fine-tuning use the same training loop structure. Here's a complete training function that handles both training and validation:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>import</span><span class=w> </span><span class=nn>time</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=kn>import</span><span class=w> </span><span class=nn>copy</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=k>def</span><span class=w> </span><span class=nf>train_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>num_epochs</span><span class=o>=</span><span class=mi>25</span><span class=p>):</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>    <span class=n>since</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>    <span class=n>best_model_wts</span> <span class=o>=</span> <span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>())</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a>    <span class=n>best_acc</span> <span class=o>=</span> <span class=mf>0.0</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch </span><span class=si>{}</span><span class=s1>/</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>num_epochs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>))</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;-&#39;</span> <span class=o>*</span> <span class=mi>10</span><span class=p>)</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a>        <span class=c1># Each epoch has a training and validation phase</span>
</span><span id=__span-4-15><a id=__codelineno-4-15 name=__codelineno-4-15 href=#__codelineno-4-15></a>        <span class=k>for</span> <span class=n>phase</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>]:</span>
</span><span id=__span-4-16><a id=__codelineno-4-16 name=__codelineno-4-16 href=#__codelineno-4-16></a>            <span class=k>if</span> <span class=n>phase</span> <span class=o>==</span> <span class=s1>&#39;train&#39;</span><span class=p>:</span>
</span><span id=__span-4-17><a id=__codelineno-4-17 name=__codelineno-4-17 href=#__codelineno-4-17></a>                <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>  <span class=c1># Set model to training mode</span>
</span><span id=__span-4-18><a id=__codelineno-4-18 name=__codelineno-4-18 href=#__codelineno-4-18></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-4-19><a id=__codelineno-4-19 name=__codelineno-4-19 href=#__codelineno-4-19></a>                <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>   <span class=c1># Set model to evaluate mode</span>
</span><span id=__span-4-20><a id=__codelineno-4-20 name=__codelineno-4-20 href=#__codelineno-4-20></a>
</span><span id=__span-4-21><a id=__codelineno-4-21 name=__codelineno-4-21 href=#__codelineno-4-21></a>            <span class=n>running_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span><span id=__span-4-22><a id=__codelineno-4-22 name=__codelineno-4-22 href=#__codelineno-4-22></a>            <span class=n>running_corrects</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-4-23><a id=__codelineno-4-23 name=__codelineno-4-23 href=#__codelineno-4-23></a>
</span><span id=__span-4-24><a id=__codelineno-4-24 name=__codelineno-4-24 href=#__codelineno-4-24></a>            <span class=c1># Iterate over data batches</span>
</span><span id=__span-4-25><a id=__codelineno-4-25 name=__codelineno-4-25 href=#__codelineno-4-25></a>            <span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>dataloaders</span><span class=p>[</span><span class=n>phase</span><span class=p>]:</span>
</span><span id=__span-4-26><a id=__codelineno-4-26 name=__codelineno-4-26 href=#__codelineno-4-26></a>                <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-4-27><a id=__codelineno-4-27 name=__codelineno-4-27 href=#__codelineno-4-27></a>                <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-4-28><a id=__codelineno-4-28 name=__codelineno-4-28 href=#__codelineno-4-28></a>
</span><span id=__span-4-29><a id=__codelineno-4-29 name=__codelineno-4-29 href=#__codelineno-4-29></a>                <span class=c1># Zero the parameter gradients</span>
</span><span id=__span-4-30><a id=__codelineno-4-30 name=__codelineno-4-30 href=#__codelineno-4-30></a>                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-4-31><a id=__codelineno-4-31 name=__codelineno-4-31 href=#__codelineno-4-31></a>
</span><span id=__span-4-32><a id=__codelineno-4-32 name=__codelineno-4-32 href=#__codelineno-4-32></a>                <span class=c1># Forward pass</span>
</span><span id=__span-4-33><a id=__codelineno-4-33 name=__codelineno-4-33 href=#__codelineno-4-33></a>                <span class=c1># Track gradients only in training phase</span>
</span><span id=__span-4-34><a id=__codelineno-4-34 name=__codelineno-4-34 href=#__codelineno-4-34></a>                <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>set_grad_enabled</span><span class=p>(</span><span class=n>phase</span> <span class=o>==</span> <span class=s1>&#39;train&#39;</span><span class=p>):</span>
</span><span id=__span-4-35><a id=__codelineno-4-35 name=__codelineno-4-35 href=#__codelineno-4-35></a>                    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span><span id=__span-4-36><a id=__codelineno-4-36 name=__codelineno-4-36 href=#__codelineno-4-36></a>                    <span class=n>_</span><span class=p>,</span> <span class=n>preds</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-4-37><a id=__codelineno-4-37 name=__codelineno-4-37 href=#__codelineno-4-37></a>                    <span class=n>outputs</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-4-38><a id=__codelineno-4-38 name=__codelineno-4-38 href=#__codelineno-4-38></a>                    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span><span id=__span-4-39><a id=__codelineno-4-39 name=__codelineno-4-39 href=#__codelineno-4-39></a>
</span><span id=__span-4-40><a id=__codelineno-4-40 name=__codelineno-4-40 href=#__codelineno-4-40></a>                    <span class=c1># Backward pass and optimization only in training phase</span>
</span><span id=__span-4-41><a id=__codelineno-4-41 name=__codelineno-4-41 href=#__codelineno-4-41></a>                    <span class=k>if</span> <span class=n>phase</span> <span class=o>==</span> <span class=s1>&#39;train&#39;</span><span class=p>:</span>
</span><span id=__span-4-42><a id=__codelineno-4-42 name=__codelineno-4-42 href=#__codelineno-4-42></a>                        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-4-43><a id=__codelineno-4-43 name=__codelineno-4-43 href=#__codelineno-4-43></a>                        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-4-44><a id=__codelineno-4-44 name=__codelineno-4-44 href=#__codelineno-4-44></a>
</span><span id=__span-4-45><a id=__codelineno-4-45 name=__codelineno-4-45 href=#__codelineno-4-45></a>                <span class=c1># Accumulate statistics</span>
</span><span id=__span-4-46><a id=__codelineno-4-46 name=__codelineno-4-46 href=#__codelineno-4-46></a>                <span class=n>running_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>*</span> <span class=n>inputs</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-4-47><a id=__codelineno-4-47 name=__codelineno-4-47 href=#__codelineno-4-47></a>                <span class=n>running_corrects</span> <span class=o>+=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>preds</span> <span class=o>==</span> <span class=n>labels</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span><span id=__span-4-48><a id=__codelineno-4-48 name=__codelineno-4-48 href=#__codelineno-4-48></a>
</span><span id=__span-4-49><a id=__codelineno-4-49 name=__codelineno-4-49 href=#__codelineno-4-49></a>            <span class=c1># Step the learning rate scheduler after each training epoch</span>
</span><span id=__span-4-50><a id=__codelineno-4-50 name=__codelineno-4-50 href=#__codelineno-4-50></a>            <span class=k>if</span> <span class=n>phase</span> <span class=o>==</span> <span class=s1>&#39;train&#39;</span><span class=p>:</span>
</span><span id=__span-4-51><a id=__codelineno-4-51 name=__codelineno-4-51 href=#__codelineno-4-51></a>                <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-4-52><a id=__codelineno-4-52 name=__codelineno-4-52 href=#__codelineno-4-52></a>
</span><span id=__span-4-53><a id=__codelineno-4-53 name=__codelineno-4-53 href=#__codelineno-4-53></a>            <span class=c1># Calculate epoch statistics</span>
</span><span id=__span-4-54><a id=__codelineno-4-54 name=__codelineno-4-54 href=#__codelineno-4-54></a>            <span class=n>epoch_loss</span> <span class=o>=</span> <span class=n>running_loss</span> <span class=o>/</span> <span class=n>dataset_sizes</span><span class=p>[</span><span class=n>phase</span><span class=p>]</span>
</span><span id=__span-4-55><a id=__codelineno-4-55 name=__codelineno-4-55 href=#__codelineno-4-55></a>            <span class=n>epoch_acc</span> <span class=o>=</span> <span class=n>running_corrects</span><span class=o>.</span><span class=n>double</span><span class=p>()</span> <span class=o>/</span> <span class=n>dataset_sizes</span><span class=p>[</span><span class=n>phase</span><span class=p>]</span>
</span><span id=__span-4-56><a id=__codelineno-4-56 name=__codelineno-4-56 href=#__codelineno-4-56></a>
</span><span id=__span-4-57><a id=__codelineno-4-57 name=__codelineno-4-57 href=#__codelineno-4-57></a>            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;</span><span class=si>{}</span><span class=s1> Loss: </span><span class=si>{:.4f}</span><span class=s1> Acc: </span><span class=si>{:.4f}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>phase</span><span class=p>,</span> <span class=n>epoch_loss</span><span class=p>,</span> <span class=n>epoch_acc</span><span class=p>))</span>
</span><span id=__span-4-58><a id=__codelineno-4-58 name=__codelineno-4-58 href=#__codelineno-4-58></a>
</span><span id=__span-4-59><a id=__codelineno-4-59 name=__codelineno-4-59 href=#__codelineno-4-59></a>            <span class=c1># Save the best model based on validation accuracy</span>
</span><span id=__span-4-60><a id=__codelineno-4-60 name=__codelineno-4-60 href=#__codelineno-4-60></a>            <span class=k>if</span> <span class=n>phase</span> <span class=o>==</span> <span class=s1>&#39;val&#39;</span> <span class=ow>and</span> <span class=n>epoch_acc</span> <span class=o>&gt;</span> <span class=n>best_acc</span><span class=p>:</span>
</span><span id=__span-4-61><a id=__codelineno-4-61 name=__codelineno-4-61 href=#__codelineno-4-61></a>                <span class=n>best_acc</span> <span class=o>=</span> <span class=n>epoch_acc</span>
</span><span id=__span-4-62><a id=__codelineno-4-62 name=__codelineno-4-62 href=#__codelineno-4-62></a>                <span class=n>best_model_wts</span> <span class=o>=</span> <span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>())</span>
</span><span id=__span-4-63><a id=__codelineno-4-63 name=__codelineno-4-63 href=#__codelineno-4-63></a>
</span><span id=__span-4-64><a id=__codelineno-4-64 name=__codelineno-4-64 href=#__codelineno-4-64></a>        <span class=nb>print</span><span class=p>()</span>
</span><span id=__span-4-65><a id=__codelineno-4-65 name=__codelineno-4-65 href=#__codelineno-4-65></a>
</span><span id=__span-4-66><a id=__codelineno-4-66 name=__codelineno-4-66 href=#__codelineno-4-66></a>    <span class=n>time_elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>since</span>
</span><span id=__span-4-67><a id=__codelineno-4-67 name=__codelineno-4-67 href=#__codelineno-4-67></a>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Training complete in </span><span class=si>{:.0f}</span><span class=s1>m </span><span class=si>{:.0f}</span><span class=s1>s&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span><span id=__span-4-68><a id=__codelineno-4-68 name=__codelineno-4-68 href=#__codelineno-4-68></a>        <span class=n>time_elapsed</span> <span class=o>//</span> <span class=mi>60</span><span class=p>,</span> <span class=n>time_elapsed</span> <span class=o>%</span> <span class=mi>60</span><span class=p>))</span>
</span><span id=__span-4-69><a id=__codelineno-4-69 name=__codelineno-4-69 href=#__codelineno-4-69></a>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Best val Acc: </span><span class=si>{:4f}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>best_acc</span><span class=p>))</span>
</span><span id=__span-4-70><a id=__codelineno-4-70 name=__codelineno-4-70 href=#__codelineno-4-70></a>
</span><span id=__span-4-71><a id=__codelineno-4-71 name=__codelineno-4-71 href=#__codelineno-4-71></a>    <span class=c1># Load best model weights</span>
</span><span id=__span-4-72><a id=__codelineno-4-72 name=__codelineno-4-72 href=#__codelineno-4-72></a>    <span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>best_model_wts</span><span class=p>)</span>
</span><span id=__span-4-73><a id=__codelineno-4-73 name=__codelineno-4-73 href=#__codelineno-4-73></a>    <span class=k>return</span> <span class=n>model</span>
</span></code></pre></div> <p>This training function implements several important practices:</p> <p><strong>Model modes:</strong></p> <ul> <li><code>model.train()</code>: Enables training mode, where dropout and batch normalization behave appropriately for training</li> <li><code>model.eval()</code>: Enables evaluation mode, where dropout is disabled and batch normalization uses running statistics</li> </ul> <p><strong>Gradient tracking:</strong></p> <ul> <li><code>torch.set_grad_enabled(phase == 'train')</code>: Disables gradient computation during validation, saving memory and computation</li> </ul> <p><strong>Learning rate scheduling:</strong></p> <ul> <li><code>scheduler.step()</code>: Reduces the learning rate according to the schedule (e.g., multiply by 0.1 every 7 epochs)</li> </ul> <p><strong>Model checkpointing:</strong></p> <ul> <li>The function saves the model weights that achieve the best validation accuracy, preventing overfitting to the training set</li> </ul> <h2 id=validation-error-and-generalization>Validation Error and Generalization<a class=headerlink href=#validation-error-and-generalization title="Permanent link">&para;</a></h2> <p><strong>Validation error</strong> measures how well your model performs on data it hasn't seen during training. In transfer learning, monitoring validation error is crucial for several reasons:</p> <ol> <li><strong>Early stopping</strong>: If validation error starts increasing while training error continues decreasing, the model is overfitting</li> <li><strong>Hyperparameter tuning</strong>: Validation error helps you compare different learning rates, architectures, or augmentation strategies</li> <li><strong>Model selection</strong>: The checkpoint with lowest validation error is typically the best model to deploy</li> </ol> <p>The relationship between training and validation error reveals important information about your model:</p> <table> <thead> <tr> <th>Training Error</th> <th>Validation Error</th> <th>Diagnosis</th> <th>Solution</th> </tr> </thead> <tbody> <tr> <td>High</td> <td>High</td> <td>Underfitting</td> <td>More training, larger model, less regularization</td> </tr> <tr> <td>Low</td> <td>High</td> <td>Overfitting</td> <td>More data, data augmentation, regularization, early stopping</td> </tr> <tr> <td>Low</td> <td>Low</td> <td>Good fit</td> <td>Model is ready for deployment</td> </tr> </tbody> </table> <p>In our ants vs. bees example with only 120 training images per class, a model trained from scratch would likely show low training error but high validation error (overfitting). Transfer learning achieves both low training and validation error by leveraging pre-trained features.</p> <p>Visualize how training and validation errors evolve during model training:</p> <iframe src=../../sims/training-validation-curves/main.html width=100% height=750 style="border: 1px solid #ccc; border-radius: 4px;"></iframe> <p><a class=md-button href=../../sims/training-validation-curves/main.html target=_blank>View Fullscreen</a> | <a href=../../sims/training-validation-curves/ >Documentation</a></p> <h2 id=case-study-ants-vs-bees-classification>Case Study: Ants vs. Bees Classification<a class=headerlink href=#case-study-ants-vs-bees-classification title="Permanent link">&para;</a></h2> <p>Let's walk through a complete transfer learning example classifying images of ants and bees. This dataset is deliberately small (only 120 training images per class and 75 validation images per class) to demonstrate transfer learning's effectiveness with limited data.</p> <p>First, we set up the dataset and dataloaders:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=n>data_dir</span> <span class=o>=</span> <span class=s1>&#39;hymenoptera_data&#39;</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=n>image_datasets</span> <span class=o>=</span> <span class=p>{</span><span class=n>x</span><span class=p>:</span> <span class=n>datasets</span><span class=o>.</span><span class=n>ImageFolder</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>data_dir</span><span class=p>,</span> <span class=n>x</span><span class=p>),</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>                                          <span class=n>data_transforms</span><span class=p>[</span><span class=n>x</span><span class=p>])</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>                  <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>]}</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=n>dataloaders</span> <span class=o>=</span> <span class=p>{</span><span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>image_datasets</span><span class=p>[</span><span class=n>x</span><span class=p>],</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>                                               <span class=n>batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>                                               <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a>                                               <span class=n>num_workers</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a>               <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>]}</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a><span class=n>dataset_sizes</span> <span class=o>=</span> <span class=p>{</span><span class=n>x</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>image_datasets</span><span class=p>[</span><span class=n>x</span><span class=p>])</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>]}</span>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a><span class=n>class_names</span> <span class=o>=</span> <span class=n>image_datasets</span><span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>classes</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&quot;cuda:0&quot;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&quot;cpu&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>The <code>ImageFolder</code> class automatically creates a dataset from a directory structure where each subdirectory name is a class label:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>hymenoptera_data/
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>├── train/
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>│   ├── ants/
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>│   │   ├── image1.jpg
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>│   │   ├── image2.jpg
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>│   │   └── ...
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>│   └── bees/
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>│       ├── image1.jpg
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>│       └── ...
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>└── val/
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>    ├── ants/
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a>    └── bees/
</span></code></pre></div> <p>Now we train the model using fine-tuning (all layers trainable):</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=n>model_ft</span> <span class=o>=</span> <span class=n>train_model</span><span class=p>(</span><span class=n>model_ft</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>optimizer_ft</span><span class=p>,</span> <span class=n>exp_lr_scheduler</span><span class=p>,</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>                       <span class=n>num_epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></code></pre></div> <p>After just 5 epochs of fine-tuning, we achieve remarkable results:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>Epoch 0/4
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>----------
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>train Loss: 0.5150 Acc: 0.7213
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>val Loss: 0.1700 Acc: 0.9346
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>Epoch 1/4
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a>----------
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a>train Loss: 0.6623 Acc: 0.7172
</span><span id=__span-8-9><a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a>val Loss: 0.1956 Acc: 0.9150
</span><span id=__span-8-10><a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a>
</span><span id=__span-8-11><a id=__codelineno-8-11 name=__codelineno-8-11 href=#__codelineno-8-11></a>Epoch 2/4
</span><span id=__span-8-12><a id=__codelineno-8-12 name=__codelineno-8-12 href=#__codelineno-8-12></a>----------
</span><span id=__span-8-13><a id=__codelineno-8-13 name=__codelineno-8-13 href=#__codelineno-8-13></a>train Loss: 0.6573 Acc: 0.7705
</span><span id=__span-8-14><a id=__codelineno-8-14 name=__codelineno-8-14 href=#__codelineno-8-14></a>val Loss: 0.4305 Acc: 0.8431
</span><span id=__span-8-15><a id=__codelineno-8-15 name=__codelineno-8-15 href=#__codelineno-8-15></a>
</span><span id=__span-8-16><a id=__codelineno-8-16 name=__codelineno-8-16 href=#__codelineno-8-16></a>Epoch 3/4
</span><span id=__span-8-17><a id=__codelineno-8-17 name=__codelineno-8-17 href=#__codelineno-8-17></a>----------
</span><span id=__span-8-18><a id=__codelineno-8-18 name=__codelineno-8-18 href=#__codelineno-8-18></a>train Loss: 0.4789 Acc: 0.8033
</span><span id=__span-8-19><a id=__codelineno-8-19 name=__codelineno-8-19 href=#__codelineno-8-19></a>val Loss: 0.2175 Acc: 0.9085
</span><span id=__span-8-20><a id=__codelineno-8-20 name=__codelineno-8-20 href=#__codelineno-8-20></a>
</span><span id=__span-8-21><a id=__codelineno-8-21 name=__codelineno-8-21 href=#__codelineno-8-21></a>Epoch 4/4
</span><span id=__span-8-22><a id=__codelineno-8-22 name=__codelineno-8-22 href=#__codelineno-8-22></a>----------
</span><span id=__span-8-23><a id=__codelineno-8-23 name=__codelineno-8-23 href=#__codelineno-8-23></a>train Loss: 0.4125 Acc: 0.8197
</span><span id=__span-8-24><a id=__codelineno-8-24 name=__codelineno-8-24 href=#__codelineno-8-24></a>val Loss: 0.1653 Acc: 0.9477
</span><span id=__span-8-25><a id=__codelineno-8-25 name=__codelineno-8-25 href=#__codelineno-8-25></a>
</span><span id=__span-8-26><a id=__codelineno-8-26 name=__codelineno-8-26 href=#__codelineno-8-26></a>Training complete in 7m 38s
</span><span id=__span-8-27><a id=__codelineno-8-27 name=__codelineno-8-27 href=#__codelineno-8-27></a>Best val Acc: 0.947712
</span></code></pre></div> <p>The model achieves <strong>94.77% validation accuracy</strong> with only 240 total training images! This demonstrates the power of transfer learning—training a model from scratch on such a small dataset would likely achieve only 60-70% accuracy.</p> <p>Compare this with feature extraction (frozen base network, only final layer trained):</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=n>model_conv</span> <span class=o>=</span> <span class=n>train_model</span><span class=p>(</span><span class=n>model_conv</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>optimizer_conv</span><span class=p>,</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>                         <span class=n>exp_lr_scheduler</span><span class=p>,</span> <span class=n>num_epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></code></pre></div> <p>Results after 5 epochs:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>Epoch 0/4
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>----------
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>train Loss: 0.6429 Acc: 0.5943
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>val Loss: 0.3860 Acc: 0.8301
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>Epoch 4/4
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>----------
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>train Loss: 0.5453 Acc: 0.7787
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a>val Loss: 0.4029 Acc: 0.8497
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a>
</span><span id=__span-10-11><a id=__codelineno-10-11 name=__codelineno-10-11 href=#__codelineno-10-11></a>Training complete in 3m 46s
</span><span id=__span-10-12><a id=__codelineno-10-12 name=__codelineno-10-12 href=#__codelineno-10-12></a>Best val Acc: 0.947712
</span></code></pre></div> <p>Feature extraction also achieves 94.77% validation accuracy, but with significantly faster training (3m 46s vs. 7m 38s) since we're only updating the final layer. For this particular task, the frozen features are sufficiently powerful that fine-tuning provides no additional benefit.</p> <h2 id=optimizers-and-momentum>Optimizers and Momentum<a class=headerlink href=#optimizers-and-momentum title="Permanent link">&para;</a></h2> <p>An <strong>optimizer</strong> is an algorithm that updates model parameters to minimize the loss function. The most common optimizer for transfer learning is <strong>Stochastic Gradient Descent (SGD)</strong> with <strong>momentum</strong>.</p> <p>Standard SGD updates parameters using the gradient:</p> <h4 id=sgd-update-rule>SGD Update Rule<a class=headerlink href=#sgd-update-rule title="Permanent link">&para;</a></h4> <p><span class=arithmatex>\(\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)\)</span></p> <p>where:</p> <ul> <li><span class=arithmatex>\(\theta_t\)</span> represents the parameters at iteration <span class=arithmatex>\(t\)</span></li> <li><span class=arithmatex>\(\eta\)</span> is the learning rate</li> <li><span class=arithmatex>\(\nabla L(\theta_t)\)</span> is the gradient of the loss with respect to parameters</li> </ul> <p><strong>Momentum</strong> improves upon basic SGD by accumulating a velocity vector that smooths out the gradient updates:</p> <h4 id=sgd-with-momentum>SGD with Momentum<a class=headerlink href=#sgd-with-momentum title="Permanent link">&para;</a></h4> <p><span class=arithmatex>\(v_{t+1} = \beta v_t + \nabla L(\theta_t)\)</span></p> <p><span class=arithmatex>\(\theta_{t+1} = \theta_t - \eta v_{t+1}\)</span></p> <p>where:</p> <ul> <li><span class=arithmatex>\(v_t\)</span> is the velocity (momentum) vector at iteration <span class=arithmatex>\(t\)</span></li> <li><span class=arithmatex>\(\beta\)</span> is the momentum coefficient (typically 0.9)</li> <li><span class=arithmatex>\(\eta\)</span> is the learning rate</li> </ul> <p>Momentum has several benefits:</p> <ol> <li><strong>Accelerates convergence</strong>: In directions where gradients consistently point the same way, momentum builds up speed</li> <li><strong>Dampens oscillations</strong>: In directions where gradients fluctuate, momentum averages them out</li> <li><strong>Escapes local minima</strong>: Accumulated momentum can carry the optimization through shallow local minima</li> </ol> <p>In PyTorch, we specify momentum when creating the optimizer:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></code></pre></div> <p>The momentum value of 0.9 is a common default that works well for most transfer learning tasks.</p> <div class="admonition note"> <p class=admonition-title>Alternative Optimizers</p> <p>While SGD with momentum is popular for fine-tuning, Adam and AdamW are also effective choices. Adam adapts learning rates per parameter and includes momentum-like behavior. For transfer learning, SGD with momentum often generalizes slightly better, but Adam can converge faster.</p> </div> <p>Understanding momentum's effect on gradient descent convergence:</p> <div style="background: #f5f5f5; padding: 20px; border-radius: 8px; margin: 20px 0;"> <h4>Gradient Descent Comparison</h4> **Without Momentum (Standard SGD):** <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>Start → ↙ ↘ ↙ ↘ ↙ ↘ (oscillates) → Minimum
</span></code></pre></div> - Oscillates perpendicular to gradient - Slow progress in valleys - Sensitive to learning rate **With Momentum (β = 0.9):** <div class="language-text highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a>Start → ↓ ↓ ↓ ↓ (smooth path) → Minimum
</span></code></pre></div> - Accelerates in consistent directions - Dampens oscillations - Faster convergence **Update Rules:** Standard SGD: $\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$ SGD with Momentum: - $v_t = \beta v_{t-1} + \eta \nabla L(\theta_t)$ - $\theta_{t+1} = \theta_t - v_t$ Where β (typically 0.9) controls how much past gradients influence current update. **Key Benefits:** - ✓ Accelerates convergence in consistent gradient directions - ✓ Reduces oscillation in high-curvature regions - ✓ Helps escape shallow local minima - ✓ More stable training with larger learning rates </div> <p>For an interactive visualization, see the <a href=../../sims/training-validation-curves/ >Training vs Validation Curves</a> which shows momentum's effect on convergence speed.</p> <h2 id=domain-adaptation>Domain Adaptation<a class=headerlink href=#domain-adaptation title="Permanent link">&para;</a></h2> <p><strong>Domain adaptation</strong> addresses the challenge of transferring knowledge when the source domain (where the model was trained) differs from the target domain (where we want to apply it). Even when pre-trained on large datasets like ImageNet, models may struggle when the target domain has different characteristics.</p> <p>Examples of domain shift:</p> <ul> <li><strong>Dataset bias</strong>: ImageNet contains mostly high-quality photos, but your application involves sketches or medical images</li> <li><strong>Environmental differences</strong>: A model trained on daytime images applied to nighttime images</li> <li><strong>Sensor differences</strong>: A model trained on camera images applied to satellite imagery</li> </ul> <p>Domain adaptation techniques help bridge this gap:</p> <p><strong>Strategies for domain adaptation:</strong></p> <ol> <li><strong>Gradual fine-tuning</strong>: Start with very small learning rates and gradually increase them</li> <li><strong>Layer-wise unfreezing</strong>: Freeze early layers longer, unfreeze later layers first, then progressively unfreeze earlier layers</li> <li><strong>Domain-specific data augmentation</strong>: Apply augmentations that simulate domain shift (e.g., color jittering, blur)</li> <li><strong>Adversarial training</strong>: Train a domain classifier to make features domain-invariant</li> <li><strong>Self-supervised pre-training</strong>: Pre-train on unlabeled target domain data before fine-tuning</li> </ol> <p>For moderate domain shift, fine-tuning with appropriate data augmentation often suffices. For severe domain shift, more sophisticated techniques may be necessary.</p> <h2 id=online-learning-and-continual-adaptation>Online Learning and Continual Adaptation<a class=headerlink href=#online-learning-and-continual-adaptation title="Permanent link">&para;</a></h2> <p><strong>Online learning</strong> refers to updating models incrementally as new data arrives, rather than retraining from scratch. This is particularly relevant for transfer learning in production systems where:</p> <ul> <li>User behavior changes over time</li> <li>New classes or categories emerge</li> <li>The data distribution shifts gradually (concept drift)</li> </ul> <p>In online learning scenarios, we can periodically fine-tune our transferred model on recent data:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=c1># Initial transfer learning</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=n>model</span> <span class=o>=</span> <span class=n>train_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>num_epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a><span class=c1># Deploy model to production</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=c1># ... time passes, collect new labeled examples ...</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=c1># Online update: fine-tune on recent data</span>
</span><span id=__span-14-8><a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a><span class=n>new_dataloaders</span> <span class=o>=</span> <span class=n>create_dataloaders</span><span class=p>(</span><span class=n>recent_data</span><span class=p>)</span>
</span><span id=__span-14-9><a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a><span class=n>model</span> <span class=o>=</span> <span class=n>train_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>num_epochs</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></code></pre></div> <p>Key considerations for online learning:</p> <ol> <li><strong>Catastrophic forgetting</strong>: Fine-tuning on new data may cause the model to forget previous knowledge. Mitigation strategies include mixing old and new data or using regularization techniques like Elastic Weight Consolidation (EWC)</li> <li><strong>Learning rate scheduling</strong>: Use smaller learning rates for online updates to preserve existing knowledge</li> <li><strong>Monitoring performance</strong>: Track performance on both old and new data to detect forgetting</li> </ol> <h4 id=online-learning-workflow>Online Learning Workflow<a class=headerlink href=#online-learning-workflow title="Permanent link">&para;</a></h4> <pre class=mermaid><code>flowchart TD
    Start(("Pre-trained Model&lt;br/&gt;(ImageNet ResNet-50)"))
    Transfer["Initial Transfer Learning&lt;br/&gt;(fine-tune on 10K images)"]
    Deploy["Deploy Model to Production&lt;br/&gt;(serve predictions)"]
    Collect["Collect User Feedback&lt;br/&gt;(gather labeled examples)"]
    CheckData{"Sufficient&lt;br/&gt;New Data?&lt;br/&gt;(500+ samples)"}
    FineTune["Online Fine-Tuning&lt;br/&gt;(2-3 epochs, LR=0.0001)"]
    Validate["Validate Performance&lt;br/&gt;(test on old + new data)"]
    CheckPerf{"Performance&lt;br/&gt;Acceptable?"}
    Investigate["Investigate Issues&lt;br/&gt;(check for forgetting)"]
    UpdateDeploy["Deploy Updated Model&lt;br/&gt;(replace in production)"]

    Start --&gt; Transfer
    Transfer --&gt; Deploy
    Deploy --&gt; Collect
    Collect --&gt; CheckData
    CheckData --&gt;|No| Collect
    CheckData --&gt;|Yes| FineTune
    FineTune --&gt; Validate
    Validate --&gt; CheckPerf
    CheckPerf --&gt;|No| Investigate
    Investigate --&gt; FineTune
    CheckPerf --&gt;|Yes| UpdateDeploy
    UpdateDeploy --&gt; Collect

    classDef setupNode fill:#4299e1,stroke:#2c5282,stroke-width:2px,color:#fff,font-size:14px
    classDef collectNode fill:#48bb78,stroke:#2f855a,stroke-width:2px,color:#fff,font-size:14px
    classDef updateNode fill:#9f7aea,stroke:#6b46c1,stroke-width:2px,color:#fff,font-size:14px
    classDef decisionNode fill:#ecc94b,stroke:#b7791f,stroke-width:2px,color:#333,font-size:14px
    classDef issueNode fill:#ed8936,stroke:#c05621,stroke-width:2px,color:#fff,font-size:14px

    class Start,Transfer,Deploy setupNode
    class Collect,Validate collectNode
    class FineTune,UpdateDeploy updateNode
    class CheckData,CheckPerf decisionNode
    class Investigate issueNode

    linkStyle default stroke:#666,stroke-width:2px,font-size:12px</code></pre> <p><strong>Continuous Improvement Loop</strong>: This workflow shows how deployed models can be continually updated with new production data, maintaining performance as data distributions shift over time. - Production Deployment - Data Collection - Model Update - Validation</p> <p>Annotations: - Cycle time: "Typical update cycle: 1-4 weeks" - Warning icon on "Catastrophic Forgetting": "Mix old and new data to prevent forgetting"</p> <p>Implementation: Flowchart with Mermaid.js or D3.js Canvas size: Responsive, minimum 700×600px </details></p> <h2 id=practical-tips-for-transfer-learning-success>Practical Tips for Transfer Learning Success<a class=headerlink href=#practical-tips-for-transfer-learning-success title="Permanent link">&para;</a></h2> <p>Based on extensive experimentation, here are proven strategies for successful transfer learning:</p> <p><strong>1. Start with feature extraction, then fine-tune if needed</strong></p> <p>Feature extraction is faster and less prone to overfitting. If accuracy is insufficient, try fine-tuning with a small learning rate.</p> <p><strong>2. Use appropriate learning rates</strong></p> <ul> <li>Feature extraction (training only final layer): 0.001 to 0.01</li> <li>Fine-tuning (training all layers): 0.0001 to 0.001</li> </ul> <p><strong>3. Apply aggressive data augmentation</strong></p> <p>With small datasets, augmentation is crucial. Use flips, crops, rotations, color jittering, and other transformations to increase effective dataset size.</p> <p><strong>4. Monitor both training and validation metrics</strong></p> <p>Watch for overfitting by comparing training and validation loss. Early stopping based on validation performance prevents overfitting.</p> <p><strong>5. Experiment with different pre-trained models</strong></p> <p>Different architectures may perform better for different tasks. Try both shallower (ResNet-18) and deeper (ResNet-50) models.</p> <p><strong>6. Consider the domain gap</strong></p> <p>If your task is very different from ImageNet (e.g., medical imaging, satellite imagery), you may need domain-specific augmentation or even domain-specific pre-trained models.</p> <p><strong>7. Use learning rate schedules</strong></p> <p>Reducing learning rate during training (e.g., multiply by 0.1 every 7 epochs) helps convergence.</p> <p><strong>8. Leverage model ensembles</strong></p> <p>Training multiple models with different random seeds and averaging their predictions often improves performance.</p> <h2 id=transfer-learning-beyond-image-classification>Transfer Learning Beyond Image Classification<a class=headerlink href=#transfer-learning-beyond-image-classification title="Permanent link">&para;</a></h2> <p>While this chapter focuses on image classification with CNNs, transfer learning applies broadly across machine learning:</p> <p><strong>Object detection</strong>: Models like Faster R-CNN use pre-trained CNN backbones</p> <p><strong>Semantic segmentation</strong>: FCN and U-Net architectures build on pre-trained encoders</p> <p><strong>Natural language processing</strong>: BERT, GPT, and other language models use transfer learning extensively</p> <p><strong>Speech recognition</strong>: Models pre-trained on large speech corpora transfer to specific accents or languages</p> <p><strong>Reinforcement learning</strong>: Policies trained in simulation transfer to real-world robots</p> <p>The fundamental principle remains the same: leverage knowledge from large-scale pre-training to achieve better performance with less data on downstream tasks.</p> <h3 id=transfer-learning-strategy-comparison>Transfer Learning Strategy Comparison<a class=headerlink href=#transfer-learning-strategy-comparison title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Strategy</th> <th>Frozen Layers</th> <th>Trainable Params</th> <th>Training Time</th> <th>Typical Accuracy</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td><strong>Feature Extraction</strong></td> <td>All except final layer</td> <td>~2,000</td> <td>Fast (minutes)</td> <td>80-85%</td> <td>Very limited data (&lt;100 samples/class)</td> </tr> <tr> <td><strong>Partial Fine-Tuning</strong></td> <td>Early layers only</td> <td>~2-5M</td> <td>Medium (hours)</td> <td>88-93%</td> <td>Moderate data (100-1000 samples/class)</td> </tr> <tr> <td><strong>Full Fine-Tuning</strong></td> <td>None</td> <td>~11-25M</td> <td>Slow (hours-days)</td> <td>92-97%</td> <td>Substantial data (1000+ samples/class)</td> </tr> </tbody> </table> <p><strong>Key Insights:</strong> - <strong>Feature Extraction</strong>: Fastest approach, leverages pre-trained features as fixed representations. Best when target task is similar to source task (e.g., both are image classification). - <strong>Partial Fine-Tuning</strong>: Balances adaptation with preservation of learned features. Early layers learn general patterns (edges, textures) that transfer well; late layers adapt to task-specific patterns. - <strong>Full Fine-Tuning</strong>: Maximum flexibility and potential performance, but requires more data and compute. Risk of overfitting with limited data.</p> <p><strong>Choosing a Strategy:</strong> 1. Start with <strong>feature extraction</strong> if you have &lt;100 examples per class 2. Use <strong>partial fine-tuning</strong> for 100-1000 examples per class 3. Try <strong>full fine-tuning</strong> only if you have 1000+ examples per class and sufficient compute 4. Always use data augmentation and monitor validation curves for overfitting</p> <h2 id=summary-and-key-takeaways>Summary and Key Takeaways<a class=headerlink href=#summary-and-key-takeaways title="Permanent link">&para;</a></h2> <p>Transfer learning enables practitioners to leverage knowledge from large-scale pre-trained models to achieve excellent performance on new tasks with limited data. The key insights from this chapter:</p> <p><strong>Core concepts:</strong></p> <ul> <li><strong>Pre-trained models</strong> from model zoos (ImageNet, etc.) have learned general visual features applicable to many tasks</li> <li><strong>Feature extraction</strong> treats the pre-trained model as a fixed feature extractor, training only a new classification head</li> <li><strong>Fine-tuning</strong> adapts all layers of the pre-trained model to the new task, typically achieving higher accuracy</li> <li><strong>Domain adaptation</strong> addresses distribution shift between source and target domains</li> </ul> <p><strong>Best practices:</strong></p> <ul> <li>Always preprocess inputs to match the pre-trained model's expected distribution</li> <li>Start with feature extraction; upgrade to fine-tuning if accuracy is insufficient</li> <li>Use small learning rates (0.0001-0.001) for fine-tuning to preserve learned features</li> <li>Apply data augmentation aggressively when working with small datasets</li> <li>Monitor validation error to detect overfitting and determine when to stop training</li> <li>Use SGD with momentum (0.9) for stable convergence</li> </ul> <p><strong>Practical impact:</strong></p> <p>Transfer learning has democratized deep learning by making it accessible to practitioners without access to massive datasets or computational resources. Tasks that once required millions of labeled examples can now be solved with hundreds or thousands, enabling applications across medicine, agriculture, manufacturing, and countless other domains.</p> <h2 id=further-reading>Further Reading<a class=headerlink href=#further-reading title="Permanent link">&para;</a></h2> <p>For deeper exploration of transfer learning concepts:</p> <ul> <li><a href=https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>PyTorch Transfer Learning Tutorial</a> - Official PyTorch documentation and examples</li> <li>Yosinski et al. (2014) - "How transferable are features in deep neural networks?" - Seminal paper analyzing feature transferability across layers</li> <li><a href=https://www.deeplearningbook.org/ >Deep Learning Book, Chapter 15.2</a> - Goodfellow, Bengio, Courville on transfer learning theory</li> <li>Kornblith et al. (2019) - "Do Better ImageNet Models Transfer Better?" - Analysis of which architectures transfer best</li> <li>Raghu et al. (2019) - "Transfusion: Understanding Transfer Learning with Applications to Medical Imaging" - Domain adaptation for medical tasks</li> </ul> <h2 id=exercises>Exercises<a class=headerlink href=#exercises title="Permanent link">&para;</a></h2> <p><strong>Exercise 1: Feature Extraction vs. Fine-Tuning</strong></p> <p>Using the ants vs. bees dataset, implement both feature extraction and fine-tuning approaches. Train each for 10 epochs and compare: - Final validation accuracy - Training time - Number of trainable parameters</p> <p>Which approach would you choose for this task and why?</p> <p><strong>Exercise 2: Learning Rate Sensitivity</strong></p> <p>Train a transfer learning model with different learning rates: [0.0001, 0.001, 0.01, 0.1]. Plot the training and validation loss curves for each. What happens when the learning rate is too large? Too small?</p> <p><strong>Exercise 3: Data Augmentation Impact</strong></p> <p>Train two models on a small subset of the data (50 images per class): 1. With data augmentation (random crops, flips, color jitter) 2. Without data augmentation</p> <p>Compare the validation accuracy and the gap between training and validation accuracy. What does this tell you about overfitting?</p> <p><strong>Exercise 4: Custom Dataset Transfer</strong></p> <p>Adapt the transfer learning code to a new dataset of your choice (e.g., <a href=https://www.kaggle.com/c/dogs-vs-cats>Kaggle's Dogs vs. Cats</a>, <a href=https://www.kaggle.com/datamunge/sign-language-mnist>Sign Language MNIST</a>, or <a href=https://www.kaggle.com/dansbecker/food-101>Food-101</a>). Experiment with different pre-trained models (ResNet-18, ResNet-50, MobileNet) and report which achieves the best performance.</p> <p><strong>Exercise 5: Layer Freezing Strategy</strong></p> <p>Implement a progressive unfreezing strategy: 1. Train only the final layer for 3 epochs 2. Unfreeze the last convolutional block and train for 3 more epochs 3. Unfreeze all layers and train for 3 final epochs</p> <p>Compare this approach to full fine-tuning from the start. Does progressive unfreezing improve final accuracy?</p> <p><strong>Exercise 6: Domain Adaptation</strong></p> <p>Create a domain shift by applying strong transformations (e.g., converting images to grayscale, adding noise, extreme color shifts) to the validation set while keeping the training set normal. How does this affect validation accuracy? What strategies might help improve performance under domain shift?</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 | CC BY-NC-SA 4.0 DEED </div> </div> <div class=md-social> <a href=https://github.com/AnvithPothula target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/anvith-pothula target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>