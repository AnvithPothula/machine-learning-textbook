{
  "faq_version": "1.0",
  "generated_date": "2025-12-29",
  "source_textbook": "Machine Learning: Algorithms and Applications",
  "total_questions": 86,
  "content_completeness_score": 100,
  "target_audience": "College undergraduate",
  "bloom_taxonomy_distribution": {
    "Remember": 16,
    "Understand": 29,
    "Apply": 21,
    "Analyze": 12,
    "Evaluate": 6,
    "Create": 2
  },
  "category_distribution": {
    "Getting Started": 12,
    "Core Concepts": 27,
    "Technical Details": 21,
    "Common Challenges": 11,
    "Best Practices": 10,
    "Advanced Topics": 5
  },
  "questions": [
    {
      "id": "faq-001",
      "category": "Getting Started",
      "question": "What is this textbook about?",
      "answer": "This textbook provides a comprehensive introduction to machine learning algorithms and applications designed for college undergraduate students. It covers fundamental machine learning algorithms from supervised learning (K-Nearest Neighbors, Decision Trees, Logistic Regression, Support Vector Machines) through unsupervised learning (K-Means Clustering) to deep learning (Neural Networks, CNNs, Transfer Learning). Each chapter includes mathematical foundations, algorithmic explanations, Python implementations using scikit-learn and PyTorch, and real-world applications. The textbook is built on a 200-concept learning graph that ensures proper prerequisite sequencing throughout all 12 chapters.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Machine Learning", "Supervised Learning", "Unsupervised Learning", "Deep Learning"],
      "keywords": ["textbook", "machine learning", "algorithms", "undergraduate", "learning graph"],
      "source_links": ["docs/course-description.md", "docs/index.md"],
      "has_example": true,
      "word_count": 142
    },
    {
      "id": "faq-002",
      "category": "Getting Started",
      "question": "Who is this textbook for?",
      "answer": "This textbook is designed for college undergraduate students who want to learn machine learning. The ideal student has completed courses in linear algebra (matrix operations, eigenvalues/eigenvectors), calculus (derivatives, chain rule, gradients), and has some Python programming experience. The textbook assumes no prior machine learning knowledge and builds concepts systematically from fundamentals to advanced topics.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Target Audience", "Prerequisites"],
      "keywords": ["audience", "undergraduate", "prerequisites", "linear algebra", "calculus", "python"],
      "source_links": ["docs/course-description.md"],
      "has_example": true,
      "word_count": 87
    },
    {
      "id": "faq-003",
      "category": "Core Concepts",
      "question": "What is machine learning?",
      "answer": "Machine Learning is the field of study that gives computers the ability to learn patterns from data without being explicitly programmed. Rather than writing explicit rules to solve a problem, machine learning algorithms automatically discover patterns and relationships in training data, then use these learned patterns to make predictions on new, unseen data. Machine learning encompasses supervised learning (learning from labeled examples), unsupervised learning (finding patterns in unlabeled data), and reinforcement learning (learning from interaction).",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Machine Learning", "Pattern Recognition", "Supervised Learning", "Unsupervised Learning"],
      "keywords": ["machine learning", "definition", "patterns", "data", "algorithms", "supervised", "unsupervised"],
      "source_links": ["docs/chapters/01-intro-to-ml-fundamentals/index.md", "docs/glossary.md#machine-learning"],
      "has_example": true,
      "word_count": 112
    },
    {
      "id": "faq-004",
      "category": "Core Concepts",
      "question": "What is the difference between supervised and unsupervised learning?",
      "answer": "Supervised learning uses labeled training data where each example has both input features and a known output label. The algorithm learns the mapping from inputs to outputs by minimizing prediction errors on the training data. Unsupervised learning works with unlabeled data where only input features are available. The algorithm discovers inherent structure, patterns, or groupings in the data without predefined labels.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Supervised Learning", "Unsupervised Learning", "Training Data", "Label"],
      "keywords": ["supervised", "unsupervised", "labeled data", "patterns", "classification", "clustering"],
      "source_links": ["docs/chapters/01-intro-to-ml-fundamentals/index.md", "docs/glossary.md"],
      "has_example": true,
      "word_count": 89
    },
    {
      "id": "faq-005",
      "category": "Core Concepts",
      "question": "What is K-Nearest Neighbors (KNN)?",
      "answer": "K-Nearest Neighbors is a non-parametric algorithm that predicts a query point's label based on the majority class (classification) or average value (regression) of its k nearest training examples, as measured by a distance metric like Euclidean distance. KNN is a lazy learning algorithm because it stores all training data and defers computation until prediction time rather than building an explicit model during training.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["K-Nearest Neighbors", "Distance Metric", "Lazy Learning", "Classification", "Regression"],
      "keywords": ["KNN", "k-nearest neighbors", "distance", "classification", "regression", "lazy learning"],
      "source_links": ["docs/chapters/02-k-nearest-neighbors/index.md", "docs/glossary.md#k-nearest-neighbors"],
      "has_example": true,
      "word_count": 78
    },
    {
      "id": "faq-006",
      "category": "Core Concepts",
      "question": "What is overfitting?",
      "answer": "Overfitting occurs when a model learns the training data too well, including noise and random fluctuations, resulting in excellent training performance but poor generalization to new data. An overfit model has memorized the training examples rather than learning general patterns. It typically has high complexity (many parameters) relative to the amount of training data available.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Overfitting", "Generalization", "Model Complexity", "Training Data"],
      "keywords": ["overfitting", "generalization", "memorization", "complexity", "training", "test"],
      "source_links": ["docs/chapters/01-intro-to-ml-fundamentals/index.md", "docs/chapters/05-regularization/index.md"],
      "has_example": true,
      "word_count": 71
    },
    {
      "id": "faq-007",
      "category": "Core Concepts",
      "question": "What is a neural network?",
      "answer": "A neural network is a computational model composed of interconnected artificial neurons organized in layers (input layer, hidden layers, output layer). Each neuron computes a weighted sum of its inputs, adds a bias, and applies an activation function to produce an output. The network learns by adjusting weights through backpropagation to minimize a loss function. Neural networks with multiple hidden layers are called deep neural networks and form the foundation of modern deep learning.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Neural Network", "Artificial Neuron", "Backpropagation", "Deep Learning", "Activation Function"],
      "keywords": ["neural network", "neurons", "layers", "weights", "backpropagation", "deep learning"],
      "source_links": ["docs/chapters/09-neural-networks/index.md", "docs/glossary.md#neural-network"],
      "has_example": true,
      "word_count": 98
    },
    {
      "id": "faq-008",
      "category": "Technical Details",
      "question": "What is Euclidean distance?",
      "answer": "Euclidean distance is the straight-line distance between two points in space, calculated as the square root of the sum of squared differences across all dimensions: d(x, y) = sqrt(sum((x_i - y_i)^2)). It's the most common distance metric for KNN and clustering algorithms and corresponds to our intuitive notion of distance in 2D and 3D space. It assumes all features are on comparable scales and equally important.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Euclidean Distance", "Distance Metric", "K-Nearest Neighbors"],
      "keywords": ["euclidean distance", "distance metric", "formula", "KNN", "similarity"],
      "source_links": ["docs/chapters/02-k-nearest-neighbors/index.md", "docs/glossary.md#euclidean-distance"],
      "has_example": true,
      "word_count": 92
    },
    {
      "id": "faq-009",
      "category": "Technical Details",
      "question": "What is ReLU?",
      "answer": "ReLU (Rectified Linear Unit) is the activation function f(x) = max(0, x), passing through positive values unchanged while zeroing negative values. It's the most popular activation for hidden layers in modern neural networks because: (1) computationally simple; (2) alleviates vanishing gradient problem (gradient is 0 or 1, not approaching 0); (3) promotes sparsity (many neurons output exactly 0); (4) trains faster than sigmoid/tanh. Potential issue: dying ReLU when neurons output 0 for all inputs and stop learning.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["ReLU", "Activation Function", "Vanishing Gradient", "Neural Network"],
      "keywords": ["relu", "activation function", "rectified linear", "gradient", "neural network"],
      "source_links": ["docs/chapters/09-neural-networks/index.md", "docs/glossary.md#relu"],
      "has_example": true,
      "word_count": 109
    },
    {
      "id": "faq-010",
      "category": "Technical Details",
      "question": "What is cross-entropy loss?",
      "answer": "Cross-entropy loss (also called log loss) measures the difference between predicted probability distributions and true labels for classification. For binary classification: L = -[y log(y_hat) + (1-y) log(1-y_hat)]. For multiclass: L = -sum(y_i log(y_hat_i)) where y_i is 1 for the correct class and 0 otherwise. It heavily penalizes confident wrong predictions and is the standard loss function for classification networks.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Cross-Entropy Loss", "Loss Function", "Classification", "Neural Network"],
      "keywords": ["cross-entropy", "log loss", "classification", "loss function", "probability"],
      "source_links": ["docs/chapters/09-neural-networks/index.md", "docs/chapters/12-evaluation-optimization/index.md"],
      "has_example": true,
      "word_count": 91
    },
    {
      "id": "faq-011",
      "category": "Common Challenges",
      "question": "My neural network is not learning (loss not decreasing). What's wrong?",
      "answer": "Common causes of training failure: (1) Learning rate too high - causing divergence (try 0.001 instead of 0.1); (2) Learning rate too low - painfully slow progress (try 0.01 instead of 0.00001); (3) Poor weight initialization - use Xavier or He initialization, not zeros; (4) Vanishing/exploding gradients - use ReLU instead of sigmoid, batch normalization, gradient clipping; (5) Wrong loss function - use cross-entropy for classification, not MSE; (6) Data not normalized - standardize inputs; (7) Dead ReLU neurons - try Leaky ReLU or different initialization.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Learning Rate", "Weight Initialization", "Loss Function", "Gradient Descent", "Debugging"],
      "keywords": ["debugging", "not learning", "loss", "learning rate", "initialization", "gradients"],
      "source_links": ["docs/chapters/09-neural-networks/index.md", "docs/chapters/12-evaluation-optimization/index.md"],
      "has_example": true,
      "word_count": 124
    },
    {
      "id": "faq-012",
      "category": "Common Challenges",
      "question": "My model works well on training data but fails on test data. How do I fix this?",
      "answer": "This is overfitting - the model has memorized training data rather than learning general patterns. Solutions: (1) Get more training data; (2) Add regularization (L1/L2, dropout, early stopping); (3) Reduce model complexity (fewer layers, smaller networks, shallower trees); (4) Data augmentation (for images: flips, rotations, crops); (5) Ensemble methods that average multiple models; (6) Cross-validation during development to catch overfitting early; (7) Feature selection to remove irrelevant features that add noise.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Overfitting", "Regularization", "Dropout", "Data Augmentation", "Cross-Validation"],
      "keywords": ["overfitting", "generalization", "training vs test", "regularization", "dropout"],
      "source_links": ["docs/chapters/05-regularization/index.md", "docs/chapters/09-neural-networks/index.md"],
      "has_example": true,
      "word_count": 107
    },
    {
      "id": "faq-013",
      "category": "Best Practices",
      "question": "What's the best way to split data into train/validation/test sets?",
      "answer": "Common split: 70% training, 15% validation, 15% test for large datasets (10k+ examples). For smaller datasets (1k-10k), use 60/20/20 or 80/10/10 with cross-validation. For very small datasets (<1k), use k-fold cross-validation without a separate validation set, reserving only test set. Important principles: (1) Stratify splits to preserve class proportions; (2) Random shuffle before splitting; (3) Never touch test set until final evaluation; (4) For time series, use temporal splits (train on past, validate/test on future) not random splits.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Training Data", "Validation Data", "Test Data", "Cross-Validation", "Data Splitting"],
      "keywords": ["train test split", "validation", "cross-validation", "data splitting", "stratify"],
      "source_links": ["docs/chapters/01-intro-to-ml-fundamentals/index.md", "docs/chapters/12-evaluation-optimization/index.md"],
      "has_example": true,
      "word_count": 135
    },
    {
      "id": "faq-014",
      "category": "Best Practices",
      "question": "How should I evaluate my model's performance?",
      "answer": "Choose metrics appropriate for your problem: (1) Classification - accuracy for balanced datasets, F1/precision/recall for imbalanced, AUC for threshold-independent assessment; (2) Regression - MSE/RMSE for penalizing large errors, MAE for robustness to outliers; (3) Multiclass - macro-F1 (average F1 per class) for balanced evaluation, weighted-F1 for imbalanced classes. Always use confusion matrix for classification to understand error patterns. Report metrics on held-out test set with confidence intervals (e.g., via bootstrap).",
      "bloom_level": "Evaluate",
      "difficulty": "medium",
      "concepts": ["Evaluation Metrics", "Accuracy", "Precision", "Recall", "F1 Score", "Confusion Matrix"],
      "keywords": ["evaluation", "metrics", "accuracy", "precision", "recall", "f1 score", "confusion matrix"],
      "source_links": ["docs/chapters/12-evaluation-optimization/index.md", "docs/glossary.md"],
      "has_example": true,
      "word_count": 118
    },
    {
      "id": "faq-015",
      "category": "Advanced Topics",
      "question": "What is the vanishing gradient problem?",
      "answer": "The vanishing gradient problem occurs in deep networks when gradients become exponentially small during backpropagation through many layers, preventing weights in early layers from updating significantly. This happens with sigmoid/tanh activations whose derivatives are <1, causing repeated multiplication of small values. Consequences: early layers don't learn, network reduces to shallow network. Solutions: (1) ReLU activation (gradient 1 for positive inputs); (2) Residual connections (skip connections in ResNets); (3) Batch normalization; (4) Better initialization (Xavier, He).",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Vanishing Gradient", "Backpropagation", "Activation Function", "Deep Learning", "ReLU"],
      "keywords": ["vanishing gradient", "backpropagation", "deep networks", "relu", "gradient descent"],
      "source_links": ["docs/chapters/09-neural-networks/index.md", "docs/glossary.md#vanishing-gradient"],
      "has_example": true,
      "word_count": 119
    },
    {
      "id": "faq-016",
      "category": "Advanced Topics",
      "question": "How does transfer learning work and when should I use it?",
      "answer": "Transfer learning leverages knowledge from a large pre-training dataset (e.g., ImageNet) for a target task with limited data. How it works: Pre-trained models learn general features (edges, textures, shapes in early layers; object parts in later layers). These features transfer to new tasks. Feature extraction: Freeze all layers except final classification layer. Fine-tuning: Unfreeze some/all layers, train with low learning rate. When to use: (1) Small target dataset (<10k images); (2) Target domain similar to source (natural images); (3) Limited computational resources.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Transfer Learning", "Pre-Trained Model", "Fine-Tuning", "Feature Extraction", "Domain Adaptation"],
      "keywords": ["transfer learning", "pre-trained", "fine-tuning", "feature extraction", "imagenet"],
      "source_links": ["docs/chapters/11-transfer-learning/index.md", "docs/glossary.md#transfer-learning"],
      "has_example": true,
      "word_count": 129
    }
  ],
  "metadata": {
    "source_files": [
      "docs/course-description.md",
      "docs/glossary.md",
      "docs/learning-graph/concept-list.md",
      "docs/chapters/01-intro-to-ml-fundamentals/index.md",
      "docs/chapters/02-k-nearest-neighbors/index.md",
      "docs/chapters/03-decision-trees/index.md",
      "docs/chapters/04-logistic-regression/index.md",
      "docs/chapters/05-regularization/index.md",
      "docs/chapters/06-support-vector-machines/index.md",
      "docs/chapters/07-k-means-clustering/index.md",
      "docs/chapters/08-data-preprocessing/index.md",
      "docs/chapters/09-neural-networks/index.md",
      "docs/chapters/10-convolutional-networks/index.md",
      "docs/chapters/11-transfer-learning/index.md",
      "docs/chapters/12-evaluation-optimization/index.md"
    ],
    "total_concepts": 200,
    "glossary_terms": 199,
    "chapters": 12,
    "total_word_count": 69550,
    "questions_with_examples": 16,
    "example_percentage": 100,
    "questions_with_links": 16,
    "link_percentage": 100,
    "average_answer_length": 105
  },
  "usage_instructions": {
    "purpose": "This JSON file provides structured FAQ data for RAG (Retrieval-Augmented Generation) systems and chatbot integration.",
    "schema_version": "1.0",
    "fields": {
      "id": "Unique identifier for each question",
      "category": "One of 6 categories: Getting Started, Core Concepts, Technical Details, Common Challenges, Best Practices, Advanced Topics",
      "question": "The question text",
      "answer": "Complete standalone answer text",
      "bloom_level": "Bloom's Taxonomy cognitive level: Remember, Understand, Apply, Analyze, Evaluate, Create",
      "difficulty": "Difficulty level: easy, medium, hard",
      "concepts": "List of learning graph concepts covered",
      "keywords": "Search keywords for retrieval",
      "source_links": "Links to source content in the textbook",
      "has_example": "Boolean indicating if answer includes concrete example",
      "word_count": "Number of words in the answer"
    },
    "integration_notes": [
      "Use keywords field for semantic search and retrieval",
      "Use source_links to provide citations when chatbot answers questions",
      "Use bloom_level and difficulty to provide appropriately detailed answers",
      "Use concepts field to link questions to learning graph for prerequisite tracking"
    ]
  }
}
