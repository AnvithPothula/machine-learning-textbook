<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A comprehensive intelligent textbook on machine learning algorithms and applications"><meta name=author content="Anvith Pothula"><link href=https://example.com/glossary/ rel=canonical><link href=../faq/ rel=prev><link href=../chapters/ rel=next><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Glossary - Machine Learning - Algorithms and Applications</title><link rel=stylesheet href=../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#glossary-of-terms class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Machine Learning - Algorithms and Applications" class="md-header__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning - Algorithms and Applications </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Glossary </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../course-description/ class=md-tabs__link> Course Description </a> </li> <li class=md-tabs__item> <a href=../faq/ class=md-tabs__link> FAQ </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> Glossary </a> </li> <li class=md-tabs__item> <a href=../chapters/ class=md-tabs__link> Chapters </a> </li> <li class=md-tabs__item> <a href=../sims/ class=md-tabs__link> MicroSims </a> </li> <li class=md-tabs__item> <a href=../learning-graph/ class=md-tabs__link> Learning Graph </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Machine Learning - Algorithms and Applications" class="md-nav__button md-logo" aria-label="Machine Learning - Algorithms and Applications" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Machine Learning - Algorithms and Applications </label> <div class=md-nav__source> <a href=https://github.com/AnvithPothula/machine-learning-textbook title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> machine-learning-textbook </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../course-description/ class=md-nav__link> <span class=md-ellipsis> Course Description </span> </a> </li> <li class=md-nav__item> <a href=../faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Glossary </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Chapters </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class=md-ellipsis> 1. ML Fundamentals </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> 1. ML Fundamentals </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/01-intro-to-ml-fundamentals/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/01-intro-to-ml-fundamentals/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> 2. K-Nearest Neighbors </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> 2. K-Nearest Neighbors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/02-k-nearest-neighbors/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/02-k-nearest-neighbors/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class=md-ellipsis> 3. Decision Trees </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> 3. Decision Trees </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/03-decision-trees/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/03-decision-trees/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class=md-ellipsis> 4. Logistic Regression </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> 4. Logistic Regression </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/04-logistic-regression/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/04-logistic-regression/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex=0> <span class=md-ellipsis> 5. Regularization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> 5. Regularization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/05-regularization/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/05-regularization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_7> <label class=md-nav__link for=__nav_5_7 id=__nav_5_7_label tabindex=0> <span class=md-ellipsis> 6. Support Vector Machines </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> 6. Support Vector Machines </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/06-support-vector-machines/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/06-support-vector-machines/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_8> <label class=md-nav__link for=__nav_5_8 id=__nav_5_8_label tabindex=0> <span class=md-ellipsis> 7. K-Means Clustering </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_5_8> <span class="md-nav__icon md-icon"></span> 7. K-Means Clustering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/07-k-means-clustering/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/07-k-means-clustering/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_9> <label class=md-nav__link for=__nav_5_9 id=__nav_5_9_label tabindex=0> <span class=md-ellipsis> 8. Data Preprocessing </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_9_label aria-expanded=false> <label class=md-nav__title for=__nav_5_9> <span class="md-nav__icon md-icon"></span> 8. Data Preprocessing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/08-data-preprocessing/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/08-data-preprocessing/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_10> <label class=md-nav__link for=__nav_5_10 id=__nav_5_10_label tabindex=0> <span class=md-ellipsis> 9. Neural Networks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_10_label aria-expanded=false> <label class=md-nav__title for=__nav_5_10> <span class="md-nav__icon md-icon"></span> 9. Neural Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/09-neural-networks/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/09-neural-networks/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_11> <label class=md-nav__link for=__nav_5_11 id=__nav_5_11_label tabindex=0> <span class=md-ellipsis> 10. Convolutional Networks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_11_label aria-expanded=false> <label class=md-nav__title for=__nav_5_11> <span class="md-nav__icon md-icon"></span> 10. Convolutional Networks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/10-convolutional-networks/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/10-convolutional-networks/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_12> <label class=md-nav__link for=__nav_5_12 id=__nav_5_12_label tabindex=0> <span class=md-ellipsis> 11. Transfer Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_12_label aria-expanded=false> <label class=md-nav__title for=__nav_5_12> <span class="md-nav__icon md-icon"></span> 11. Transfer Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/11-transfer-learning/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/11-transfer-learning/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_13> <label class=md-nav__link for=__nav_5_13 id=__nav_5_13_label tabindex=0> <span class=md-ellipsis> 12. Evaluation & Optimization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_13_label aria-expanded=false> <label class=md-nav__title for=__nav_5_13> <span class="md-nav__icon md-icon"></span> 12. Evaluation & Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../chapters/12-evaluation-optimization/ class=md-nav__link> <span class=md-ellipsis> Content </span> </a> </li> <li class=md-nav__item> <a href=../chapters/12-evaluation-optimization/quiz/ class=md-nav__link> <span class=md-ellipsis> Quiz </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> MicroSims </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> MicroSims </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../sims/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../sims/activation-functions/ class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=../sims/categorical-encoding-explorer/ class=md-nav__link> <span class=md-ellipsis> Categorical Encoding Explorer </span> </a> </li> <li class=md-nav__item> <a href=../sims/cnn-architecture/ class=md-nav__link> <span class=md-ellipsis> CNN Architecture </span> </a> </li> <li class=md-nav__item> <a href=../sims/confusion-matrix-explorer/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix Explorer </span> </a> </li> <li class=md-nav__item> <a href=../sims/convolution-operation/ class=md-nav__link> <span class=md-ellipsis> Convolution Operation </span> </a> </li> <li class=md-nav__item> <a href=../sims/distance-metrics/ class=md-nav__link> <span class=md-ellipsis> Distance Metrics </span> </a> </li> <li class=md-nav__item> <a href=../sims/entropy-gini-comparison/ class=md-nav__link> <span class=md-ellipsis> Entropy-Gini Comparison </span> </a> </li> <li class=md-nav__item> <a href=../sims/feature-scaling-visualizer/ class=md-nav__link> <span class=md-ellipsis> Feature Scaling Visualizer </span> </a> </li> <li class=md-nav__item> <a href=../sims/k-selection-simulator/ class=md-nav__link> <span class=md-ellipsis> K-Selection Simulator </span> </a> </li> <li class=md-nav__item> <a href=../sims/kfold-cross-validation/ class=md-nav__link> <span class=md-ellipsis> K-Fold Cross Validation </span> </a> </li> <li class=md-nav__item> <a href=../sims/lasso-regression-geometry/ class=md-nav__link> <span class=md-ellipsis> Lasso Regression Geometry </span> </a> </li> <li class=md-nav__item> <a href=../sims/network-architecture-visualizer/ class=md-nav__link> <span class=md-ellipsis> Network Architecture Visualizer </span> </a> </li> <li class=md-nav__item> <a href=../sims/ridge-regression-geometry/ class=md-nav__link> <span class=md-ellipsis> Ridge Regression Geometry </span> </a> </li> <li class=md-nav__item> <a href=../sims/roc-curve-comparison/ class=md-nav__link> <span class=md-ellipsis> ROC Curve Comparison </span> </a> </li> <li class=md-nav__item> <a href=../sims/sigmoid-explorer/ class=md-nav__link> <span class=md-ellipsis> Sigmoid Explorer </span> </a> </li> <li class=md-nav__item> <a href=../sims/svm-margin-maximization/ class=md-nav__link> <span class=md-ellipsis> SVM Margin Maximization </span> </a> </li> <li class=md-nav__item> <a href=../sims/training-validation-curves/ class=md-nav__link> <span class=md-ellipsis> Training Validation Curves </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Learning Graph </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Learning Graph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../learning-graph/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../sims/graph-viewer/ class=md-nav__link> <span class=md-ellipsis> Graph Viewer </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/course-description-assessment/ class=md-nav__link> <span class=md-ellipsis> Course Description Assessment </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/concept-list/ class=md-nav__link> <span class=md-ellipsis> Concept List </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/concept-taxonomy/ class=md-nav__link> <span class=md-ellipsis> Concept Taxonomy </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/learning-graph.csv class=md-nav__link> <span class=md-ellipsis> Learning Graph (CSV) </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/learning-graph.json class=md-nav__link> <span class=md-ellipsis> Learning Graph (JSON) </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/quality-metrics/ class=md-nav__link> <span class=md-ellipsis> Quality Metrics </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/taxonomy-distribution/ class=md-nav__link> <span class=md-ellipsis> Taxonomy Distribution </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/glossary-quality-report/ class=md-nav__link> <span class=md-ellipsis> Glossary Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/faq-quality-report/ class=md-nav__link> <span class=md-ellipsis> FAQ Quality Report </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/faq-coverage-gaps/ class=md-nav__link> <span class=md-ellipsis> FAQ Coverage Gaps </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/book-metrics/ class=md-nav__link> <span class=md-ellipsis> Book Metrics </span> </a> </li> <li class=md-nav__item> <a href=../learning-graph/chapter-metrics/ class=md-nav__link> <span class=md-ellipsis> Chapter Metrics </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=glossary-of-terms>Glossary of Terms<a class=headerlink href=#glossary-of-terms title="Permanent link">&para;</a></h1> <h4 id=accuracy>Accuracy<a class=headerlink href=#accuracy title="Permanent link">&para;</a></h4> <p>The proportion of correct predictions (both true positives and true negatives) among all predictions made by a classification model.</p> <p>Accuracy is calculated as (TP + TN) / (TP + TN + FP + FN), where TP is true positives, TN is true negatives, FP is false positives, and FN is false negatives. While accuracy is intuitive, it can be misleading for imbalanced datasets where one class dominates.</p> <p><strong>Example:</strong> A model that correctly classifies 95 out of 100 iris flowers has 95% accuracy.</p> <h4 id=activation-function>Activation Function<a class=headerlink href=#activation-function title="Permanent link">&para;</a></h4> <p>A mathematical function applied to a neuron's weighted sum of inputs to introduce non-linearity into the neural network.</p> <p><strong>Example:</strong> The ReLU activation function outputs max(0, x), passing positive values unchanged while zeroing negative values.</p> <h4 id=adam-optimizer>Adam Optimizer<a class=headerlink href=#adam-optimizer title="Permanent link">&para;</a></h4> <p>An adaptive learning rate optimization algorithm that combines momentum and RMSprop by maintaining exponential moving averages of both gradients and squared gradients.</p> <p><strong>Example:</strong> Adam with default parameters (lr=0.001, β₁=0.9, β₂=0.999) is the most common optimizer for training deep neural networks.</p> <h4 id=algorithm>Algorithm<a class=headerlink href=#algorithm title="Permanent link">&para;</a></h4> <p>A step-by-step procedure for solving a problem or performing a computation in machine learning.</p> <p><strong>Example:</strong> The k-nearest neighbors algorithm finds the k training examples closest to a query point and predicts based on their labels.</p> <h4 id=alexnet>AlexNet<a class=headerlink href=#alexnet title="Permanent link">&para;</a></h4> <p>A deep convolutional neural network architecture that won the ImageNet 2012 competition, featuring 8 layers (5 convolutional, 3 fully connected) and popularizing ReLU activation and dropout.</p> <p><strong>Example:</strong> AlexNet reduced ImageNet top-5 error from 26% to 15.3%, demonstrating the power of deep CNNs for image classification.</p> <h4 id=artificial-neuron>Artificial Neuron<a class=headerlink href=#artificial-neuron title="Permanent link">&para;</a></h4> <p>A computational unit that takes weighted inputs, sums them with a bias term, and applies an activation function to produce an output.</p> <p><strong>Example:</strong> A neuron computes output = σ(w₁x₁ + w₂x₂ + b), where σ is the activation function, w are weights, x are inputs, and b is the bias.</p> <h4 id=auc>AUC<a class=headerlink href=#auc title="Permanent link">&para;</a></h4> <p>Area Under the Curve, measuring the area under the ROC curve to summarize classifier performance across all classification thresholds with a single value between 0 and 1.</p> <p><strong>Example:</strong> An AUC of 0.95 indicates the model has a 95% chance of ranking a random positive example higher than a random negative example.</p> <h4 id=average-pooling>Average Pooling<a class=headerlink href=#average-pooling title="Permanent link">&para;</a></h4> <p>A pooling operation that computes the average value within each pooling window to downsample feature maps in convolutional neural networks.</p> <p><strong>Example:</strong> Average pooling over a 2×2 window containing values [1, 3, 2, 4] produces output value 2.5.</p> <h4 id=backpropagation>Backpropagation<a class=headerlink href=#backpropagation title="Permanent link">&para;</a></h4> <p>An algorithm for computing gradients of the loss function with respect to network weights by applying the chain rule backward through the network layers.</p> <p><strong>Example:</strong> In a 3-layer network, backpropagation starts from the output layer loss and propagates gradients backward through hidden layers to the input layer.</p> <h4 id=batch-processing>Batch Processing<a class=headerlink href=#batch-processing title="Permanent link">&para;</a></h4> <p>Processing multiple data instances simultaneously in groups (batches) rather than one at a time to improve computational efficiency.</p> <p><strong>Example:</strong> Training a neural network on batches of 32 images at a time instead of processing images individually reduces training time by parallelizing computations.</p> <h4 id=batch-size>Batch Size<a class=headerlink href=#batch-size title="Permanent link">&para;</a></h4> <p>The number of training examples processed together in one forward and backward pass during neural network training.</p> <p><strong>Example:</strong> A batch size of 64 means the model processes 64 images before updating weights, balancing memory usage and gradient stability.</p> <h4 id=bayesian-optimization>Bayesian Optimization<a class=headerlink href=#bayesian-optimization title="Permanent link">&para;</a></h4> <p>A sequential model-based optimization approach that builds a probabilistic model of the objective function to intelligently select hyperparameters for evaluation.</p> <p><strong>Example:</strong> Bayesian optimization uses a Gaussian process to model validation accuracy as a function of learning rate and regularization strength, selecting promising hyperparameters to try next.</p> <h4 id=bias>Bias<a class=headerlink href=#bias title="Permanent link">&para;</a></h4> <p>A learnable parameter added to the weighted sum of inputs in a neuron before applying the activation function, allowing the neuron to fit patterns that don't pass through the origin.</p> <p><strong>Example:</strong> In the linear function y = mx + b, the bias term b shifts the line vertically.</p> <h4 id=bias-variance-tradeoff>Bias-Variance Tradeoff<a class=headerlink href=#bias-variance-tradeoff title="Permanent link">&para;</a></h4> <p>The fundamental tradeoff in machine learning where reducing model bias (underfitting) tends to increase variance (overfitting), and vice versa.</p> <p>Simple models have high bias (systematic errors) but low variance (stable predictions), while complex models have low bias but high variance (sensitivity to training data variations). The optimal model balances both sources of error.</p> <p><strong>Example:</strong> A linear model on nonlinear data has high bias (underfits), while a 50-layer decision tree has high variance (overfits to noise).</p> <h4 id=binary-classification>Binary Classification<a class=headerlink href=#binary-classification title="Permanent link">&para;</a></h4> <p>A supervised learning task where the goal is to assign each instance to one of exactly two classes or categories.</p> <p><strong>Example:</strong> Email spam detection classifies each email as either "spam" or "not spam."</p> <h4 id=categorical-features>Categorical Features<a class=headerlink href=#categorical-features title="Permanent link">&para;</a></h4> <p>Input variables that take values from a discrete, finite set of categories or groups without inherent numerical ordering.</p> <p><strong>Example:</strong> Color (red, blue, green), country (USA, Canada, Mexico), and email provider (Gmail, Yahoo, Outlook) are categorical features.</p> <h4 id=centroid>Centroid<a class=headerlink href=#centroid title="Permanent link">&para;</a></h4> <p>The center point of a cluster in k-means clustering, calculated as the mean of all data points assigned to that cluster.</p> <p><strong>Example:</strong> For cluster points (1,2), (3,4), and (5,6), the centroid is ((1+3+5)/3, (2+4+6)/3) = (3, 4).</p> <h4 id=classification>Classification<a class=headerlink href=#classification title="Permanent link">&para;</a></h4> <p>A supervised learning task where the goal is to predict a discrete class label for each input instance from a predefined set of categories.</p> <p><strong>Example:</strong> Classifying iris flowers into species (setosa, versicolor, virginica) based on petal and sepal measurements.</p> <h4 id=cluster-assignment>Cluster Assignment<a class=headerlink href=#cluster-assignment title="Permanent link">&para;</a></h4> <p>The step in k-means clustering where each data point is assigned to the nearest centroid based on distance.</p> <p><strong>Example:</strong> In iteration 3 of k-means with 3 clusters, each of 150 data points is assigned to whichever of the 3 centroids is closest.</p> <h4 id=cluster-update>Cluster Update<a class=headerlink href=#cluster-update title="Permanent link">&para;</a></h4> <p>The step in k-means clustering where centroid positions are recalculated as the mean of all points assigned to each cluster.</p> <p><strong>Example:</strong> After assigning points to clusters, the centroid for cluster 1 moves from (2, 3) to (2.5, 3.2) based on the new mean position.</p> <h4 id=cnn-architecture>CNN Architecture<a class=headerlink href=#cnn-architecture title="Permanent link">&para;</a></h4> <p>The overall design and layer organization of a convolutional neural network, specifying the sequence and configuration of convolutional, pooling, and fully connected layers.</p> <p><strong>Example:</strong> A typical CNN architecture: Input → Conv → ReLU → Pool → Conv → ReLU → Pool → Flatten → FC → Softmax.</p> <h4 id=computational-complexity>Computational Complexity<a class=headerlink href=#computational-complexity title="Permanent link">&para;</a></h4> <p>A measure of the resources (time and memory) required to execute an algorithm as a function of input size.</p> <p><strong>Example:</strong> K-nearest neighbors has O(nd) time complexity for n training examples with d features when making a single prediction.</p> <h4 id=confusion-matrix>Confusion Matrix<a class=headerlink href=#confusion-matrix title="Permanent link">&para;</a></h4> <p>A table showing the counts of true positives, false positives, true negatives, and false negatives for a classification model's predictions.</p> <p><strong>Example:</strong> A 2×2 confusion matrix for binary classification shows actual classes in rows and predicted classes in columns, with diagonal entries representing correct predictions.</p> <h4 id=continuous-features>Continuous Features<a class=headerlink href=#continuous-features title="Permanent link">&para;</a></h4> <p>Input variables that can take any numerical value within a range, typically representing measurements on a continuous scale.</p> <p><strong>Example:</strong> Height (175.3 cm), temperature (72.5°F), and income ($45,250.00) are continuous features.</p> <h4 id=convergence-criteria>Convergence Criteria<a class=headerlink href=#convergence-criteria title="Permanent link">&para;</a></h4> <p>The conditions that determine when an iterative optimization algorithm should stop, typically based on change in loss or parameters falling below a threshold.</p> <p><strong>Example:</strong> K-means stops when centroids move less than 0.001 units between iterations or after 300 iterations, whichever comes first.</p> <h4 id=convolution-operation>Convolution Operation<a class=headerlink href=#convolution-operation title="Permanent link">&para;</a></h4> <p>A mathematical operation that slides a filter (kernel) across an input, computing element-wise products and summing the results at each position to produce a feature map.</p> <p><strong>Example:</strong> Convolving a 3×3 edge detection filter across a 28×28 image produces a 26×26 feature map highlighting edges.</p> <h4 id=convolutional-neural-network>Convolutional Neural Network<a class=headerlink href=#convolutional-neural-network title="Permanent link">&para;</a></h4> <p>A type of deep neural network specialized for processing grid-structured data (like images) that uses convolution operations to learn hierarchical spatial features.</p> <p><strong>Example:</strong> A CNN for image classification might use 5 convolutional layers followed by 2 fully connected layers to classify objects in photos.</p> <h4 id=cross-entropy-loss>Cross-Entropy Loss<a class=headerlink href=#cross-entropy-loss title="Permanent link">&para;</a></h4> <p>A loss function measuring the difference between predicted probability distributions and true distributions, commonly used for classification tasks.</p> <p><strong>Example:</strong> For binary classification, cross-entropy loss is -[y log(p) + (1-y) log(1-p)], where y is the true label and p is the predicted probability.</p> <h4 id=cross-validation>Cross-Validation<a class=headerlink href=#cross-validation title="Permanent link">&para;</a></h4> <p>A model evaluation technique that partitions data into multiple subsets, training on some subsets while testing on others, then averaging results across all partitions.</p> <p><strong>Example:</strong> 5-fold cross-validation splits data into 5 parts, trains on 4 parts and tests on the remaining part, repeating 5 times with different test folds.</p> <h4 id=curse-of-dimensionality>Curse of Dimensionality<a class=headerlink href=#curse-of-dimensionality title="Permanent link">&para;</a></h4> <p>The phenomenon where data becomes increasingly sparse and distances become less meaningful as the number of features (dimensions) increases.</p> <p><strong>Example:</strong> In 10 dimensions, you need exponentially more data points than in 2 dimensions to maintain the same data density for k-NN to work effectively.</p> <h4 id=data-augmentation>Data Augmentation<a class=headerlink href=#data-augmentation title="Permanent link">&para;</a></h4> <p>Artificially expanding a training dataset by applying transformations that preserve labels while creating new variations of existing examples.</p> <p><strong>Example:</strong> For image classification, rotating, flipping, cropping, and adjusting brightness of training images creates additional training examples without collecting new data.</p> <h4 id=data-preprocessing>Data Preprocessing<a class=headerlink href=#data-preprocessing title="Permanent link">&para;</a></h4> <p>The process of transforming raw data into a format suitable for machine learning algorithms through cleaning, scaling, encoding, and feature engineering.</p> <p><strong>Example:</strong> Preprocessing includes removing missing values, scaling features to [0,1] range, and converting categorical variables to one-hot vectors.</p> <h4 id=decision-boundary>Decision Boundary<a class=headerlink href=#decision-boundary title="Permanent link">&para;</a></h4> <p>The surface or curve in feature space that separates regions belonging to different classes in a classification model.</p> <p><strong>Example:</strong> A linear SVM's decision boundary is a straight line (in 2D) or hyperplane (in higher dimensions) that maximally separates positive and negative examples.</p> <h4 id=decision-tree>Decision Tree<a class=headerlink href=#decision-tree title="Permanent link">&para;</a></h4> <p>A tree-structured model that makes predictions by recursively splitting the feature space based on feature values, with decision rules at internal nodes and predictions at leaf nodes.</p> <p><strong>Example:</strong> A decision tree for iris classification might split first on petal length &lt; 2.5 cm (setosa vs. others), then on petal width &lt; 1.8 cm (versicolor vs. virginica).</p> <h4 id=deep-learning>Deep Learning<a class=headerlink href=#deep-learning title="Permanent link">&para;</a></h4> <p>A subfield of machine learning focused on neural networks with multiple hidden layers that can learn hierarchical representations of data.</p> <p><strong>Example:</strong> A 50-layer ResNet that learns to recognize objects by building representations from edges to textures to parts to complete objects.</p> <h4 id=dimensionality-reduction>Dimensionality Reduction<a class=headerlink href=#dimensionality-reduction title="Permanent link">&para;</a></h4> <p>Techniques for reducing the number of features in a dataset while preserving important information and structure.</p> <p><strong>Example:</strong> Principal Component Analysis (PCA) reduces 100 features to 10 principal components that capture 95% of the data variance.</p> <h4 id=distance-metric>Distance Metric<a class=headerlink href=#distance-metric title="Permanent link">&para;</a></h4> <p>A function that quantifies the dissimilarity or separation between two data points in feature space.</p> <p><strong>Example:</strong> Euclidean distance between points (1, 2) and (4, 6) is √[(4-1)² + (6-2)²] = 5.</p> <h4 id=domain-adaptation>Domain Adaptation<a class=headerlink href=#domain-adaptation title="Permanent link">&para;</a></h4> <p>Techniques for transferring knowledge from a source domain to a related but different target domain to improve learning with limited target domain data.</p> <p><strong>Example:</strong> A model trained on daytime outdoor images (source domain) is adapted to work on nighttime images (target domain) through fine-tuning.</p> <h4 id=dropout>Dropout<a class=headerlink href=#dropout title="Permanent link">&para;</a></h4> <p>A regularization technique that randomly sets a fraction of neuron activations to zero during training to prevent overfitting by reducing co-adaptation between neurons.</p> <p><strong>Example:</strong> With dropout rate 0.5, each neuron has a 50% probability of being temporarily removed during each training iteration.</p> <h4 id=dual-formulation>Dual Formulation<a class=headerlink href=#dual-formulation title="Permanent link">&para;</a></h4> <p>An alternative mathematical formulation of an optimization problem (like SVM) expressed in terms of Lagrange multipliers rather than the original primal variables.</p> <p><strong>Example:</strong> The dual formulation of SVM allows the kernel trick by expressing the solution entirely in terms of dot products between training examples.</p> <h4 id=early-stopping>Early Stopping<a class=headerlink href=#early-stopping title="Permanent link">&para;</a></h4> <p>A regularization technique that halts training when validation performance stops improving, preventing overfitting that occurs from training too long.</p> <p><strong>Example:</strong> Training stops at epoch 47 when validation loss hasn't decreased for 10 consecutive epochs, even though training was set for 100 epochs.</p> <h4 id=elbow-method>Elbow Method<a class=headerlink href=#elbow-method title="Permanent link">&para;</a></h4> <p>A heuristic for selecting the number of clusters in k-means by plotting inertia versus k and choosing the "elbow" point where adding more clusters yields diminishing returns.</p> <p><strong>Example:</strong> If inertia drops sharply from k=1 to k=4 but only slightly from k=4 to k=10, choose k=4 as the optimal number of clusters.</p> <h4 id=entropy>Entropy<a class=headerlink href=#entropy title="Permanent link">&para;</a></h4> <p>A measure of impurity or disorder in a dataset, quantifying the average amount of information needed to identify the class of a randomly selected instance.</p> <p><strong>Example:</strong> A dataset with 50% positive and 50% negative examples has maximum entropy of 1.0 bit, while a pure dataset (100% one class) has zero entropy.</p> <h4 id=epoch>Epoch<a class=headerlink href=#epoch title="Permanent link">&para;</a></h4> <p>One complete pass through the entire training dataset during neural network training.</p> <p><strong>Example:</strong> Training for 50 epochs on a dataset of 10,000 images means the model sees all 10,000 images 50 times during training.</p> <h4 id=euclidean-distance>Euclidean Distance<a class=headerlink href=#euclidean-distance title="Permanent link">&para;</a></h4> <p>The straight-line distance between two points in feature space, calculated as the square root of the sum of squared differences across all dimensions.</p> <p><strong>Example:</strong> Euclidean distance between (1,2,3) and (4,5,6) is √[(4-1)² + (5-2)² + (6-3)²] = √27 ≈ 5.20.</p> <h4 id=exploding-gradient>Exploding Gradient<a class=headerlink href=#exploding-gradient title="Permanent link">&para;</a></h4> <p>A numerical instability during neural network training where gradients grow exponentially large, causing weight updates that destabilize learning.</p> <p><strong>Example:</strong> In a 100-layer network without proper initialization, gradients might grow from 0.01 at layer 100 to 10^30 at layer 1, causing NaN values.</p> <h4 id=f1-score>F1 Score<a class=headerlink href=#f1-score title="Permanent link">&para;</a></h4> <p>The harmonic mean of precision and recall, providing a single metric that balances both measures of classifier performance.</p> <p><strong>Example:</strong> With precision 0.8 and recall 0.6, F1 score is 2×(0.8×0.6)/(0.8+0.6) = 0.686.</p> <h4 id=false-negative>False Negative<a class=headerlink href=#false-negative title="Permanent link">&para;</a></h4> <p>An instance where the model incorrectly predicts the negative class when the true class is positive (Type II error).</p> <p><strong>Example:</strong> A medical test that fails to detect a disease in a patient who actually has the disease is a false negative.</p> <h4 id=false-positive>False Positive<a class=headerlink href=#false-positive title="Permanent link">&para;</a></h4> <p>An instance where the model incorrectly predicts the positive class when the true class is negative (Type I error).</p> <p><strong>Example:</strong> A spam filter that marks a legitimate email as spam is a false positive.</p> <h4 id=feature>Feature<a class=headerlink href=#feature title="Permanent link">&para;</a></h4> <p>An individual measurable property or characteristic of an instance used as input to a machine learning model.</p> <p><strong>Example:</strong> In house price prediction, features include square footage, number of bedrooms, and location zip code.</p> <h4 id=feature-engineering>Feature Engineering<a class=headerlink href=#feature-engineering title="Permanent link">&para;</a></h4> <p>The process of creating new features or transforming existing features to improve model performance.</p> <p><strong>Example:</strong> Creating a "price per square foot" feature by dividing house price by square footage, or extracting day-of-week from a timestamp.</p> <h4 id=feature-extraction>Feature Extraction<a class=headerlink href=#feature-extraction title="Permanent link">&para;</a></h4> <p>Using a pre-trained model as a fixed feature extractor by freezing its weights and only training a new classification head on the extracted features.</p> <p><strong>Example:</strong> Using a frozen ResNet-50 (pre-trained on ImageNet) to extract 2048-dimensional feature vectors, then training only a new linear classifier on these features.</p> <h4 id=feature-map>Feature Map<a class=headerlink href=#feature-map title="Permanent link">&para;</a></h4> <p>The output of applying a filter through convolution across an input, highlighting specific patterns or features detected at different spatial locations.</p> <p><strong>Example:</strong> A 3×3 edge detection filter applied to a 28×28 image produces a 26×26 feature map with high values where edges are detected.</p> <h4 id=feature-selection>Feature Selection<a class=headerlink href=#feature-selection title="Permanent link">&para;</a></h4> <p>The process of choosing a subset of relevant features from the original feature set to reduce dimensionality and improve model performance.</p> <p><strong>Example:</strong> Using correlation analysis to select the 20 most predictive features from an original set of 100 features.</p> <h4 id=feature-space-partitioning>Feature Space Partitioning<a class=headerlink href=#feature-space-partitioning title="Permanent link">&para;</a></h4> <p>The division of feature space into regions or subspaces, each associated with a specific prediction or decision.</p> <p><strong>Example:</strong> A decision tree partitions 2D feature space into rectangular regions, each corresponding to a different class prediction.</p> <h4 id=feature-vector>Feature Vector<a class=headerlink href=#feature-vector title="Permanent link">&para;</a></h4> <p>A numerical array representing all features of a single instance, serving as input to machine learning models.</p> <p><strong>Example:</strong> An iris flower represented as a 4-dimensional feature vector [5.1, 3.5, 1.4, 0.2] for sepal length, sepal width, petal length, and petal width.</p> <h4 id=filter>Filter<a class=headerlink href=#filter title="Permanent link">&para;</a></h4> <p>A small matrix of learnable weights that slides across input data in a convolutional layer to detect specific patterns or features.</p> <p><strong>Example:</strong> A 3×3 filter might learn to detect vertical edges by having weights like [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]].</p> <h4 id=fine-tuning>Fine-Tuning<a class=headerlink href=#fine-tuning title="Permanent link">&para;</a></h4> <p>Continuing to train a pre-trained model on a new dataset by updating all or most of its weights with a small learning rate.</p> <p><strong>Example:</strong> Fine-tuning a ResNet-50 pre-trained on ImageNet by training all layers with learning rate 0.0001 on a cats-vs-dogs dataset.</p> <h4 id=forward-propagation>Forward Propagation<a class=headerlink href=#forward-propagation title="Permanent link">&para;</a></h4> <p>The process of computing a neural network's output by passing input data through each layer sequentially, applying weights, biases, and activation functions.</p> <p><strong>Example:</strong> In forward propagation, an input image flows through convolutional layers, pooling layers, and fully connected layers to produce class probabilities.</p> <h4 id=freezing-layers>Freezing Layers<a class=headerlink href=#freezing-layers title="Permanent link">&para;</a></h4> <p>Setting neural network layers to non-trainable by preventing their weights from being updated during training, commonly used in transfer learning.</p> <p><strong>Example:</strong> Freezing the first 40 layers of a 50-layer ResNet while fine-tuning only the last 10 layers on a new dataset.</p> <h4 id=fully-connected-layer>Fully Connected Layer<a class=headerlink href=#fully-connected-layer title="Permanent link">&para;</a></h4> <p>A neural network layer where every neuron connects to every neuron in the previous layer, with each connection having a learnable weight.</p> <p><strong>Example:</strong> A fully connected layer with 512 input neurons and 10 output neurons has 512 × 10 = 5,120 trainable weight parameters (plus 10 biases).</p> <h4 id=gaussian-kernel>Gaussian Kernel<a class=headerlink href=#gaussian-kernel title="Permanent link">&para;</a></h4> <p>A radial basis function kernel for support vector machines that measures similarity between points based on Gaussian-weighted distance, equivalent to RBF kernel.</p> <p><strong>Example:</strong> The Gaussian kernel K(x, y) = exp(-γ||x - y||²) with γ=0.1 assigns similarity close to 1 for nearby points and close to 0 for distant points.</p> <h4 id=generalization>Generalization<a class=headerlink href=#generalization title="Permanent link">&para;</a></h4> <p>A model's ability to perform well on new, unseen data drawn from the same distribution as the training data.</p> <p><strong>Example:</strong> A model with 95% training accuracy and 94% test accuracy generalizes well, while one with 99% training accuracy and 70% test accuracy overfits.</p> <h4 id=gini-impurity>Gini Impurity<a class=headerlink href=#gini-impurity title="Permanent link">&para;</a></h4> <p>A measure of impurity for decision tree splitting that quantifies the probability of incorrectly classifying a randomly chosen instance if classified according to class distribution.</p> <p><strong>Example:</strong> A node with 40 positive and 60 negative examples has Gini impurity = 1 - (0.4² + 0.6²) = 0.48.</p> <h4 id=gradient-clipping>Gradient Clipping<a class=headerlink href=#gradient-clipping title="Permanent link">&para;</a></h4> <p>A technique that limits the magnitude of gradients during backpropagation to prevent exploding gradients by scaling them when they exceed a threshold.</p> <p><strong>Example:</strong> Clipping gradients to maximum norm 1.0 means if the gradient vector has norm 5.0, it's scaled down by a factor of 5 to have norm 1.0.</p> <h4 id=gradient-descent>Gradient Descent<a class=headerlink href=#gradient-descent title="Permanent link">&para;</a></h4> <p>An iterative optimization algorithm that updates model parameters in the direction opposite to the gradient of the loss function to minimize loss.</p> <p><strong>Example:</strong> Starting with random weights, gradient descent repeatedly computes gradients and updates weights as w_new = w_old - learning_rate × gradient until convergence.</p> <h4 id=grid-search>Grid Search<a class=headerlink href=#grid-search title="Permanent link">&para;</a></h4> <p>A hyperparameter tuning method that exhaustively evaluates all combinations of specified hyperparameter values using cross-validation.</p> <p><strong>Example:</strong> Grid search over learning rates [0.001, 0.01, 0.1] and regularization strengths [0.1, 1, 10] trains and evaluates 3 × 3 = 9 models.</p> <h4 id=hard-margin-svm>Hard Margin SVM<a class=headerlink href=#hard-margin-svm title="Permanent link">&para;</a></h4> <p>A support vector machine that requires perfect linear separation of classes with no training errors allowed.</p> <p><strong>Example:</strong> Hard margin SVM works on linearly separable data but fails if even a single training example cannot be correctly classified with a linear boundary.</p> <h4 id=he-initialization>He Initialization<a class=headerlink href=#he-initialization title="Permanent link">&para;</a></h4> <p>A weight initialization strategy for neural networks using ReLU activation that draws weights from a Gaussian distribution with variance 2/n_in, where n_in is the number of input units.</p> <p><strong>Example:</strong> Initializing a layer with 256 input neurons using He initialization samples weights from N(0, √(2/256)).</p> <h4 id=hidden-layer>Hidden Layer<a class=headerlink href=#hidden-layer title="Permanent link">&para;</a></h4> <p>An intermediate layer in a neural network between the input and output layers that learns internal representations of the data.</p> <p><strong>Example:</strong> A 3-layer neural network has an input layer, one hidden layer with 128 neurons, and an output layer with 10 neurons.</p> <h4 id=holdout-method>Holdout Method<a class=headerlink href=#holdout-method title="Permanent link">&para;</a></h4> <p>A model evaluation approach that splits data into separate training and test sets, trains on the training set, and evaluates on the held-out test set.</p> <p><strong>Example:</strong> Using an 80/20 split, train a model on 80% of data and evaluate on the remaining 20% that was held out.</p> <h4 id=hyperparameter>Hyperparameter<a class=headerlink href=#hyperparameter title="Permanent link">&para;</a></h4> <p>A configuration setting for a learning algorithm that is set before training begins rather than learned from data.</p> <p><strong>Example:</strong> Learning rate, number of trees in a random forest, and k in k-nearest neighbors are all hyperparameters.</p> <h4 id=hyperparameter-tuning>Hyperparameter Tuning<a class=headerlink href=#hyperparameter-tuning title="Permanent link">&para;</a></h4> <p>The process of finding optimal hyperparameter values to maximize model performance, typically using cross-validation.</p> <p><strong>Example:</strong> Testing learning rates from 0.0001 to 0.1 and batch sizes from 16 to 128 to find the combination that minimizes validation loss.</p> <h4 id=hyperplane>Hyperplane<a class=headerlink href=#hyperplane title="Permanent link">&para;</a></h4> <p>A linear subspace of dimension n-1 in an n-dimensional space that divides the space into two half-spaces, used as decision boundaries in linear classifiers.</p> <p><strong>Example:</strong> In 3D space, a hyperplane is a 2D plane defined by equation w₁x₁ + w₂x₂ + w₃x₃ + b = 0 that separates positive and negative examples.</p> <h4 id=imagenet>ImageNet<a class=headerlink href=#imagenet title="Permanent link">&para;</a></h4> <p>A large-scale image database containing 14 million labeled images across 20,000+ categories, with a subset of 1.2 million images in 1,000 categories commonly used for training deep learning models.</p> <p><strong>Example:</strong> Pre-trained models like ResNet-50 are trained on ImageNet's 1,000-class subset before being fine-tuned for specific tasks.</p> <h4 id=inception>Inception<a class=headerlink href=#inception title="Permanent link">&para;</a></h4> <p>A CNN architecture family (including GoogLeNet, Inception-v3, Inception-v4) that uses inception modules with parallel convolutional filters of different sizes to capture multi-scale features efficiently.</p> <p><strong>Example:</strong> An inception module applies 1×1, 3×3, and 5×5 convolutions in parallel, concatenating their outputs to capture features at multiple scales simultaneously.</p> <h4 id=inertia>Inertia<a class=headerlink href=#inertia title="Permanent link">&para;</a></h4> <p>The sum of squared distances from each data point to its assigned cluster centroid in k-means clustering, measuring cluster compactness.</p> <p><strong>Example:</strong> Lower inertia indicates tighter clusters; inertia of 150 means the average squared distance from points to their centroids is 150.</p> <h4 id=information-gain>Information Gain<a class=headerlink href=#information-gain title="Permanent link">&para;</a></h4> <p>The reduction in entropy (or increase in information) achieved by splitting a dataset on a particular feature, used to select the best split in decision trees.</p> <p><strong>Example:</strong> If parent node entropy is 0.8 and splitting produces children with entropies 0.3 and 0.5, information gain is 0.8 - weighted_average(0.3, 0.5).</p> <h4 id=input-layer>Input Layer<a class=headerlink href=#input-layer title="Permanent link">&para;</a></h4> <p>The first layer of a neural network that receives raw input features and passes them to subsequent layers.</p> <p><strong>Example:</strong> For 28×28 grayscale images, the input layer has 784 neurons (28 × 28), one for each pixel value.</p> <h4 id=instance>Instance<a class=headerlink href=#instance title="Permanent link">&para;</a></h4> <p>A single data point, observation, or example in a dataset, consisting of feature values and optionally a label.</p> <p><strong>Example:</strong> One row in a dataset representing a single iris flower with measurements [5.1, 3.5, 1.4, 0.2, "setosa"] is an instance.</p> <h4 id=k-fold-cross-validation>K-Fold Cross-Validation<a class=headerlink href=#k-fold-cross-validation title="Permanent link">&para;</a></h4> <p>A cross-validation technique that divides data into k equal folds, using each fold once as a test set while training on the remaining k-1 folds, then averaging performance across all k trials.</p> <p><strong>Example:</strong> 10-fold cross-validation on 1,000 examples creates 10 train/test splits, each using 900 examples for training and 100 for testing.</p> <h4 id=k-means-clustering>K-Means Clustering<a class=headerlink href=#k-means-clustering title="Permanent link">&para;</a></h4> <p>An unsupervised learning algorithm that partitions data into k clusters by iteratively assigning points to nearest centroids and updating centroids as cluster means.</p> <p><strong>Example:</strong> K-means with k=3 on customer data might discover three natural segments: high-value, medium-value, and low-value customers.</p> <h4 id=k-means-initialization>K-Means Initialization<a class=headerlink href=#k-means-initialization title="Permanent link">&para;</a></h4> <p>The method for setting initial centroid positions before k-means iterations begin, significantly affecting convergence and final cluster quality.</p> <p><strong>Example:</strong> Random initialization selects k random data points as initial centroids, while k-means++ selects centroids spread far apart to improve results.</p> <h4 id=k-means-initialization_1>K-Means++ Initialization<a class=headerlink href=#k-means-initialization_1 title="Permanent link">&para;</a></h4> <p>An improved initialization method for k-means that selects initial centroids probabilistically, favoring points far from already-chosen centroids to improve clustering.</p> <p><strong>Example:</strong> K-means++ first selects one random centroid, then selects each subsequent centroid with probability proportional to its squared distance from the nearest existing centroid.</p> <h4 id=k-nearest-neighbors>K-Nearest Neighbors<a class=headerlink href=#k-nearest-neighbors title="Permanent link">&para;</a></h4> <p>A non-parametric algorithm that predicts a query point's label based on the majority class (classification) or average value (regression) of its k nearest training examples.</p> <p><strong>Example:</strong> 5-NN for iris classification finds the 5 closest training flowers to a new flower and predicts the majority species among those 5 neighbors.</p> <h4 id=k-selection>K Selection<a class=headerlink href=#k-selection title="Permanent link">&para;</a></h4> <p>The process of choosing an appropriate value for k (number of neighbors) in k-nearest neighbors or k (number of clusters) in k-means.</p> <p><strong>Example:</strong> Testing k values from 1 to 20 using cross-validation and selecting k=7 because it achieves the lowest validation error.</p> <h4 id=kernel-size>Kernel Size<a class=headerlink href=#kernel-size title="Permanent link">&para;</a></h4> <p>The dimensions of a convolutional filter, typically specified as height × width (and optionally depth for multi-channel inputs).</p> <p><strong>Example:</strong> A kernel size of 3×3 means the filter covers a 3×3 spatial region when sliding across the input.</p> <h4 id=kernel-trick>Kernel Trick<a class=headerlink href=#kernel-trick title="Permanent link">&para;</a></h4> <p>A mathematical technique in SVMs that implicitly maps data to high-dimensional feature spaces using kernel functions without explicitly computing the transformation.</p> <p><strong>Example:</strong> The RBF kernel allows SVMs to learn non-linear decision boundaries by implicitly mapping 2D data to infinite-dimensional space while only computing dot products.</p> <h4 id=knn-for-classification>KNN for Classification<a class=headerlink href=#knn-for-classification title="Permanent link">&para;</a></h4> <p>Applying k-nearest neighbors to classification tasks by assigning a query point the majority class among its k nearest neighbors.</p> <p><strong>Example:</strong> In binary classification with k=5, if a point's 5 nearest neighbors include 4 positive and 1 negative examples, predict positive class.</p> <h4 id=knn-for-regression>KNN for Regression<a class=headerlink href=#knn-for-regression title="Permanent link">&para;</a></h4> <p>Applying k-nearest neighbors to regression tasks by predicting a query point's value as the average of its k nearest neighbors' values.</p> <p><strong>Example:</strong> For house price prediction with k=3, predict the price as the average of the 3 most similar houses' prices.</p> <h4 id=l1-regularization>L1 Regularization<a class=headerlink href=#l1-regularization title="Permanent link">&para;</a></h4> <p>A regularization technique that adds the sum of absolute values of weights to the loss function, encouraging sparse models with many weights driven to exactly zero.</p> <p><strong>Example:</strong> L1 penalty λ Σ|wᵢ| with λ=0.01 penalizes large weights, often resulting in 80% of weights becoming exactly zero (feature selection).</p> <h4 id=l2-regularization>L2 Regularization<a class=headerlink href=#l2-regularization title="Permanent link">&para;</a></h4> <p>A regularization technique that adds the sum of squared weights to the loss function, encouraging small but non-zero weights.</p> <p><strong>Example:</strong> L2 penalty λ Σwᵢ² with λ=0.01 shrinks all weights toward zero proportionally without making them exactly zero.</p> <h4 id=label>Label<a class=headerlink href=#label title="Permanent link">&para;</a></h4> <p>The target output or ground truth value associated with an instance in supervised learning, representing the correct answer the model should learn to predict.</p> <p><strong>Example:</strong> In email classification, labels are "spam" or "not spam"; in house price prediction, labels are dollar amounts.</p> <h4 id=label-encoding>Label Encoding<a class=headerlink href=#label-encoding title="Permanent link">&para;</a></h4> <p>Converting categorical variables to integers by assigning each unique category a number, creating an ordinal relationship.</p> <p><strong>Example:</strong> Encoding colors {red, blue, green} as {0, 1, 2}.</p> <h4 id=lasso-regression>Lasso Regression<a class=headerlink href=#lasso-regression title="Permanent link">&para;</a></h4> <p>Linear regression with L1 regularization that performs automatic feature selection by driving some coefficients to exactly zero.</p> <p><strong>Example:</strong> Lasso regression with α=1.0 on 100 features might select only 15 non-zero coefficients, effectively performing feature selection.</p> <h4 id=lazy-learning>Lazy Learning<a class=headerlink href=#lazy-learning title="Permanent link">&para;</a></h4> <p>A learning paradigm where the algorithm defers processing until prediction time rather than building an explicit model during training.</p> <p><strong>Example:</strong> K-nearest neighbors is lazy learning because it stores all training data and performs computation only when making predictions, unlike eager learners like decision trees.</p> <h4 id=leaf-node>Leaf Node<a class=headerlink href=#leaf-node title="Permanent link">&para;</a></h4> <p>A terminal node in a decision tree that contains no children and makes a final prediction based on the training instances that reach it.</p> <p><strong>Example:</strong> A leaf node in an iris decision tree might predict "setosa" with 100% confidence based on 50 training instances that all belong to the setosa class.</p> <h4 id=learning-rate>Learning Rate<a class=headerlink href=#learning-rate title="Permanent link">&para;</a></h4> <p>A hyperparameter controlling the step size for weight updates during gradient descent optimization.</p> <p><strong>Example:</strong> With learning rate 0.01, if the gradient is 5.0, the weight update is 0.01 × 5.0 = 0.05.</p> <h4 id=learning-rate-scheduling>Learning Rate Scheduling<a class=headerlink href=#learning-rate-scheduling title="Permanent link">&para;</a></h4> <p>Adjusting the learning rate during training according to a predefined schedule or adaptive rule to improve convergence.</p> <p><strong>Example:</strong> Starting with learning rate 0.1 and multiplying by 0.1 every 10 epochs: epochs 1-10 use 0.1, epochs 11-20 use 0.01, epochs 21-30 use 0.001.</p> <h4 id=lenet>LeNet<a class=headerlink href=#lenet title="Permanent link">&para;</a></h4> <p>An early convolutional neural network architecture designed by Yann LeCun for handwritten digit recognition, featuring 2 convolutional layers followed by 3 fully connected layers.</p> <p><strong>Example:</strong> LeNet-5 processes 32×32 grayscale images through Conv→Pool→Conv→Pool→FC→FC→FC layers to classify MNIST digits.</p> <h4 id=linear-kernel>Linear Kernel<a class=headerlink href=#linear-kernel title="Permanent link">&para;</a></h4> <p>A kernel function for support vector machines that computes the dot product between feature vectors without transformation, suitable for linearly separable data.</p> <p><strong>Example:</strong> Linear kernel K(x, y) = x · y creates the same decision boundary as a linear SVM without kernels but allows the dual formulation.</p> <h4 id=local-connectivity>Local Connectivity<a class=headerlink href=#local-connectivity title="Permanent link">&para;</a></h4> <p>A property of convolutional layers where each neuron connects only to a small local region of the input rather than all input units.</p> <p><strong>Example:</strong> In a convolutional layer, each neuron in the feature map connects to only a 3×3 patch of the previous layer instead of all pixels.</p> <h4 id=log-loss>Log-Loss<a class=headerlink href=#log-loss title="Permanent link">&para;</a></h4> <p>The cross-entropy loss function for binary classification, measuring the negative log-likelihood of the true labels given the predicted probabilities.</p> <p><strong>Example:</strong> For true label y=1 and predicted probability p=0.9, log-loss is -log(0.9) ≈ 0.105, penalizing incorrect predictions exponentially.</p> <h4 id=logistic-regression>Logistic Regression<a class=headerlink href=#logistic-regression title="Permanent link">&para;</a></h4> <p>A linear classification algorithm that models class probabilities using the logistic (sigmoid) function and estimates weights via maximum likelihood.</p> <p><strong>Example:</strong> Logistic regression for spam detection computes P(spam|email) = σ(w₁×word_count + w₂×link_count + b), where σ is the sigmoid function.</p> <h4 id=loss-function>Loss Function<a class=headerlink href=#loss-function title="Permanent link">&para;</a></h4> <p>A function measuring the difference between a model's predictions and true labels, quantifying how well the model performs on the training data.</p> <p><strong>Example:</strong> Mean squared error (MSE) loss for regression: (1/n) Σ(yᵢ - ŷᵢ)², where yᵢ is true value and ŷᵢ is predicted value.</p> <h4 id=machine-learning>Machine Learning<a class=headerlink href=#machine-learning title="Permanent link">&para;</a></h4> <p>A field of artificial intelligence focused on developing algorithms that improve automatically through experience and data without explicit programming.</p> <p><strong>Example:</strong> A machine learning system learns to recognize cats in photos by analyzing thousands of labeled cat and non-cat images, discovering visual patterns automatically.</p> <h4 id=manhattan-distance>Manhattan Distance<a class=headerlink href=#manhattan-distance title="Permanent link">&para;</a></h4> <p>The distance between two points calculated as the sum of absolute differences across all dimensions, equivalent to the distance traveled along grid lines.</p> <p><strong>Example:</strong> Manhattan distance between (1, 2) and (4, 6) is |4-1| + |6-2| = 7.</p> <h4 id=margin>Margin<a class=headerlink href=#margin title="Permanent link">&para;</a></h4> <p>The perpendicular distance from the decision boundary to the nearest training examples (support vectors) in support vector machines.</p> <p><strong>Example:</strong> If the decision boundary is 2x₁ + x₂ = 5 and the nearest point is at (2, 1), the margin is the perpendicular distance from this point to the line.</p> <h4 id=margin-maximization>Margin Maximization<a class=headerlink href=#margin-maximization title="Permanent link">&para;</a></h4> <p>The optimization objective in support vector machines of finding the decision boundary that maximizes the margin between classes.</p> <p><strong>Example:</strong> Among infinitely many hyperplanes that separate positive and negative examples, SVM selects the one with maximum distance to the nearest examples on both sides.</p> <h4 id=max-pooling>Max Pooling<a class=headerlink href=#max-pooling title="Permanent link">&para;</a></h4> <p>A pooling operation that selects the maximum value within each pooling window to downsample feature maps while retaining the strongest activations.</p> <p><strong>Example:</strong> Max pooling over a 2×2 window containing values [1, 3, 2, 4] outputs 4, the maximum value.</p> <h4 id=maximum-likelihood>Maximum Likelihood<a class=headerlink href=#maximum-likelihood title="Permanent link">&para;</a></h4> <p>A principle for estimating model parameters by finding values that maximize the probability of observing the training data.</p> <p><strong>Example:</strong> In logistic regression, maximum likelihood estimation finds weights that make the observed class labels most probable given the features.</p> <h4 id=mean-squared-error>Mean Squared Error<a class=headerlink href=#mean-squared-error title="Permanent link">&para;</a></h4> <p>A loss function for regression that computes the average of squared differences between predicted and true values.</p> <p><strong>Example:</strong> For predictions [2, 3, 5] and true values [1, 4, 4], MSE = [(2-1)² + (3-4)² + (5-4)²]/3 = 1.</p> <h4 id=mini-batch-gradient-descent>Mini-Batch Gradient Descent<a class=headerlink href=#mini-batch-gradient-descent title="Permanent link">&para;</a></h4> <p>A gradient descent variant that computes gradients and updates weights using a randomly selected subset (mini-batch) of training examples.</p> <p><strong>Example:</strong> With batch size 32, mini-batch gradient descent uses 32 randomly sampled examples to compute each gradient update, balancing efficiency and gradient quality.</p> <h4 id=min-max-scaling>Min-Max Scaling<a class=headerlink href=#min-max-scaling title="Permanent link">&para;</a></h4> <p>A normalization technique that linearly transforms features to a fixed range (typically [0, 1]) by subtracting the minimum and dividing by the range.</p> <p><strong>Example:</strong> Feature with values [10, 20, 30] normalized to [0, 1] range becomes [0, 0.5, 1] using formula (x - min)/(max - min).</p> <h4 id=model>Model<a class=headerlink href=#model title="Permanent link">&para;</a></h4> <p>A mathematical representation learned from data that maps inputs to outputs, encapsulating the patterns discovered during training.</p> <p><strong>Example:</strong> A trained decision tree model with specific split thresholds and leaf predictions represents the learned relationship between features and labels.</p> <h4 id=model-evaluation>Model Evaluation<a class=headerlink href=#model-evaluation title="Permanent link">&para;</a></h4> <p>The process of measuring a trained model's performance using appropriate metrics on test data to assess its predictive capability.</p> <p><strong>Example:</strong> Evaluating a classifier using accuracy, precision, recall, and F1 score on a held-out test set to quantify performance.</p> <h4 id=model-selection>Model Selection<a class=headerlink href=#model-selection title="Permanent link">&para;</a></h4> <p>The process of choosing the best model architecture or algorithm from multiple candidates based on validation performance.</p> <p><strong>Example:</strong> Comparing k-NN, decision trees, and logistic regression using cross-validation and selecting the model with lowest validation error.</p> <h4 id=model-zoo>Model Zoo<a class=headerlink href=#model-zoo title="Permanent link">&para;</a></h4> <p>A collection of pre-trained neural network models publicly available for download and use in transfer learning.</p> <p><strong>Example:</strong> PyTorch's model zoo provides pre-trained ResNet, VGG, and AlexNet models trained on ImageNet that can be downloaded and fine-tuned.</p> <h4 id=momentum>Momentum<a class=headerlink href=#momentum title="Permanent link">&para;</a></h4> <p>An optimization technique that accelerates gradient descent by accumulating a velocity vector in the direction of persistent gradient components.</p> <p><strong>Example:</strong> Momentum with coefficient 0.9 makes the optimizer build up speed in directions where gradients consistently point the same way, like a ball rolling downhill.</p> <h4 id=multiclass-classification>Multiclass Classification<a class=headerlink href=#multiclass-classification title="Permanent link">&para;</a></h4> <p>A classification task where instances must be assigned to one of three or more distinct classes.</p> <p><strong>Example:</strong> Classifying iris flowers into three species (setosa, versicolor, virginica) or recognizing handwritten digits (0-9) are multiclass problems.</p> <h4 id=multilayer-perceptron>Multilayer Perceptron<a class=headerlink href=#multilayer-perceptron title="Permanent link">&para;</a></h4> <p>A feedforward neural network with one or more hidden layers between input and output layers, capable of learning non-linear relationships.</p> <p><strong>Example:</strong> A multilayer perceptron with architecture [784, 128, 64, 10] has an input layer (784 pixels), two hidden layers (128 and 64 neurons), and an output layer (10 classes).</p> <h4 id=nesterov-momentum>Nesterov Momentum<a class=headerlink href=#nesterov-momentum title="Permanent link">&para;</a></h4> <p>An improved momentum variant that computes gradients at an approximate future position rather than the current position, often converging faster.</p> <p><strong>Example:</strong> Nesterov momentum looks ahead by temporarily moving in the momentum direction before computing the gradient, providing better anticipation of the loss landscape.</p> <h4 id=network-architecture>Network Architecture<a class=headerlink href=#network-architecture title="Permanent link">&para;</a></h4> <p>The overall structure and organization of a neural network, specifying the number, types, and connections of layers.</p> <p><strong>Example:</strong> Architecture [Input: 784] → [Dense: 512, ReLU] → [Dropout: 0.5] → [Dense: 10, Softmax] defines a 2-layer fully connected network.</p> <h4 id=neural-network>Neural Network<a class=headerlink href=#neural-network title="Permanent link">&para;</a></h4> <p>A computational model inspired by biological neural networks consisting of interconnected nodes (neurons) organized in layers that learn to map inputs to outputs.</p> <p><strong>Example:</strong> A neural network for image classification consists of an input layer receiving pixel values, hidden layers learning features, and an output layer producing class probabilities.</p> <h4 id=normalization>Normalization<a class=headerlink href=#normalization title="Permanent link">&para;</a></h4> <p>Transforming features to a common scale to improve learning algorithm performance and convergence.</p> <p><strong>Example:</strong> Normalizing all features to the range [0, 1] using min-max scaling or to mean 0 and standard deviation 1 using standardization.</p> <h4 id=one-hot-encoding>One-Hot Encoding<a class=headerlink href=#one-hot-encoding title="Permanent link">&para;</a></h4> <p>Converting categorical variables into binary vectors where exactly one element is 1 (hot) and all others are 0, creating separate features for each category.</p> <p><strong>Example:</strong> Encoding colors {red, blue, green} as red=[1,0,0], blue=[0,1,0], green=[0,0,1].</p> <h4 id=one-vs-all>One-vs-All<a class=headerlink href=#one-vs-all title="Permanent link">&para;</a></h4> <p>A multiclass classification strategy that trains k binary classifiers (one per class) to distinguish each class from all others combined.</p> <p><strong>Example:</strong> For 5 classes, one-vs-all trains 5 binary classifiers: class_1 vs. others, class_2 vs. others, ..., class_5 vs. others.</p> <h4 id=one-vs-one>One-vs-One<a class=headerlink href=#one-vs-one title="Permanent link">&para;</a></h4> <p>A multiclass classification strategy that trains k(k-1)/2 binary classifiers, one for each pair of classes, then aggregates their votes.</p> <p><strong>Example:</strong> For 5 classes, one-vs-one trains 10 binary classifiers for all pairs: (1,2), (1,3), (1,4), (1,5), (2,3), (2,4), (2,5), (3,4), (3,5), (4,5).</p> <h4 id=online-learning>Online Learning<a class=headerlink href=#online-learning title="Permanent link">&para;</a></h4> <p>A learning paradigm where models are updated incrementally as new data arrives, rather than being trained once on a fixed dataset.</p> <p><strong>Example:</strong> A spam filter that updates its model every time a user marks an email as spam or not spam, continuously adapting to new spam patterns.</p> <h4 id=optimizer>Optimizer<a class=headerlink href=#optimizer title="Permanent link">&para;</a></h4> <p>An algorithm that adjusts model parameters to minimize the loss function, typically variants of gradient descent.</p> <p><strong>Example:</strong> Adam, SGD with momentum, and RMSprop are popular optimizers for training neural networks, each with different strategies for updating weights.</p> <h4 id=output-layer>Output Layer<a class=headerlink href=#output-layer title="Permanent link">&para;</a></h4> <p>The final layer of a neural network that produces predictions, with the number of neurons matching the prediction task.</p> <p><strong>Example:</strong> For 10-class classification, the output layer has 10 neurons with softmax activation producing probability estimates for each class.</p> <h4 id=overfitting>Overfitting<a class=headerlink href=#overfitting title="Permanent link">&para;</a></h4> <p>A modeling error where a model learns patterns specific to the training data (including noise) rather than general patterns, resulting in poor performance on new data.</p> <p><strong>Example:</strong> A decision tree with 50 levels that achieves 100% training accuracy but only 60% test accuracy has overfit to training noise.</p> <h4 id=padding>Padding<a class=headerlink href=#padding title="Permanent link">&para;</a></h4> <p>Adding extra pixels (typically zeros) around the border of an input before convolution to control the spatial dimensions of the output feature map.</p> <p><strong>Example:</strong> Adding 1 pixel of padding around a 5×5 image creates a 7×7 padded input, allowing a 3×3 filter to produce a 5×5 output instead of 3×3.</p> <h4 id=perceptron>Perceptron<a class=headerlink href=#perceptron title="Permanent link">&para;</a></h4> <p>A single-layer neural network that learns a linear decision boundary for binary classification using a simple update rule.</p> <p><strong>Example:</strong> The perceptron learning algorithm adjusts weights when a training example is misclassified: w_new = w_old + learning_rate × y × x.</p> <h4 id=polynomial-kernel>Polynomial Kernel<a class=headerlink href=#polynomial-kernel title="Permanent link">&para;</a></h4> <p>A kernel function for SVMs that computes polynomial combinations of features, enabling learning of polynomial decision boundaries.</p> <p><strong>Example:</strong> Polynomial kernel K(x, y) = (x · y + c)^d with degree d=2 allows SVMs to learn parabolic decision boundaries.</p> <h4 id=pooling-layer>Pooling Layer<a class=headerlink href=#pooling-layer title="Permanent link">&para;</a></h4> <p>A downsampling layer in CNNs that reduces the spatial dimensions of feature maps while retaining important information.</p> <p><strong>Example:</strong> A 2×2 max pooling layer reduces a 28×28 feature map to 14×14 by taking the maximum value in each 2×2 window.</p> <h4 id=precision>Precision<a class=headerlink href=#precision title="Permanent link">&para;</a></h4> <p>The proportion of positive predictions that are actually correct, measuring how many predicted positives are true positives.</p> <p><strong>Example:</strong> If a spam filter marks 100 emails as spam and 80 actually are spam, precision is 80/100 = 0.80.</p> <h4 id=pre-trained-model>Pre-Trained Model<a class=headerlink href=#pre-trained-model title="Permanent link">&para;</a></h4> <p>A neural network model that has been trained on a large dataset and whose learned weights can be reused for related tasks through transfer learning.</p> <p><strong>Example:</strong> A ResNet-50 pre-trained on ImageNet with weights learned from 1.2 million images that can be fine-tuned for medical image classification.</p> <h4 id=primal-formulation>Primal Formulation<a class=headerlink href=#primal-formulation title="Permanent link">&para;</a></h4> <p>The original form of an optimization problem expressed in terms of the primary decision variables (like weights in SVM).</p> <p><strong>Example:</strong> The primal SVM formulation minimizes ||w||² subject to constraints on the margin for each training example.</p> <h4 id=pruning>Pruning<a class=headerlink href=#pruning title="Permanent link">&para;</a></h4> <p>Removing branches or nodes from a decision tree to reduce complexity and prevent overfitting.</p> <p><strong>Example:</strong> Post-pruning removes tree branches where validation error doesn't improve, reducing a 20-level tree to 8 levels while maintaining or improving test accuracy.</p> <h4 id=radial-basis-function>Radial Basis Function<a class=headerlink href=#radial-basis-function title="Permanent link">&para;</a></h4> <p>A kernel function for SVMs that measures similarity between points based on their Euclidean distance, creating circular (radial) decision boundaries.</p> <p><strong>Example:</strong> RBF kernel K(x, y) = exp(-γ||x - y||²) with γ=0.1 creates smooth, non-linear decision boundaries that can form complex shapes.</p> <h4 id=random-initialization>Random Initialization<a class=headerlink href=#random-initialization title="Permanent link">&para;</a></h4> <p>Starting k-means clustering with k randomly selected data points as initial centroids.</p> <p><strong>Example:</strong> For k=3, randomly choose 3 of the 150 training points as initial cluster centers before beginning k-means iterations.</p> <h4 id=random-search>Random Search<a class=headerlink href=#random-search title="Permanent link">&para;</a></h4> <p>A hyperparameter tuning method that randomly samples hyperparameter combinations from specified distributions and evaluates them using cross-validation.</p> <p><strong>Example:</strong> Sampling 50 random combinations of learning rate from [0.0001, 0.1] and regularization from [0.001, 10] using logarithmic distributions.</p> <h4 id=recall>Recall<a class=headerlink href=#recall title="Permanent link">&para;</a></h4> <p>The proportion of actual positives that are correctly identified, measuring how many true positives are captured by the model.</p> <p><strong>Example:</strong> If 100 emails are actually spam and a filter correctly identifies 70 of them, recall is 70/100 = 0.70 (also called sensitivity or true positive rate).</p> <h4 id=receptive-field>Receptive Field<a class=headerlink href=#receptive-field title="Permanent link">&para;</a></h4> <p>The region of the input that influences a particular neuron's activation in a neural network, growing larger in deeper layers of CNNs.</p> <p><strong>Example:</strong> In a CNN with two 3×3 convolutional layers, a neuron in the second layer has a 5×5 receptive field in the original input image.</p> <h4 id=regression>Regression<a class=headerlink href=#regression title="Permanent link">&para;</a></h4> <p>A supervised learning task where the goal is to predict a continuous numerical value for each input instance.</p> <p><strong>Example:</strong> Predicting house prices (in dollars) based on features like square footage, location, and number of bedrooms.</p> <h4 id=regularization>Regularization<a class=headerlink href=#regularization title="Permanent link">&para;</a></h4> <p>Techniques for reducing model complexity and preventing overfitting by adding constraints or penalties to the learning process.</p> <p><strong>Example:</strong> Adding an L2 penalty term λ Σwᵢ² to the loss function discourages large weights, reducing overfitting in linear regression.</p> <h4 id=relu>ReLU<a class=headerlink href=#relu title="Permanent link">&para;</a></h4> <p>Rectified Linear Unit activation function that outputs the input if positive, otherwise zero: f(x) = max(0, x).</p> <p><strong>Example:</strong> ReLU([-2, 0, 3]) = [0, 0, 3], eliminating negative values while preserving positive values unchanged.</p> <h4 id=resnet>ResNet<a class=headerlink href=#resnet title="Permanent link">&para;</a></h4> <p>A deep CNN architecture using residual connections (skip connections) that enable training of very deep networks (50-200 layers) by addressing vanishing gradients.</p> <p><strong>Example:</strong> ResNet-50 uses residual blocks with skip connections, allowing gradients to flow directly through the network without diminishing over 50 layers.</p> <h4 id=ridge-regression>Ridge Regression<a class=headerlink href=#ridge-regression title="Permanent link">&para;</a></h4> <p>Linear regression with L2 regularization that shrinks coefficient estimates to reduce overfitting.</p> <p><strong>Example:</strong> Ridge regression with α=1.0 minimizes (Σ(yᵢ - ŷᵢ)² + α Σwⱼ²), trading some training error for smaller, more stable coefficients.</p> <h4 id=rmsprop>RMSprop<a class=headerlink href=#rmsprop title="Permanent link">&para;</a></h4> <p>An adaptive learning rate optimizer that divides the learning rate by a running average of recent gradient magnitudes.</p> <p><strong>Example:</strong> RMSprop adapts learning rates per parameter, using larger steps for parameters with small gradients and smaller steps for those with large gradients.</p> <h4 id=roc-curve>ROC Curve<a class=headerlink href=#roc-curve title="Permanent link">&para;</a></h4> <p>Receiver Operating Characteristic curve plotting true positive rate against false positive rate at various classification thresholds.</p> <p><strong>Example:</strong> An ROC curve shows how recall and false positive rate change as you vary the threshold for classifying examples as positive, with perfect classifiers reaching point (0, 1).</p> <h4 id=same-padding>Same Padding<a class=headerlink href=#same-padding title="Permanent link">&para;</a></h4> <p>A padding strategy that adds enough border pixels so the output feature map has the same spatial dimensions as the input.</p> <p><strong>Example:</strong> For a 5×5 input with a 3×3 filter, same padding adds 1 pixel of padding on all sides to produce a 5×5 output instead of 3×3.</p> <h4 id=scalability>Scalability<a class=headerlink href=#scalability title="Permanent link">&para;</a></h4> <p>A system's ability to handle increasing amounts of data or computational demands by adding resources.</p> <p><strong>Example:</strong> An algorithm with O(n log n) time complexity is more scalable than one with O(n²) because runtime grows more slowly as dataset size n increases.</p> <h4 id=sensitivity>Sensitivity<a class=headerlink href=#sensitivity title="Permanent link">&para;</a></h4> <p>The true positive rate or recall, measuring the proportion of actual positives correctly identified by the classifier.</p> <p><strong>Example:</strong> A medical test with 90% sensitivity correctly identifies 90 out of 100 patients who actually have the disease.</p> <h4 id=sigmoid-activation>Sigmoid Activation<a class=headerlink href=#sigmoid-activation title="Permanent link">&para;</a></h4> <p>An activation function that maps inputs to the range (0, 1) using the formula f(x) = 1 / (1 + e^(-x)).</p> <p><strong>Example:</strong> Sigmoid(0) = 0.5, sigmoid(2) ≈ 0.88, sigmoid(-2) ≈ 0.12, creating a smooth S-shaped curve useful for binary classification output layers.</p> <h4 id=sigmoid-function>Sigmoid Function<a class=headerlink href=#sigmoid-function title="Permanent link">&para;</a></h4> <p>A mathematical function that maps any real number to the range (0, 1), commonly used in logistic regression and neural network output layers for binary classification.</p> <p><strong>Example:</strong> The sigmoid function σ(x) = 1/(1 + e^(-x)) converts a linear combination of features into a probability estimate between 0 and 1.</p> <h4 id=silhouette-score>Silhouette Score<a class=headerlink href=#silhouette-score title="Permanent link">&para;</a></h4> <p>A metric measuring how well-separated clusters are by comparing the average distance to points in the same cluster versus the average distance to points in the nearest different cluster.</p> <p><strong>Example:</strong> Silhouette scores range from -1 (poor clustering) to +1 (excellent clustering), with values near 0 indicating overlapping clusters.</p> <h4 id=slack-variables>Slack Variables<a class=headerlink href=#slack-variables title="Permanent link">&para;</a></h4> <p>Variables in soft margin SVMs that allow some training examples to violate the margin constraints, enabling solutions for non-linearly separable data.</p> <p><strong>Example:</strong> A slack variable ξᵢ &gt; 0 for example i indicates it lies within the margin or on the wrong side of the decision boundary, with larger values for worse violations.</p> <h4 id=soft-margin-svm>Soft Margin SVM<a class=headerlink href=#soft-margin-svm title="Permanent link">&para;</a></h4> <p>A support vector machine variant that tolerates some classification errors and margin violations through slack variables and a penalty parameter C.</p> <p><strong>Example:</strong> Soft margin SVM with C=1.0 allows misclassifications to achieve a wider margin, balancing margin maximization with training accuracy.</p> <h4 id=softmax-function>Softmax Function<a class=headerlink href=#softmax-function title="Permanent link">&para;</a></h4> <p>A function that converts a vector of real numbers into a probability distribution, where each element is between 0 and 1 and all elements sum to 1.</p> <p><strong>Example:</strong> Softmax([2, 1, 0]) = [0.66, 0.24, 0.10], converting raw scores (logits) into probabilities that sum to 1 for multiclass classification.</p> <h4 id=space-complexity>Space Complexity<a class=headerlink href=#space-complexity title="Permanent link">&para;</a></h4> <p>The amount of memory required by an algorithm as a function of input size.</p> <p><strong>Example:</strong> K-nearest neighbors has O(nd) space complexity because it stores all n training examples with d features, while logistic regression has O(d) space for just the weight vector.</p> <h4 id=spatial-hierarchies>Spatial Hierarchies<a class=headerlink href=#spatial-hierarchies title="Permanent link">&para;</a></h4> <p>The layered representation of features in CNNs where early layers detect simple patterns (edges) and deeper layers detect complex patterns (objects) by combining simpler features.</p> <p><strong>Example:</strong> In a CNN for face recognition, layer 1 detects edges, layer 2 detects facial features (eyes, nose), layer 3 detects face parts, layer 4 detects complete faces.</p> <h4 id=specificity>Specificity<a class=headerlink href=#specificity title="Permanent link">&para;</a></h4> <p>The true negative rate, measuring the proportion of actual negatives correctly identified by the classifier.</p> <p><strong>Example:</strong> A test with 95% specificity correctly identifies 95 out of 100 people who don't have the disease as healthy (5 false positives).</p> <h4 id=splitting-criterion>Splitting Criterion<a class=headerlink href=#splitting-criterion title="Permanent link">&para;</a></h4> <p>The rule or metric used to select features and thresholds for splitting nodes in decision tree construction.</p> <p><strong>Example:</strong> A decision tree might use information gain as the splitting criterion, selecting at each node the feature and threshold that maximize information gain.</p> <h4 id=standardization>Standardization<a class=headerlink href=#standardization title="Permanent link">&para;</a></h4> <p>A normalization technique that transforms features to have mean 0 and standard deviation 1 by subtracting the mean and dividing by the standard deviation.</p> <p><strong>Example:</strong> Feature with values [10, 20, 30] (mean=20, std=8.165) standardized becomes [-1.22, 0, 1.22] using formula (x - μ)/σ.</p> <h4 id=stochastic-gradient-descent>Stochastic Gradient Descent<a class=headerlink href=#stochastic-gradient-descent title="Permanent link">&para;</a></h4> <p>A gradient descent variant that computes gradients and updates weights using one randomly selected training example at a time.</p> <p><strong>Example:</strong> Instead of computing gradients over all 10,000 training examples, SGD randomly samples one example, computes its gradient, updates weights, then repeats with another random example.</p> <h4 id=stratified-sampling>Stratified Sampling<a class=headerlink href=#stratified-sampling title="Permanent link">&para;</a></h4> <p>A sampling strategy that preserves the proportion of each class when creating train/test splits, particularly important for imbalanced datasets.</p> <p><strong>Example:</strong> For data with 80% class A and 20% class B, stratified sampling ensures both training and test sets maintain the same 80/20 split.</p> <h4 id=stride>Stride<a class=headerlink href=#stride title="Permanent link">&para;</a></h4> <p>The step size by which a convolutional filter moves across the input at each position.</p> <p><strong>Example:</strong> A stride of 2 means the filter moves 2 pixels at a time, reducing the output size by half in each dimension compared to stride 1.</p> <h4 id=supervised-learning>Supervised Learning<a class=headerlink href=#supervised-learning title="Permanent link">&para;</a></h4> <p>A machine learning paradigm where models learn from labeled training data to predict labels for new, unseen instances.</p> <p><strong>Example:</strong> Training a model on 1,000 labeled images of cats and dogs (supervised data) to predict whether new images contain cats or dogs.</p> <h4 id=support-vector-machine>Support Vector Machine<a class=headerlink href=#support-vector-machine title="Permanent link">&para;</a></h4> <p>A supervised learning algorithm that finds the optimal hyperplane maximizing the margin between classes, optionally using kernel functions for non-linear decision boundaries.</p> <p><strong>Example:</strong> SVM with RBF kernel separates non-linearly separable data by implicitly mapping it to a higher-dimensional space where a linear boundary works.</p> <h4 id=support-vectors>Support Vectors<a class=headerlink href=#support-vectors title="Permanent link">&para;</a></h4> <p>The training examples closest to the decision boundary in an SVM that determine the position and orientation of the optimal hyperplane.</p> <p><strong>Example:</strong> In a dataset of 1,000 examples, only 50 might be support vectors (points lying on the margin), and removing non-support vectors doesn't change the decision boundary.</p> <h4 id=tanh>Tanh<a class=headerlink href=#tanh title="Permanent link">&para;</a></h4> <p>Hyperbolic tangent activation function that maps inputs to the range (-1, 1) using f(x) = (e^x - e^(-x)) / (e^x + e^(-x)).</p> <p><strong>Example:</strong> Tanh(0) = 0, tanh(2) ≈ 0.96, tanh(-2) ≈ -0.96, providing a zero-centered alternative to sigmoid.</p> <h4 id=test-data>Test Data<a class=headerlink href=#test-data title="Permanent link">&para;</a></h4> <p>A held-out dataset used only for final model evaluation after all training and hyperparameter tuning is complete, providing an unbiased estimate of performance.</p> <p><strong>Example:</strong> Setting aside 20% of data as test data that is never used during model development, only for the final performance report.</p> <h4 id=test-error>Test Error<a class=headerlink href=#test-error title="Permanent link">&para;</a></h4> <p>The error rate or loss computed on the test set, measuring model performance on unseen data and serving as an estimate of real-world performance.</p> <p><strong>Example:</strong> A model with 5% test error (95% test accuracy) is expected to correctly classify 95% of new, previously unseen examples.</p> <h4 id=time-complexity>Time Complexity<a class=headerlink href=#time-complexity title="Permanent link">&para;</a></h4> <p>The computational time required by an algorithm as a function of input size, typically expressed using Big-O notation.</p> <p><strong>Example:</strong> K-nearest neighbors has O(nd) time complexity per prediction for n training examples with d features, while decision tree prediction is O(log n) for a balanced tree.</p> <h4 id=training-data>Training Data<a class=headerlink href=#training-data title="Permanent link">&para;</a></h4> <p>The dataset used to fit a machine learning model by adjusting its parameters to minimize the loss function.</p> <p><strong>Example:</strong> Using 80% of available labeled images to train a classifier by iteratively updating weights to reduce classification errors.</p> <h4 id=training-error>Training Error<a class=headerlink href=#training-error title="Permanent link">&para;</a></h4> <p>The error rate or loss computed on the training set, measuring how well the model fits the data it was trained on.</p> <p><strong>Example:</strong> A model with 2% training error correctly classifies 98% of training examples, but this may indicate overfitting if test error is much higher.</p> <h4 id=transfer-learning>Transfer Learning<a class=headerlink href=#transfer-learning title="Permanent link">&para;</a></h4> <p>A technique that leverages knowledge learned from one task or domain to improve learning in a different but related task or domain, typically by starting with pre-trained model weights.</p> <p><strong>Example:</strong> Using a ResNet-50 pre-trained on ImageNet to classify medical images by fine-tuning the final layers on a smaller medical image dataset.</p> <h4 id=translation-invariance>Translation Invariance<a class=headerlink href=#translation-invariance title="Permanent link">&para;</a></h4> <p>A property of CNNs where the network's ability to recognize patterns is approximately independent of their spatial location in the input.</p> <p><strong>Example:</strong> A CNN trained to detect cats can recognize a cat whether it appears in the top-left, center, or bottom-right of an image.</p> <h4 id=tree-depth>Tree Depth<a class=headerlink href=#tree-depth title="Permanent link">&para;</a></h4> <p>The maximum number of edges from the root to any leaf node in a decision tree, controlling model complexity.</p> <p><strong>Example:</strong> A tree with depth 5 makes up to 5 sequential decisions to reach a prediction, while depth 1 (decision stump) makes a single decision.</p> <h4 id=tree-node>Tree Node<a class=headerlink href=#tree-node title="Permanent link">&para;</a></h4> <p>An internal point in a decision tree that splits data based on a feature value test, with child nodes for each outcome of the test.</p> <p><strong>Example:</strong> A node might test "petal length &lt; 2.5 cm" and direct examples with shorter petals to the left child and longer petals to the right child.</p> <h4 id=true-negative>True Negative<a class=headerlink href=#true-negative title="Permanent link">&para;</a></h4> <p>An instance where the model correctly predicts the negative class when the true class is indeed negative.</p> <p><strong>Example:</strong> A spam filter correctly identifies a legitimate email as "not spam" is a true negative.</p> <h4 id=true-positive>True Positive<a class=headerlink href=#true-positive title="Permanent link">&para;</a></h4> <p>An instance where the model correctly predicts the positive class when the true class is indeed positive.</p> <p><strong>Example:</strong> A medical test correctly identifying a patient with disease as "positive" is a true positive.</p> <h4 id=underfitting>Underfitting<a class=headerlink href=#underfitting title="Permanent link">&para;</a></h4> <p>A modeling error where a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data.</p> <p><strong>Example:</strong> Using linear regression (underfitting) on highly non-linear data yields 70% training accuracy and 68% test accuracy, both poor.</p> <h4 id=universal-approximation>Universal Approximation<a class=headerlink href=#universal-approximation title="Permanent link">&para;</a></h4> <p>A theorem stating that a feedforward neural network with at least one hidden layer and sufficient neurons can approximate any continuous function to arbitrary precision.</p> <p><strong>Example:</strong> A single hidden layer MLP with enough neurons can theoretically learn any continuous mapping from inputs to outputs, though depth often makes learning more practical.</p> <h4 id=unsupervised-learning>Unsupervised Learning<a class=headerlink href=#unsupervised-learning title="Permanent link">&para;</a></h4> <p>A machine learning paradigm where models learn patterns and structure from unlabeled data without explicit target outputs.</p> <p><strong>Example:</strong> K-means clustering discovers natural groupings in customer data without being told how many groups exist or what defines each group.</p> <h4 id=valid-padding>Valid Padding<a class=headerlink href=#valid-padding title="Permanent link">&para;</a></h4> <p>A padding strategy where no padding is added, so the output size is smaller than the input by the filter size minus one.</p> <p><strong>Example:</strong> A 5×5 input with a 3×3 filter and valid padding produces a 3×3 output (5 - 3 + 1 = 3).</p> <h4 id=validation-data>Validation Data<a class=headerlink href=#validation-data title="Permanent link">&para;</a></h4> <p>A dataset held out from training used to tune hyperparameters and make model selection decisions during development.</p> <p><strong>Example:</strong> Using 20% of training data as validation data to select the best learning rate, then retraining on all training data with the chosen learning rate before final test evaluation.</p> <h4 id=validation-error>Validation Error<a class=headerlink href=#validation-error title="Permanent link">&para;</a></h4> <p>The error rate or loss computed on the validation set, used for hyperparameter tuning and model selection without touching the test set.</p> <p><strong>Example:</strong> Tracking validation error during training to implement early stopping when it stops decreasing, preventing overfitting.</p> <h4 id=vanishing-gradient>Vanishing Gradient<a class=headerlink href=#vanishing-gradient title="Permanent link">&para;</a></h4> <p>A numerical instability during neural network training where gradients become extremely small in early layers, preventing effective learning.</p> <p><strong>Example:</strong> In a 100-layer network using sigmoid activation, gradients might shrink from 0.01 at layer 100 to 10^(-30) at layer 1, making learning impossible without techniques like ReLU or batch normalization.</p> <h4 id=vgg>VGG<a class=headerlink href=#vgg title="Permanent link">&para;</a></h4> <p>A CNN architecture family characterized by using many small (3×3) convolutional filters in deep networks (VGG-16, VGG-19), demonstrating that depth improves performance.</p> <p><strong>Example:</strong> VGG-16 uses 16 weight layers with consistent 3×3 filters throughout, achieving strong ImageNet performance with a simple, uniform architecture.</p> <h4 id=voronoi-diagram>Voronoi Diagram<a class=headerlink href=#voronoi-diagram title="Permanent link">&para;</a></h4> <p>A partitioning of space into regions where each region contains all points closest to a particular data point, visualizing k-NN decision boundaries.</p> <p><strong>Example:</strong> For k=1, the k-NN decision boundary forms a Voronoi diagram with each region corresponding to one training example's class.</p> <h4 id=weight-initialization>Weight Initialization<a class=headerlink href=#weight-initialization title="Permanent link">&para;</a></h4> <p>The strategy for setting initial values of neural network weights before training begins, critically affecting convergence speed and final performance.</p> <p><strong>Example:</strong> Xavier initialization sets initial weights to random values from N(0, √(1/n_in)), preventing activations from vanishing or exploding in early training.</p> <h4 id=weight-sharing>Weight Sharing<a class=headerlink href=#weight-sharing title="Permanent link">&para;</a></h4> <p>A property of convolutional layers where the same filter weights are applied at every spatial location, dramatically reducing parameters compared to fully connected layers.</p> <p><strong>Example:</strong> A 3×3 filter with weight sharing uses only 9 parameters regardless of input size, while a fully connected layer connecting 100 input to 100 output neurons needs 10,000 parameters.</p> <h4 id=weights>Weights<a class=headerlink href=#weights title="Permanent link">&para;</a></h4> <p>Learnable parameters in a neural network that multiply input values, representing the strength of connections between neurons.</p> <p><strong>Example:</strong> In a neuron computing output = w₁x₁ + w₂x₂ + b, the weights w₁ and w₂ determine how much each input contributes to the output.</p> <h4 id=within-cluster-variance>Within-Cluster Variance<a class=headerlink href=#within-cluster-variance title="Permanent link">&para;</a></h4> <p>A measure of how spread out points are within each cluster, typically measured as the sum of squared distances from points to their cluster centroid.</p> <p><strong>Example:</strong> Lower within-cluster variance indicates tighter, more compact clusters with points close to their centroids.</p> <h4 id=xavier-initialization>Xavier Initialization<a class=headerlink href=#xavier-initialization title="Permanent link">&para;</a></h4> <p>A weight initialization strategy for neural networks using symmetric activations (like tanh) that draws weights from a distribution with variance 1/n_in.</p> <p><strong>Example:</strong> Xavier initialization for a layer with 100 input neurons samples weights from N(0, √(1/100)) to maintain activation variance across layers.</p> <h4 id=z-score-normalization>Z-Score Normalization<a class=headerlink href=#z-score-normalization title="Permanent link">&para;</a></h4> <p>A standardization technique that transforms features to have mean 0 and standard deviation 1 by subtracting the mean and dividing by the standard deviation.</p> <p><strong>Example:</strong> Feature values [10, 20, 30] with mean 20 and std 8.165 become [-1.22, 0, 1.22] after z-score normalization.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 | CC BY-NC-SA 4.0 DEED </div> </div> <div class=md-social> <a href=https://github.com/AnvithPothula target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/anvith-pothula target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.79ae519e.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>